<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Jason Antman's Blog</title><link>http://blog.jasonantman.com/</link><description></description><atom:link href="http://blog.jasonantman.com/feeds/tags/aws.rss.xml" rel="self"></atom:link><lastBuildDate>Tue, 19 Apr 2016 07:40:00 -0400</lastBuildDate><item><title>Terraform Shortcomings - No Interpolated Default Values, No Functions, No Conditionals, Local State Storage</title><link>http://blog.jasonantman.com/2016/04/terraform-shortcomings-no-interpolated-default-values-no-functions/</link><description>&lt;p&gt;Lately I&amp;#8217;ve been using HashiCorp&amp;#8217;s &lt;a href="https://www.terraform.io/"&gt;Terraform&lt;/a&gt; a lot to manage infrastructure. It certainly has some big things going for it; it supports a whole bunch of providers (including on-prem, non-cloud stuff like VMWare and Docker) as well as some database engines and &lt;span class="caps"&gt;DNS&lt;/span&gt; providers and can even manage GitHub teams, it can plan changes before committing them (which CloudFormation only &lt;a href="https://aws.amazon.com/blogs/aws/new-change-sets-for-aws-cloudformation/"&gt;very recently&lt;/a&gt; learned), and it can store the current state of your infrastructure in &lt;a href="https://www.consul.io/"&gt;Consul&lt;/a&gt;. Also a big step past CloudFormation, it has &lt;a href="https://www.terraform.io/docs/provisioners/index.html"&gt;provisioners&lt;/a&gt; including local execution, remote execution, file copying and Chef (strangely no built-in support for Puppet, but the remote-exec can do that) that can reach out to your newly-created instances and take actions on&amp;nbsp;them.&lt;/p&gt;
&lt;p&gt;Terraform also has a &lt;a href="https://www.terraform.io/docs/providers/template/index.html"&gt;template provider&lt;/a&gt; that&amp;#8217;s used any time you need a templated file, such as &lt;span class="caps"&gt;EC2&lt;/span&gt; instance user-data or dynamically generated scripts to place on hosts. Terraform uses a &lt;span class="caps"&gt;DSL&lt;/span&gt; for its &lt;a href="https://www.terraform.io/docs/configuration/index.html"&gt;configuration&lt;/a&gt;, either the &lt;span class="caps"&gt;JSON&lt;/span&gt;-like but slightly-more-human-readable &lt;a href="https://github.com/hashicorp/hcl"&gt;Hashicorp Configuration Language (&lt;span class="caps"&gt;HCL&lt;/span&gt;)&lt;/a&gt; or the same information conveyed in pure &lt;span class="caps"&gt;JSON&lt;/span&gt;. The configuration language supports variables (passed in at the command line or in a file) and is based on &lt;a href="https://www.terraform.io/docs/configuration/interpolation.html"&gt;string interpolation&lt;/a&gt; with a handful of functions defined. It&amp;#8217;s also worth noting that Terraform is written in Go; it has a &lt;a href="https://www.terraform.io/docs/plugins/index.html"&gt;plugin system&lt;/a&gt; but only for Providers and Provisioners; there&amp;#8217;s no way to add core functionality (I suppose I&amp;#8217;ve been spolied by Puppet having such good support for adding core functionality via Ruby, or HashiCorp&amp;#8217;s Vagrant having a config file that itself is&amp;nbsp;Ruby).&lt;/p&gt;
&lt;p&gt;Now that I&amp;#8217;ve been nice and said some great things about Terraform (and it really is; at least for the way my current job is managing infrastructure, I&amp;#8217;ve fallen in love with it, and it certainly does fix some shortcomings that I found in CloudFormation, specifically with pre-execution plans and ability to interact with resources), on to my complaints of the&amp;nbsp;day.&lt;/p&gt;
&lt;h2 id="local-state-storage"&gt;Local State&amp;nbsp;Storage&lt;/h2&gt;
&lt;p&gt;My first complaint is that by default, Terraform stores the state of your infrastrucutre in a file in your current working directory. It uses this to attempt to figure out the already-existing resources you&amp;#8217;ve created, and only make the required changes. The first time I used terraform, I completely destroyed one of our (luckily non-production) services; coworkers of mine have brought down production services because of&amp;nbsp;this.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s say that we have a Terraform configuration which takes one variable, &lt;code&gt;environment&lt;/code&gt;. That variable determines the &lt;span class="caps"&gt;VPC&lt;/span&gt; and subnets we deploy into, our &lt;span class="caps"&gt;DNS&lt;/span&gt; names, and also gets passed to &lt;span class="caps"&gt;EC2&lt;/span&gt; instances via user-data. We build our infrastructure with &lt;code&gt;environment = "prod"&lt;/code&gt;, and everything works right - we now have a production cluster of our service. Then we want to test some changes, so we run again with &lt;code&gt;environment = "dev"&lt;/code&gt;. The naive - and logical - assumption would be that we get a second &amp;#8220;dev&amp;#8221; cluster of our service. Nope. Terraform finds the &lt;code&gt;terraform.tfstate&lt;/code&gt; file in our current directory, reads it, and takes it to be the current state of our infrastructure. It sees that we &lt;strong&gt;changed&lt;/strong&gt; &lt;code&gt;environment&lt;/code&gt; from &amp;#8220;prod&amp;#8221; to &amp;#8220;dev&amp;#8221;&amp;#8230; so it destroys our &lt;span class="caps"&gt;EC2&lt;/span&gt; instances and &lt;span class="caps"&gt;DNS&lt;/span&gt; record, and creates new ones for &amp;#8220;dev&amp;#8221; (applying the requested&amp;nbsp;changes).&lt;/p&gt;
&lt;p&gt;This teaches us two important&amp;nbsp;points:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Always&lt;/strong&gt; run &lt;code&gt;terraform plan&lt;/code&gt;. Even if you think your changes are trivial, examine what Terraform will do before running &lt;code&gt;apply&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Always&lt;/strong&gt; run &lt;code&gt;terraform&lt;/code&gt; through a wrapper. We have a simple Rake task in an internal rubygem that ensures that Terraform will always store state in Consul, so it won&amp;#8217;t be locked to one person&amp;#8217;s local machine, and also removes any local state files before running so they won&amp;#8217;t pollute the run or result in changes intended for one isolated instance of our Terraform configuration from being applied to&amp;nbsp;another.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="functions"&gt;Functions&lt;/h2&gt;
&lt;p&gt;Terraform&amp;#8217;s &lt;a href="https://www.terraform.io/docs/configuration/interpolation.html"&gt;configuration interpolation&lt;/a&gt; has a bunch of built-in functions for working with variables. They&amp;#8217;re a subset of what you&amp;#8217;d expect in a language that is mainly based around strings, arrays and maps/hashes: split, join, concat, lookup (get a hash item by key), index (find the index of an item in a list), element (return the n&amp;#8217;th element of a list), format (sprintf-like), etc. However, there&amp;#8217;s no function to retrieve only unique elements from a list. This becomes a problem especially when dealing with multi-&lt;span class="caps"&gt;AZ&lt;/span&gt;/multi-subnet &lt;span class="caps"&gt;AWS&lt;/span&gt; resources, as some of them (e.g. managing a set number individual &lt;span class="caps"&gt;EC2&lt;/span&gt; instances outside of an &lt;span class="caps"&gt;ASG&lt;/span&gt;, such as when assigning static IPs) require a list of subnets matching the number of resources, and others (cross-&lt;span class="caps"&gt;AZ&lt;/span&gt; ELBs) require a list of unique&amp;nbsp;subnets.&lt;/p&gt;
&lt;p&gt;Terraform and its language have no way to add this functionality (&lt;em&gt;see note below&lt;/em&gt;); the only option that I&amp;#8217;ve found is to wrap Terraform in some sort of runner (I use &lt;a href="https://github.com/ruby/rake"&gt;Rake&lt;/a&gt; but you could use any scripting or Make-like language) that does whatever manipulation and calculation is needed, and passes in the necessary values distinct variable values (i.e. the full subnet list, and the unique subnet list, as separate variables). To make this even more difficult, though Terraform supports loading built-time variables from a &lt;span class="caps"&gt;JSON&lt;/span&gt; or &lt;span class="caps"&gt;HCL&lt;/span&gt; file instead of the command line, it only supports taking in variables as strings (even in &lt;span class="caps"&gt;JSON&lt;/span&gt;). So in our subnet example, our wrapper script needs to join the list of subnets into a string (i.e. &lt;span class="caps"&gt;CSV&lt;/span&gt;) and then whenever we use the variable in Terraform, we need to &lt;code&gt;split()&lt;/code&gt; it on our separator character (because Terraform doesn&amp;#8217;t support variable setting or&amp;nbsp;manipulation).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;Terraform [has] no way to add this functionality&amp;#8221;&lt;/em&gt; - I&amp;#8217;m aware that I could fork Terraform, learn Go, and submit pull requests for all of the features I think would be useful; and if I had maybe half a dozen less unfinished projects, I&amp;#8217;d probably do that. However, this still means that HashiCorp would need to accept and merge my PRs and release a new version, or else I&amp;#8217;d need to build and distribute my forked version. Terraform supports &lt;a href="https://www.terraform.io/docs/plugins/index.html"&gt;plugins&lt;/a&gt;, but only for Providers and Provisioners, not language internals. What I&amp;#8217;d really like is a way to define plugin functions that could be distributed without having to rebuild all of&amp;nbsp;Terraform.&lt;/p&gt;
&lt;h2 id="no-interpolated-default-variable-values"&gt;No Interpolated Default Variable&amp;nbsp;Values&lt;/h2&gt;
&lt;p&gt;Terraform variables can have default values defined for them. However, these default values have no way of using other variables. This means that even for relatively common use cases - like a service that has a name and a &lt;span class="caps"&gt;DNS&lt;/span&gt; record, both of which can be overridden but with the &lt;span class="caps"&gt;DNS&lt;/span&gt; record defaulting to &amp;#8220;SERVICE_NAME.example.com&amp;#8221;, you can&amp;#8217;t do that. The only options that I&amp;#8217;ve been able to figure out are to either do it in your wrapper script (which means the Terraform configs can&amp;#8217;t be run without the wrapper) or use the &lt;code&gt;coalesce&lt;/code&gt; function to give your variable an empty default value, and then choose a second interpolated string if the variable is&amp;nbsp;empty.&lt;/p&gt;
&lt;h2 id="no-conditionals"&gt;No&amp;nbsp;Conditionals&lt;/h2&gt;
&lt;p&gt;Terraform&amp;#8217;s configuration language also lacks conditional statements such as &lt;code&gt;if&lt;/code&gt;. This poses a problem with all but the simplest applications, and is certainly likely to be an issue for anyone who wants to do the right thing and use the same tooling to deploy multiple environments. It seems that the only options are to either pass in the necessary information as variables from a wrapper script, or generate Terraform configurations with other tooling. The former works only if the desired result is a variable in your configuration; there&amp;#8217;s simply no way that I&amp;#8217;ve found to have a conditional around resource(s). The only obvious option for that is to take advantage of Terraform&amp;#8217;s ability to read configurations as &lt;span class="caps"&gt;JSON&lt;/span&gt;, and simply generate your entire terraform configuration with another&amp;nbsp;tool.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Tue, 19 Apr 2016 07:40:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2016-04-19:2016/04/terraform-shortcomings-no-interpolated-default-values-no-functions/</guid><category>terraform</category><category>hashicorp</category><category>AWS</category><category>go</category></item><item><title>AwsLimitChecker - Check Your AWS Usage Against Service Limits</title><link>http://blog.jasonantman.com/2015/07/awslimitchecker-check-your-aws-usage-against-service-limits/</link><description>&lt;p&gt;Over the past year or so, at my day job, we&amp;#8217;ve been leveraging &lt;span class="caps"&gt;AWS&lt;/span&gt; more and more, specifically
&lt;a href="https://aws.amazon.com/cloudformation/"&gt;CloudFormation&lt;/a&gt; to manage complete application stacks. One
side-effect of this is that we went through a few periods where we were constantly hitting various
&lt;span class="caps"&gt;AWS&lt;/span&gt; Service Limits - subnet groups, ElastiCache clusters, security groups, and a whole slew of others.
In pretty much all these cases, we weren&amp;#8217;t &lt;em&gt;really&lt;/em&gt; aware of the limits; we (the Tooling and
Automation team) had succeeded in our goal of handing our internal customers the tools to spin up
complete application environments, per-developer, on-demand. And it was wonderful until we hit some
magic number of CloudFormation stacks, at which point almost every day for a week or two we had to
open a new &lt;span class="caps"&gt;AWS&lt;/span&gt; Support ticket to have a different limit increased, and deal with completely broken
deploys until that was done (or send out a frantic &amp;#8220;someone please delete a dev stack&amp;#8221;&amp;nbsp;email).&lt;/p&gt;
&lt;p&gt;Early last month we decided that we had to do something about this. As much as I tried, I couldn&amp;#8217;t
find an existing solution that would monitor our limits and alert us when we approached them; there
were some open source scripts that would do so for a handful of limits (generally just &lt;span class="caps"&gt;EC2&lt;/span&gt; usage),
and the proprietary solutions that I was able to find didn&amp;#8217;t seem much better; none of them stated
that they handle &lt;span class="caps"&gt;VPC&lt;/span&gt; or ElastiCache limits, which had been problematic for us. &lt;span class="caps"&gt;AWS&lt;/span&gt;&amp;#8217;s own
&lt;a href="https://aws.amazon.com/premiumsupport/trustedadvisor/"&gt;Trusted Advisor&lt;/a&gt; has a Service Limits
check available to Business- and Enterprise-level support accounts, but it only monitors 17 of the
94 Service Limits that we identified as relevant to us, and it sends out &lt;em&gt;weekly&lt;/em&gt; alerts. So,
I decided to write something to solve the problem. My co-workers and I have been trying to get
corporate legal approval to release our work publicly under an &lt;span class="caps"&gt;OSI&lt;/span&gt;-approved license for years,
to no avail. I asked my team if they&amp;#8217;d support waiting a while for this work, so I could do it
entirely in my own time, publicly, under an open source license. Happily, they&amp;nbsp;agreed.&lt;/p&gt;
&lt;p&gt;Today I&amp;#8217;m making the first release of &lt;a href="https://github.com/jantman/awslimitchecker"&gt;awslimitchecker&lt;/a&gt;,
an &lt;span class="caps"&gt;AGPL&lt;/span&gt; 3.0-licensed Python tool to calculate your &lt;span class="caps"&gt;AWS&lt;/span&gt; resource usage for various services bound by
&lt;a href="http://awslimitchecker.readthedocs.org/en/latest/limits.html#current-checks"&gt;service limits&lt;/a&gt;, and tell you which ones exceed a given threshold (actually, warning and critical
thresholds). Effective limits are hard-coded to the &lt;a href="http://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html"&gt;published defaults&lt;/a&gt;,
but can be overridden in cases where you&amp;#8217;ve received limit increases, and will be automatically updated
from Trusted Advisor data for all limits that it monitors (if your account includes the full &lt;span class="caps"&gt;TA&lt;/span&gt; checks).
awslimitchecker provides warning and critical thresholds that can be set globally as a percentage of the
limit (defaults are 80% and 99%, respectively) or overridden on a per-limit basis, as either a percentage
or a fixed integer usage&amp;nbsp;value.&lt;/p&gt;
&lt;p&gt;awslimitchecker is available &lt;a href="https://pypi.python.org/pypi/awslimitchecker/0.1.0"&gt;from pypi&lt;/a&gt;.
It is compatible and tested with Python versions 2.6 through 3.4, though the library it uses to communicate
with &lt;span class="caps"&gt;AWS&lt;/span&gt;, &lt;a href="http://boto.readthedocs.org/en/latest/"&gt;boto&lt;/a&gt;, still has a few &lt;span class="caps"&gt;AWS&lt;/span&gt; services which are not python3-compatible.
awslimitchecker includes both a Python module with a &lt;a href="http://awslimitchecker.readthedocs.org/en/latest/awslimitchecker.checker.html"&gt;documented &lt;span class="caps"&gt;API&lt;/span&gt;&lt;/a&gt; for those who
don&amp;#8217;t mind working with Python, and a command line script for those who&amp;nbsp;do.&lt;/p&gt;
&lt;p&gt;The project is still very young, and only being used by one organization, but it&amp;#8217;s proven
stable for us, and I&amp;#8217;m more than happy to accept questions, comments, criticisms,
&lt;a href="https://github.com/jantman/awslimitchecker/issues"&gt;issues/feature requests&lt;/a&gt; and Pull&amp;nbsp;Requests.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Sat, 25 Jul 2015 08:35:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2015-07-25:2015/07/awslimitchecker-check-your-aws-usage-against-service-limits/</guid><category>aws</category><category>ec2</category><category>limits</category><category>python</category><category>awslimitchecker</category><category>cloud</category></item><item><title>Local S3 Server to Acceptance Test Netflix Ice Installation In Isolation</title><link>http://blog.jasonantman.com/2015/05/local-s3-server-to-acceptance-test-netflix-ice-installation-in-isolation/</link><description>&lt;p&gt;At work, we recently started using &lt;a href="http://netflix.github.io/"&gt;Netflix &lt;span class="caps"&gt;OSS&lt;/span&gt;&lt;/a&gt;&amp;#8216;s &lt;a href="https://github.com/Netflix/ice"&gt;Ice&lt;/a&gt; &lt;span class="caps"&gt;AWS&lt;/span&gt; cost analysis tool.
It provides a Java daemon to read and parse &lt;span class="caps"&gt;AWS&lt;/span&gt;&amp;#8217; &lt;a href="http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/detailed-billing-reports.html"&gt;detailed billing reports&lt;/a&gt;
and a web interface to the data (&lt;a href="https://github.com/Netflix/ice/blob/master/README.md#screenshots"&gt;screenshots&lt;/a&gt;). The single biggest feature for us
is the ability to do cost breakdowns (by hour/day/week/month) based on Cost Allocation tags in the detailed billing reports. We tag every billable &lt;span class="caps"&gt;AWS&lt;/span&gt;
resource with the Application Name, Service Class (environment; dev/test/prod) and Responsible Party. Ice lets us configure &amp;#8220;Application Groups&amp;#8221;
based on applications as seen from a business/budgetary standpoint and allow up-to-the-hour data on that available to anyone who needs&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;We spun up the development install of Ice for a few weeks to give it a spin, but once people started complaining that my screen session died and took
Ice with it, it was clear we needed a real, permanent installation. While there is &lt;a href="https://github.com/mdsol/ice_cookbook"&gt;chef&lt;/a&gt; and &lt;a href="https://github.com/Answers4AWS/netflixoss-ansible"&gt;ansible&lt;/a&gt;
code to install and configure Ice, we&amp;#8217;re a Puppet shop, and there wasn&amp;#8217;t anything available that I could find for Puppet. So, I set about writing a
module to install and configure Ice, running in Tomcat behind an Nginx proxy. Like any good modern module, I wanted not only &lt;a href="http://rspec-puppet.com/"&gt;rspec-puppet&lt;/a&gt;
unit tests but also &lt;a href="https://github.com/puppetlabs/beaker"&gt;beaker&lt;/a&gt; acceptance tests. For those unfamiliar, Beaker is an acceptance testing framework for Puppet
that&amp;#8217;s similar to Test Kitchen; it spins up Vagrant machines, runs some code in them, and then uses &lt;a href="http://serverspec.org/"&gt;serverspec&lt;/a&gt; to make assertions about
the state of the system (file contents, running processes, command output, etc.) (side note: if you used Beaker prior to the
&lt;a href="https://github.com/puppetlabs/beaker/blob/master/HISTORY.md#beaker2.0.0"&gt;2.0 release&lt;/a&gt; in December 2014, you should really try it again; they&amp;#8217;ve made some great&amp;nbsp;improvements).&lt;/p&gt;
&lt;h2 id="the-problem"&gt;The&amp;nbsp;Problem&lt;/h2&gt;
&lt;p&gt;This posed a bit of a challenge, as Ice (in addition to being pretty poorly documented) is really designed to run in &lt;span class="caps"&gt;AWS&lt;/span&gt;. Firstly, the very reason we started running Ice was
to get a handle on our fast-growing &lt;span class="caps"&gt;AWS&lt;/span&gt; spend; as a result, we&amp;#8217;re trying hard not to use &lt;span class="caps"&gt;AWS&lt;/span&gt; for small-scale projects that could use existing resources. Second, while our
company very unfortunately doesn&amp;#8217;t have an open source policy and isn&amp;#8217;t releasing anything (hopefully this may be changing soon), we try hard to write generic, forge-quality&amp;nbsp;modules.&lt;/p&gt;
&lt;p&gt;As a result, I wanted to use the default Vagrant/VirtualBox provider for Beaker. To make matters worse, in keeping with the spirit of a community module, I didn&amp;#8217;t
want the acceptance tests to require anything specific to my company, such as an S3 bucket preseeded with our billing data. Ice both reads the detailed billing reports
(one of its three inputs; &lt;span class="caps"&gt;EC2&lt;/span&gt; pricing data and your accounts&amp;#8217; reservation pricing/capacity being the others) and writes state from and to S3. So, this was a bit difficult.
As we don&amp;#8217;t plan on upgrading Ice terribly often, and we wanted to install from the &lt;a href="https://netflixoss.ci.cloudbees.com/job/ice-master/"&gt;cloudbees master builds&lt;/a&gt;, we wanted
acceptance testing of not just the provisioning tooling, but also some basic smoke tests for the application&amp;nbsp;itself.&lt;/p&gt;
&lt;h2 id="the-solution"&gt;The&amp;nbsp;Solution&lt;/h2&gt;
&lt;p&gt;I managed to come up with a working, albeit somewhat Rube Goldberg, method of getting isolated acceptance tests to work. What follows is the gist of how I got Ice
working in complete isolation. The majority of this happens in &lt;code&gt;spec/acceptance/0prerequisite_spec.rb&lt;/code&gt; which runs first and both does the prerequisite setup
and validates that everything is setup right and working for the tests. The following solution is based on the amazingly helpful &lt;a href="https://github.com/jubos/fake-s3"&gt;fakes3&lt;/a&gt;
Ruby gem, the &lt;a href="http://www.apsis.ch/pound/"&gt;Pound&lt;/a&gt; reverse proxy, and some &lt;span class="caps"&gt;SSL&lt;/span&gt; certificate trickery. While my code was specific to Beaker, this should be generic
enough to use with any system acceptance testing&amp;nbsp;tool.&lt;/p&gt;
&lt;h2 id="prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;First, we obtain or create some files that we&amp;#8217;ll need on the test&amp;nbsp;instance:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Grab a relatively recent Detailed Billing With Resources and Tags zipped &lt;span class="caps"&gt;CSV&lt;/span&gt; report from an &lt;span class="caps"&gt;AWS&lt;/span&gt; account of yours (the filename is in the format
    &lt;code&gt;&amp;lt;ACCOUNT NUMBER&amp;gt;-aws-billing-detailed-line-items-with-resources-and-tags-&amp;lt;YYYY&amp;gt;-&amp;lt;MM&amp;gt;.csv&lt;/code&gt;). Manually trim it down to a sufficient sample of data;
    I took a few hours&amp;#8217; worth of data from one day and trimmed it down to just that referencing a few randomly chosen &lt;span class="caps"&gt;RDS&lt;/span&gt; instances, ELBs, on-demand &lt;span class="caps"&gt;EC2&lt;/span&gt;
    instances and reserved &lt;span class="caps"&gt;EC2&lt;/span&gt; instances. I then anonymized the account number, resource IDs, tag values, and anything else identifying. Ice needs billing
    data in order to do anything, so this will serve as our test&amp;nbsp;data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When Ice runs, it attempts to retrieve reserved instance pricing. It appears (I&amp;#8217;ve lost the mailing list or GitHub issue reference) that it&amp;#8217;s typical for
    the first Ice run on an empty S3 work directory to die because these files are missing. As a result, grab the &lt;code&gt;reservation_prices.oneyear.*&lt;/code&gt; files from
    the S3 work bucket of a running/working Ice installation. This will prevent a time-consuming shutdown of Ice on the first&amp;nbsp;run.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generate a self-signed &lt;span class="caps"&gt;SSL&lt;/span&gt; key and certificate for &lt;code&gt;fakebucket.s3.amazonaws.com&lt;/code&gt;. Package them together in a &lt;span class="caps"&gt;PEM&lt;/span&gt; file suitable for use in web servers.
    (Note that most modern S3 &lt;span class="caps"&gt;API&lt;/span&gt; clients accept a full &lt;span class="caps"&gt;URL&lt;/span&gt; to a bucket, as there are now third parties that implement the S3 &lt;span class="caps"&gt;API&lt;/span&gt;. Ice does not; it connects
    to https://&lt;span class="caps"&gt;BUCKETNAME&lt;/span&gt;.s3amazonaws.com. As a result, this &lt;span class="caps"&gt;SSL&lt;/span&gt; foolery is&amp;nbsp;required.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="setup"&gt;Setup&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Install the &lt;a href="https://rubygems.org/gems/fakes3"&gt;fakes3&lt;/a&gt; rubygem; this provides an s3-compliant &lt;span class="caps"&gt;API&lt;/span&gt; backed by local filesystem storage.
    Configure it to run during your tests (I set it up as a systemd service, but there are certainly other ways to do this). Note that
    while fakes3 stores the uploaded data on the local filesystem, it maintains a mapping of known objects in memory; as such, the process
    always starts completely empty, regardless of what&amp;#8217;s in the backing directory on the filesystem. fakes3 allows all &lt;span class="caps"&gt;IAM&lt;/span&gt; credentials,
    so fake ones are fine. It also automatically creates buckets the first time they&amp;#8217;re&amp;nbsp;accessed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install the &lt;a href="http://www.apsis.ch/pound/"&gt;pound&lt;/a&gt; reverse proxy and configure it to listen on port 443 with the &lt;span class="caps"&gt;PEM&lt;/span&gt; file you generated
    earlier, and proxy to fakes3 (which listens by default on port 10000). The &lt;code&gt;ListenHTTPS&lt;/code&gt;section of &lt;code&gt;pound.cfg&lt;/code&gt; will need the
    &lt;code&gt;xHTTP 1&lt;/code&gt; directive in order to enable &lt;span class="caps"&gt;HTTP&lt;/span&gt; verbs other than&amp;nbsp;&lt;span class="caps"&gt;GET&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Setup a local hosts file entry pointing &lt;code&gt;fakebucket.s3.amazonaws.com&lt;/code&gt; at &lt;code&gt;127.0.0.1&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After fakes3 starts, upload your sample billing data file and your reserved instance pricing files to the appropriate paths under a
    bucket called &amp;#8220;fakebucket&amp;#8221;. You can use a tool such as &lt;a href="http://s3tools.org/s3cmd"&gt;s3cmd&lt;/a&gt; to manipulate its contents, and other
    supported tools are listed in &lt;a href="https://github.com/jubos/fake-s3/wiki/Supported-Clients"&gt;the documentation&lt;/a&gt;. This step also serves
    to validate your Pound configuration, which should pass &lt;span class="caps"&gt;HTTPS&lt;/span&gt; port 443 traffic through to fakes3 and allow you to store and
    retrieve&amp;nbsp;objects.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Figure out the path to the trusted keystore for the version of Java that you&amp;#8217;re running Ice under. On CentOS 7 with OpenJDK 1.7.0,
    this was (after a lot of symlinks) &lt;code&gt;/usr/lib/jvm/jre/lib/security/cacerts&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Import your self-signed certificate into the Java keystore as a trusted certificate. This will allow &lt;span class="caps"&gt;SSL&lt;/span&gt; verification to succeed even
    with a self-signed&amp;nbsp;certificate:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;/bin/keytool -importcert -alias fakebucket -file fakebucket.s3.amazonaws.com.crt -keystore /usr/lib/jvm/jre/lib/security/cacerts -storepass changeit -trustcacerts -noprompt
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure &lt;code&gt;ice.properties&lt;/code&gt; for the above. The important and unintuitive parts that I found&amp;nbsp;are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Going by the above examples, your billing and work S3 bucket names should both be&amp;nbsp;&amp;#8220;fakebucket&amp;#8221;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Unless you want to mock out bigger parts of the &lt;span class="caps"&gt;AWS&lt;/span&gt; metadata service, run Ice with
   &lt;code&gt;-Dice.s3AccessKeyId=NotAValidAccessKeyId -Dice.s3SecretKey=NotAValidAwsSecretKeyXxxxxxxxxxxxxxxxxxx&lt;/code&gt;
   in the &lt;code&gt;JAVA_OPTS&lt;/code&gt;. If Ice can&amp;#8217;t retrieve an instance&amp;#8217;s &lt;span class="caps"&gt;IAM&lt;/span&gt; role from the metadata service
   (http://169.254.169.254/latest/meta-data/iam/security-credentials/) and doesn&amp;#8217;t have the
   access and secret keys defined, it won&amp;#8217;t run. Also note that while the documentation is &lt;strong&gt;very&lt;/strong&gt;
   unclear on this, a number of &lt;a href="https://github.com/Netflix/ice/issues/49#issuecomment-23497701"&gt;github issues&lt;/a&gt;
   clarify that these need to be passed in as Java runtime options; they can&amp;#8217;t be put in the properties&amp;nbsp;file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Disable the Reservation Capacity Poller (&lt;code&gt;ice.reservationCapacityPoller=false&lt;/code&gt;). This service
   needs to connect to the &lt;span class="caps"&gt;EC2&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt;, and will cause Ice to die if it&amp;nbsp;can&amp;#8217;t.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For testing purposes, it&amp;#8217;s a lot simpler and less error-prone (as well as being a lot faster) to
   test the processor and reader separately - at least in serial instead of simultaneously in the same&amp;nbsp;instance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once all this is done, running the Ice Processor should retrieve the billing file, process it, and write the processed data to the
fakes3 bucket. Running the Reader should display the data properly. So far I&amp;#8217;ve been unable to find any features (other than the
Reservation Capacity Poller, noted above) that don&amp;#8217;t work with this&amp;nbsp;setup.&lt;/p&gt;
&lt;p&gt;Whether it&amp;#8217;s related to Ice itself or ideas for acceptance testing isolated applications, I hope this can be of use to&amp;nbsp;someone&amp;#8230;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Tue, 05 May 2015 06:45:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2015-05-05:2015/05/local-s3-server-to-acceptance-test-netflix-ice-installation-in-isolation/</guid><category>netflix</category><category>ice</category><category>puppet</category><category>beaker</category><category>acceptance testing</category><category>aws</category><category>s3</category><category>fakes3</category><category>testing</category></item><item><title>AWS CloudFormation and RDS Snapshots</title><link>http://blog.jasonantman.com/2014/12/aws-cloudformation-and-rds-snapshots/</link><description>&lt;p&gt;For the past few weeks, I&amp;#8217;ve been working on spinning up a WordPress stack on Amazon &lt;span class="caps"&gt;AWS&lt;/span&gt;. It&amp;#8217;s intended to be a production application,
so it uses Multi-&lt;span class="caps"&gt;AZ&lt;/span&gt; and a few other tricks to try to achieve relatively high fault tolerance (nothing insane, still in one region). It uses
&lt;span class="caps"&gt;AWS&lt;/span&gt;&amp;#8217;s &lt;a href="https://aws.amazon.com/rds/"&gt;&lt;span class="caps"&gt;RDS&lt;/span&gt;&lt;/a&gt; hosted MySQL service for the database, and the stacks are created with &lt;a href="https://aws.amazon.com/cloudformation/"&gt;CloudFormation&lt;/a&gt;.
Using CloudFormation has been an utterly wonderful experience and being able to spin up an entire stack - multiple autoscaling web server
instances, a database, memcache, etc. with the click of a button in ~20 minutes - is as close to operations nirvana as I&amp;#8217;ve ever&amp;nbsp;gotten.&lt;/p&gt;
&lt;p&gt;One of the last steps for me was to work on database backups and restoration; both restoring the production application&amp;#8217;s database to a
previous snapshot, and restoring a production database snapshot to a test or development stack. This took a few days of testing, and I
wasn&amp;#8217;t able to find much complete information on the nuances of it; there are also some pieces that are not intuitive and (&lt;span class="caps"&gt;IMO&lt;/span&gt;) not
documented well enough in the &lt;span class="caps"&gt;AWS&lt;/span&gt; docs. In short, it&amp;#8217;s horribly easy to blow away your entire database. So, I&amp;#8217;m going to attempt to document
some of what I learned, in the hope that it will benefit&amp;nbsp;others.&lt;/p&gt;
&lt;p&gt;At the bottom of this post I&amp;#8217;ve included some snippets from my CloudFormation template, which I make reference to. It&amp;#8217;s probably worth looking
through that, as I make reference to some of the names used in it. Also, to make sense of this, you should be familiar with the nomenclature used by CloudFormation,
such as the &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html"&gt;template anatomy&lt;/a&gt; and the difference between
parameters and properties, and resources and&amp;nbsp;instances.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; I&amp;#8217;m writing this in mid-December 2014. I&amp;#8217;ll make every effort to keep this updated as I continue working with &lt;span class="caps"&gt;AWS&lt;/span&gt;, but it&amp;#8217;s possible
that some of the problems described herein will be fixed by &lt;span class="caps"&gt;AWS&lt;/span&gt; in the&amp;nbsp;future.&lt;/p&gt;
&lt;h2 id="deletionpolicy-snapshot"&gt;DeletionPolicy&amp;nbsp;Snapshot&lt;/h2&gt;
&lt;p&gt;CloudFormation resources support a &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html"&gt;DeletionPolicy&lt;/a&gt;
attribute that says what to do to a resource when deleted. For &lt;span class="caps"&gt;RDS&lt;/span&gt; instances, &amp;#8220;Snapshot&amp;#8221; is an option, which takes a manual snapshot when the resource
is deleted (manual snapshots, unlike the automated daily ones, live on even after the instance is deleted). Be warned, this only takes effect when you
delete the &lt;strong&gt;entire stack&lt;/strong&gt;. If you make a change to one of the &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html"&gt;DBInstance properties&lt;/a&gt;
that requires a &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks.html#update-replacement"&gt;resource replacement&lt;/a&gt; to
take effect, the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance will be replaced with a new one, and all of the data and automatic snapshots from the old one will be deleted.
That last part deserves repeating: automatic snapshots (the daily ones created by &lt;span class="caps"&gt;RDS&lt;/span&gt;) are tied to the instance; if the instance is replaced
by CloudFormation, you lose all automatic (backup) snapshots with&amp;nbsp;it.&lt;/p&gt;
&lt;h2 id="stack-policy-to-prevent-updates"&gt;Stack Policy to Prevent&amp;nbsp;Updates&lt;/h2&gt;
&lt;p&gt;To prevent &lt;span class="caps"&gt;RDS&lt;/span&gt; data loss from accidentally changing a property of the instance, it&amp;#8217;s wise to add a
&lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html"&gt;stack policy to prevent updates to &lt;span class="caps"&gt;RDS&lt;/span&gt; resources&lt;/a&gt;.
This will prevent CloudFormation from making any changes to the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance at all. Once the stack policy
is in place, in order to make changes to the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance you would either need to set a temporary stack policy
to allow the update (see the &amp;#8220;Updating Protected Resources&amp;#8221; section of the &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html"&gt;stack policy documentation&lt;/a&gt;)
or simply delete and re-create the stack (the recommended method, if it&amp;#8217;s feasible for&amp;nbsp;you).&lt;/p&gt;
&lt;p&gt;Setting a proper stack policy should prevent many of the pitfalls I describe below; however, for completeness,
I&amp;#8217;ve described how &lt;span class="caps"&gt;RDS&lt;/span&gt; resources behave currently without a stack policy protecting them. The
&lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html"&gt;&lt;span class="caps"&gt;AWS&lt;/span&gt;::&lt;span class="caps"&gt;RDS&lt;/span&gt;::DBInstance resource documentation&lt;/a&gt;
describes which properties can be updated in-place (&amp;#8220;Update requires: No interruption&amp;#8221; or &amp;#8220;some interruptions&amp;#8221;)
and which trigger complete replacement of the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance (&amp;#8220;Update requires:&amp;nbsp;replacement&amp;#8221;).&lt;/p&gt;
&lt;p&gt;When you try to update a protected resource through the &lt;code&gt;aws&lt;/code&gt; &lt;span class="caps"&gt;CLI&lt;/span&gt; tools, the update will appear to have worked, but the event
log on the stack will show the update denied and the update will be rolled&amp;nbsp;back.&lt;/p&gt;
&lt;h2 id="restoring-snapshots-and-dbname"&gt;Restoring Snapshots and&amp;nbsp;DBName&lt;/h2&gt;
&lt;p&gt;The DBSnapshotIdentifier property on a MySQL &lt;span class="caps"&gt;RDS&lt;/span&gt; instance specifies a &lt;span class="caps"&gt;RDS&lt;/span&gt; snapshot to restore into the instance. The DBName
property will create a new &lt;span class="caps"&gt;RDS&lt;/span&gt; instance with a single blank database of that name. This bears repeating again; if the DBName
property ever changes, your &lt;span class="caps"&gt;RDS&lt;/span&gt; instance will be replaced with one with a new, blank database of that name.
When creating a MySQL &lt;span class="caps"&gt;RDS&lt;/span&gt; instance, you can specify either the &lt;code&gt;DBName&lt;/code&gt; or &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; property, but not both;
if you attempt to specify both, you&amp;#8217;ll get an error, &amp;#8220;DBName must be null when Restoring for this&amp;nbsp;Engine.&amp;#8221;&lt;/p&gt;
&lt;p&gt;If you want to restore a snapshot to a new &lt;span class="caps"&gt;RDS&lt;/span&gt; instance, you&amp;#8217;ll need to ensure that &lt;code&gt;DBName&lt;/code&gt; is null (either not specified at all, or the special &lt;code&gt;AWS::NoValue&lt;/code&gt;
&lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html"&gt;pseudo parameter&lt;/a&gt;). In order
to do this automatically (and since NoValue/null can&amp;#8217;t be passed in as a template parameter), in the template snippet below I&amp;#8217;ve defined a
&lt;code&gt;UseDbSnapshot&lt;/code&gt; &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html"&gt;condition&lt;/a&gt;
that evaluates to true if the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; parameter is not empty. In my &lt;code&gt;RDS::DBInstance&lt;/code&gt; resource,
I conditionally set (using &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-conditions.html#d0e42982"&gt;&lt;code&gt;Fn::If&lt;/code&gt;&lt;/a&gt;)
the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; and &lt;code&gt;DBName&lt;/code&gt; properties depending on the value of &lt;code&gt;UseDbSnapshot&lt;/code&gt;. The end result is that if the
&lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; parameter is not empty, it is passed in as the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; property of the resource and
the &lt;code&gt;DBName&lt;/code&gt; property is set to &lt;code&gt;AWS::NoValue&lt;/code&gt;. Otherwise, the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; property is set to &lt;code&gt;AWS::NoValue&lt;/code&gt;
and the &lt;code&gt;DBName&lt;/code&gt; parameter is passed in to the corresponding property on the resource (indicating to create a new blank database
of that&amp;nbsp;name).&lt;/p&gt;
&lt;p&gt;To explain this a bit more, CloudFormation seems to have no introspection into &lt;span class="caps"&gt;RDS&lt;/span&gt; instances. The &lt;code&gt;DBName&lt;/code&gt; parameter
exists only in CloudFormation itself, and is only evaluated as a diff from the previous template; if it changes,
CloudFormation spins up a completely new &lt;span class="caps"&gt;RDS&lt;/span&gt; instance with a single blank database of that name. Whether or not
the value of &lt;code&gt;DBName&lt;/code&gt; matches the database currently in the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance (say, restored from a snapshot)
is not known by CloudFormation. In short, if you create an &lt;span class="caps"&gt;RDS&lt;/span&gt; instance from a snapshot of a &amp;#8220;foo&amp;#8221; database
and then change the template to have a &lt;code&gt;DBName&lt;/code&gt; of &amp;#8220;foo&amp;#8221;, CloudFormation will spin up a new &lt;span class="caps"&gt;RDS&lt;/span&gt; instance
with an empty &amp;#8220;foo&amp;#8221;&amp;nbsp;database.&lt;/p&gt;
&lt;h2 id="restoring-to-a-new-stack"&gt;Restoring to a New&amp;nbsp;Stack&lt;/h2&gt;
&lt;p&gt;When restoring to a new stack (stack creation), specify the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; and make sure &lt;code&gt;DBName&lt;/code&gt; is set
to &lt;code&gt;AWS::NoValue&lt;/code&gt; per the previous paragraph (condition in the template). Note that for the life of the stack, you
must continue specifying these parameters (or the &amp;#8220;use previous value&amp;#8221; option for them). Using my example template
below, if you restored into a new stack using the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; parameter and then later updated the stack
and omitted that parameter (which, because of the condition, would set it to &lt;code&gt;NoValue&lt;/code&gt; and set the &lt;code&gt;DBName&lt;/code&gt; parameter
to its default value) the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance would be replaced with a new one with a blank&amp;nbsp;database.&lt;/p&gt;
&lt;p&gt;Because of this, stack updates should always use the previous value for the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; parameter; this can
be done through the &lt;span class="caps"&gt;AWS&lt;/span&gt; Console, or using the &lt;code&gt;aws&lt;/code&gt; command line tools and a parameter like: &lt;code&gt;--parameters ParameterKey=DBSnapshotIdentifier,UsePreviousValue=true&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="restoring-to-an-existing-stack"&gt;Restoring to an Existing&amp;nbsp;Stack&lt;/h2&gt;
&lt;p&gt;Restoring a snapshot to an existing stack is a bit more nuanced. You can&amp;#8217;t restore a snapshot to an existing &lt;span class="caps"&gt;RDS&lt;/span&gt; instance,
you can only restore to a new instance. If you do this through the &lt;span class="caps"&gt;AWS&lt;/span&gt; Console, you&amp;#8217;ll end up with an &lt;span class="caps"&gt;RDS&lt;/span&gt; instance disconnected
from your CloudFormation stack. So the way to do this is more or less the same as restoring to a new stack - specify
the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; parameter for your template, and it will create a new &lt;span class="caps"&gt;RDS&lt;/span&gt; instance with the snapshot. The same
rules about using previous values for the parameters hold true. If you used a stack policy to prevent updates to the &lt;span class="caps"&gt;RDS&lt;/span&gt;
instance, you&amp;#8217;ll need to override that with a temporary policy when doing the&amp;nbsp;restore.&lt;/p&gt;
&lt;p&gt;There are a few caveats to keep in mind with this procedure. The first, obviously, is that there may be some application downtime
when the existing database is replaced with the new (restored) one, and any writes will obviously be lost. Also, this only
works on &lt;span class="caps"&gt;RDS&lt;/span&gt; instances that were created with DBName or a &lt;strong&gt;different&lt;/strong&gt; snapshot. In order to restore the same snapshot to
an &lt;span class="caps"&gt;RDS&lt;/span&gt; resource a second time, you need to first update with the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; parameter removed and have the &lt;span class="caps"&gt;RDS&lt;/span&gt;
instance re-created with an empty database, and then update again with the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; in order to do the restore.
This is because CloudFormation doesn&amp;#8217;t reconcile the current state of instances to determine which actions to take, it only diffs
the updated template against the existing one. If the existing template and the updated one have the same value for the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance&amp;#8217;s
properties (specifically &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt;), CloudFormation determines there are no changes, and does&amp;nbsp;nothing.&lt;/p&gt;
&lt;h2 id="launchconfig-metadata-issues"&gt;LaunchConfig Metadata&amp;nbsp;Issues&lt;/h2&gt;
&lt;p&gt;The &lt;span class="caps"&gt;EC2&lt;/span&gt; instances I&amp;#8217;m using for this project are &amp;#8220;baked&amp;#8221; AMIs (built with &lt;a href="https://packer.io/"&gt;packer.io&lt;/a&gt;) in an Auto-Scaling Group (&lt;span class="caps"&gt;ASG&lt;/span&gt;).
They use a &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-launchconfig.html"&gt;LaunchConfig&lt;/a&gt; to write
out a file on disk with the database connection information for the application. In addition, my &lt;span class="caps"&gt;ASG&lt;/span&gt; has an &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html"&gt;UpdatePolicy&lt;/a&gt;
designed to perform rolling updates (termination and replacement) of &lt;span class="caps"&gt;EC2&lt;/span&gt; instances when their properties&amp;nbsp;change.&lt;/p&gt;
&lt;p&gt;In my testing, I noticed a number of times where updates to the &lt;span class="caps"&gt;RDS&lt;/span&gt; resource that triggered creation of a new &lt;span class="caps"&gt;RDS&lt;/span&gt; instance - such as restoring from
a snapshot in an existing stack, or changing the DBName - properly triggered an update of the LaunchConfig, but failed to trigger
the rolling update of the &lt;span class="caps"&gt;EC2&lt;/span&gt; instances. This left the application in a state where one or more (sometimes all) of the &lt;span class="caps"&gt;EC2&lt;/span&gt;
instances couldn&amp;#8217;t connect to the database, because the file written out by the LaunchConfig still contained the old &lt;span class="caps"&gt;DB&lt;/span&gt; connection
information. For non-production stacks where the entire stack can be deleted and recreated instead of updating the &lt;span class="caps"&gt;RDS&lt;/span&gt; resource,
this shouldn&amp;#8217;t be an issue. Otherwise, if changes are made that replace the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance, I&amp;#8217;d recommend watching for the
LaunchConfig update completion, and manually terminating instances (or increasing the size of the &lt;span class="caps"&gt;ASG&lt;/span&gt; to add instances)
to ensure that the running &lt;span class="caps"&gt;EC2&lt;/span&gt; instances have the updated&amp;nbsp;LaunchConfig.&lt;/p&gt;
&lt;p&gt;Another option would be to use the &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-hup.html"&gt;cfn-hup daemon&lt;/a&gt; to
listen for stack updates that cause changes in resource metadata, and perform the required actions without needing the rolling update
to replace the&amp;nbsp;instances.&lt;/p&gt;
&lt;h2 id="how-to-do-things-using-the-template-below"&gt;How to Do Things Using the Template&amp;nbsp;Below&lt;/h2&gt;
&lt;p&gt;I&amp;#8217;m currently using the &lt;code&gt;aws&lt;/code&gt; command line tools to perform stack creation and updates,
wrapped in a Rakefile (I plan on changing this to use &lt;a href="https://github.com/boto/boto"&gt;boto&lt;/a&gt;
inside a &lt;a href="http://jenkins-ci.org/"&gt;Jenkins&lt;/a&gt; job). What follows is a quick high-level guide
on how to accomplish various &lt;span class="caps"&gt;RDS&lt;/span&gt;-related tasks, using the template snippet&amp;nbsp;below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Build a new stack using a &lt;span class="caps"&gt;RDS&lt;/span&gt; snapshot and a stack policy to prevent updates&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;cat /tmp/stack_policy.json
&lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;Statement&amp;quot;&lt;/span&gt; : &lt;span class="o"&gt;[&lt;/span&gt;
    &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="s2"&gt;&amp;quot;Effect&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;Deny&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;Action&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;Update:*&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;Principal&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;Resource&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;LogicalResourceId/DBInstance&amp;quot;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;,
    &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="s2"&gt;&amp;quot;Effect&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;Allow&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;Action&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;Update:*&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;Principal&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;Resource&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
  &lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;aws cloudformation create-stack --stack-name mystack --stack-policy-body file:///tmp/stack_policy.json --template-body file:///home/myuser/cloudformation_template.json --parameters &lt;span class="nv"&gt;ParameterKey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;DBSnapshotIdentifier,ParameterValue&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;my-snapshot-identifier&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Temporarily override stack policy to allow updates&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a file with the following contents (we&amp;#8217;ll assume it&amp;#8217;s at &lt;code&gt;/home/myuser/allow_all_updates.json&lt;/code&gt;):&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;Statement&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Effect&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Allow&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Action&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Update:*&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Principal&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Resource&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the following &lt;code&gt;aws&lt;/code&gt; commands, append &lt;code&gt;--stack-policy-during-update-body file:///home/myuser/allow_all_updates.json&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Update a stack (built using a &lt;span class="caps"&gt;RDS&lt;/span&gt; snapshot), without losing data&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;aws cloudformation update-stack --stack-name mystack --template-body file:///home/myuser/cloudformation_template.json --parameters &lt;span class="nv"&gt;ParameterKey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;DBSnapshotIdentifier,UsePreviousValue&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Load a &lt;span class="caps"&gt;RDS&lt;/span&gt; snapshot into an existing stack&lt;/strong&gt; (that isn&amp;#8217;t already using this&amp;nbsp;snapshot):&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;aws cloudformation update-stack --stack-name mystack --template-body file:///home/myuser/cloudformation_template.json --parameters &lt;span class="nv"&gt;ParameterKey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;DBSnapshotIdentifier,ParameterValue&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;my-snapshot-identifier&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Load a &lt;span class="caps"&gt;RDS&lt;/span&gt; snapshot into an existing stack again&lt;/strong&gt; (i.e. restore from the same snapshot a second time; this one is a&amp;nbsp;kludge):&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="c"&gt;# re-create the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance with a blank &lt;span class="caps"&gt;DB&lt;/span&gt; (DBName)&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;aws cloudformation update-stack --stack-name mystack --template-body file:///home/myuser/cloudformation_template.json --parameters &lt;span class="nv"&gt;ParameterKey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;DBSnapshotIdentifier,ParameterValue&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="c"&gt;# then load the snapshot again&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;aws cloudformation update-stack --stack-name mystack --template-body file:///home/myuser/cloudformation_template.json --parameters &lt;span class="nv"&gt;ParameterKey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;DBSnapshotIdentifier,ParameterValue&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;my-snapshot-identifier&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="cloudformation-template-snippet"&gt;CloudFormation Template&amp;nbsp;Snippet&lt;/h2&gt;
&lt;p&gt;This is by no means complete, but just includes the parameters, conditions, and resources which I make reference&amp;nbsp;to.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;Parameters&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;DBName&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Default&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;wordpress&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Description&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;The WordPress database name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;String&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;MinLength&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;MaxLength&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;64&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;AllowedPattern&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;[a-zA-Z][a-zA-Z0-9]*&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;ConstraintDescription&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;must begin with a letter and contain only alphanumeric characters.&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;DBSnapshotIdentifier&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Description&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot; The &lt;span class="caps"&gt;RDS&lt;/span&gt; MySQL snapshot name to restore to the new &lt;span class="caps"&gt;DB&lt;/span&gt; instance.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;String&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Default&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;quot;Conditions&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;UseDbSnapshot&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Fn::Not&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;Fn::Equals&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
          &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBSnapshotIdentifier&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
          &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;]&lt;/span&gt;
      &lt;span class="p"&gt;}]&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;quot;Resources&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;DBInstance&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;AWS&lt;/span&gt;::&lt;span class="caps"&gt;RDS&lt;/span&gt;::DBInstance&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Properties&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;DBName&amp;quot;&lt;/span&gt;            &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;Fn::If&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;UseDbSnapshot&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;AWS&lt;/span&gt;::NoValue&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBName&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
          &lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;Engine&amp;quot;&lt;/span&gt;            &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;MySQL&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;MasterUsername&amp;quot;&lt;/span&gt;    &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBUsername&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;DBInstanceClass&amp;quot;&lt;/span&gt;   &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBClass&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;DBSecurityGroups&amp;quot;&lt;/span&gt;  &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBSecurityGroup&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;}],&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;DBSubnetGroupName&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBSubnetGroup&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;AllocatedStorage&amp;quot;&lt;/span&gt;  &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBAllocatedStorage&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;MasterUserPassword&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBPassword&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;DBSnapshotIdentifier&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;Fn::If&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;UseDbSnapshot&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBSnapshotIdentifier&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;AWS&lt;/span&gt;::NoValue&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
          &lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;MultiAZ&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;DeletionPolicy&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Snapshot&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;WebServerGroup&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;AWS&lt;/span&gt;::AutoScaling::AutoScalingGroup&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Properties&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;LaunchConfigurationName&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;LaunchConfig&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;UpdatePolicy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;AutoScalingRollingUpdate&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;MinInstancesInService&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;MaxBatchSize&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;WaitOnResourceSignals&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;true&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;PauseTime&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;PT10M&lt;/span&gt;&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;AutoScalingScheduledAction&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;IgnoreUnmodifiedGroupSizeProperties&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;CreationPolicy&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;ResourceSignal&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;Timeout&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;PT10M&lt;/span&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;Count&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;2&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;LaunchConfig&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;AWS&lt;/span&gt;::AutoScaling::LaunchConfiguration&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Metadata&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;&lt;span class="caps"&gt;AWS&lt;/span&gt;::CloudFormation::Init&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;config&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;files&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
              &lt;span class="nt"&gt;&amp;quot;/opt/wordpress/cloudformation_db.php&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="nt"&gt;&amp;quot;content&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Fn::Join&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
                  &lt;span class="s2"&gt;&amp;quot;&amp;lt;?php\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="s2"&gt;&amp;quot;define(&amp;#39;DB_NAME&amp;#39;,          &amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBName&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;#39;);\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="s2"&gt;&amp;quot;define(&amp;#39;DB_USER&amp;#39;,          &amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBUsername&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;#39;);\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="s2"&gt;&amp;quot;define(&amp;#39;DB_PASSWORD&amp;#39;,      &amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBPassword&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;#39;);\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="s2"&gt;&amp;quot;define(&amp;#39;DB_HOST&amp;#39;,          &amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;Fn::GetAtt&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;DBInstance&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Endpoint.Address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]},&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;#39;);\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
              &lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
          &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Mon, 15 Dec 2014 09:29:00 -0500</pubDate><guid>tag:blog.jasonantman.com,2014-12-15:2014/12/aws-cloudformation-and-rds-snapshots/</guid><category>aws</category><category>cloudformation</category><category>rds</category><category>mysql</category><category>snapshot</category></item><item><title>Watching Jenkins Jobs and CloudFormation Updates with Pushover Notification</title><link>http://blog.jasonantman.com/2014/12/watching-jenkins-jobs-and-cloudformation-updates-with-pushover-notification/</link><description>&lt;p&gt;A few months ago I &lt;a href="http://blog.jasonantman.com/2014/09/pushover-notifications-for-shell-command-completion-and-status/"&gt;posted&lt;/a&gt;
about a script I wrote to send &lt;a href="https://pushover.net/"&gt;Pushover&lt;/a&gt; notifications for shell command&amp;nbsp;completion.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve been doing quite a bit of work lately both with testing some &lt;a href="http://jenkins-ci.org/"&gt;Jenkins&lt;/a&gt; jobs, and spinning up
&lt;span class="caps"&gt;AWS&lt;/span&gt; stacks using &lt;a href="https://aws.amazon.com/cloudformation/"&gt;CloudFormation&lt;/a&gt;. Last week I wrote two python scripts to aid in&amp;nbsp;this.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/jantman/misc-scripts/blob/master/watch_cloudformation.py"&gt;watch_cloudformation.py&lt;/a&gt; uses the popular &lt;a href="https://github.com/boto/boto"&gt;boto&lt;/a&gt;
Python &lt;span class="caps"&gt;AWS&lt;/span&gt; interface to list (and display) the events on a specified CloudFormation stack, and exit 0 or 1 when it finds a (&lt;span class="caps"&gt;CREATE&lt;/span&gt;|&lt;span class="caps"&gt;UPDATE&lt;/span&gt;)_(&lt;span class="caps"&gt;FAILED&lt;/span&gt;|&lt;span class="caps"&gt;COMPLETE&lt;/span&gt;) event.
It also optionally uses &lt;a href="https://pypi.python.org/pypi/python-pushover"&gt;python-pushover&lt;/a&gt; to send the notification to your devices via&amp;nbsp;Pushover.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/jantman/misc-scripts/blob/master/watch_jenkins.py"&gt;watch_jenkins.py&lt;/a&gt; takes the &lt;span class="caps"&gt;URL&lt;/span&gt; to a Jenkins job or build, and uses
&lt;a href="https://pypi.python.org/pypi/python-jenkins"&gt;python-jenkins&lt;/a&gt; to poll the status of the build (or the latest build, if given a Job url)
and display the result when the build finishes, also optionally using python-pushover to send notifications to your&amp;nbsp;device.&lt;/p&gt;
&lt;p&gt;They&amp;#8217;re really quick-and-dirty scripts and might not be suitable for everyone&amp;#8217;s use case, but I took the time to write them,
so hopefully they&amp;#8217;ll be useful to someone&amp;nbsp;else.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Sun, 14 Dec 2014 19:22:00 -0500</pubDate><guid>tag:blog.jasonantman.com,2014-12-14:2014/12/watching-jenkins-jobs-and-cloudformation-updates-with-pushover-notification/</guid><category>script</category><category>pushover</category><category>jenkins</category><category>hudson</category><category>aws</category><category>cloudformation</category></item><item><title>Managing EC2 SSH Keys - An Idea</title><link>http://blog.jasonantman.com/2014/10/managing-ec2-ssh-keys-an-idea/</link><description>&lt;p&gt;At work, we have a bunch of &lt;span class="caps"&gt;EC2&lt;/span&gt; instances (currently hundreds, and growing quickly). We also have a bunch
(probably now around 100, counting contractors) of users. Some users - mainly engineers - need &lt;span class="caps"&gt;SSH&lt;/span&gt; access to all
of the &lt;span class="caps"&gt;EC2&lt;/span&gt; instances; many others only need access to their team&amp;#8217;s instances. While I usually advocate sanity checks
and training over access control for employees, many teams have expressed legitimate concern that they don&amp;#8217;t want
others on their instances; commands that are safe to run in dev/test (like loading test data) might be disastrous
on production instances. So, as part of our automation and tooling team, I&amp;#8217;ve been trying to come up with a way to manage
access to all these instances. Right now we have a single &amp;#8220;bastion&amp;#8221; (a.k.a. jump box / ssh gateway / keyhole) instance, with a single
shared used keyed to access every &lt;span class="caps"&gt;EC2&lt;/span&gt; instance; that doesn&amp;#8217;t scale and doesn&amp;#8217;t meet the security&amp;nbsp;requirements.&lt;/p&gt;
&lt;p&gt;What follows is one theory of mine on how to solve this problem. I&amp;#8217;ve been thinking about this for the past
day; this might not be the Right answer, and it&amp;#8217;s just a theory at this point, but I think it&amp;nbsp;works.&lt;/p&gt;
&lt;h1 id="requirements-and-assumptions"&gt;Requirements and&amp;nbsp;Assumptions&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;We have Active Directory as the one source of authentication/authorization truth, but it&amp;#8217;s only in the corporate
  network. For various reasons both technical and political, accessing it from &lt;span class="caps"&gt;AWS&lt;/span&gt; (whether directly, over &lt;span class="caps"&gt;VPN&lt;/span&gt;,
  via replication, or via data feeds to a separate &lt;span class="caps"&gt;LDAP&lt;/span&gt; infrastructure in &lt;span class="caps"&gt;EC2&lt;/span&gt;) is simply not&amp;nbsp;possible.&lt;/li&gt;
&lt;li&gt;We want to control &lt;span class="caps"&gt;SSH&lt;/span&gt; access to a bunch of instances. Some of them are persistent and some are ephemeral. Some
  are pre-baked AMIs in auto-scaling groups, with &lt;em&gt;no&lt;/em&gt; changes made outside the &lt;span class="caps"&gt;AMI&lt;/span&gt;. Some of them are persistent
  or semi-persistent instances that run Puppet every 30 minutes. Some of them are somewhat special, and can&amp;#8217;t be
  trivially torn&amp;nbsp;down.&lt;/li&gt;
&lt;li&gt;Most of our instances are in a &lt;span class="caps"&gt;VPC&lt;/span&gt;, and have proper security controls which include &lt;span class="caps"&gt;SSH&lt;/span&gt; access from only a specifically
  white-listed range of IPs. However, some instances are in &amp;#8220;&lt;span class="caps"&gt;EC2&lt;/span&gt; Classic&amp;#8221; and have &lt;span class="caps"&gt;SSH&lt;/span&gt; open to the world. We want a
  solution that also protects these&amp;nbsp;instances.&lt;/li&gt;
&lt;li&gt;We&amp;#8217;re mainly concerned with securing access from (a) users inadvertently accessing an instance they shouldn&amp;#8217;t be
  on, (b) outside/untrusted parties, and (c) former employees. We trust our employees within reason, and accept that,
  within our security stance, if an employee &lt;em&gt;really&lt;/em&gt; wants privilege escalation, they&amp;#8217;re going to get it. We&amp;#8217;re not
  overly concerned with protecting against determined, malicious users who already have some access but want&amp;nbsp;more.&lt;/li&gt;
&lt;li&gt;Our current process for security cleanup for former employees is largely based on corporate &lt;span class="caps"&gt;IT&lt;/span&gt; (or is it &lt;span class="caps"&gt;HR&lt;/span&gt;?) turning
  off their &lt;span class="caps"&gt;AD&lt;/span&gt; account. We want to minimize additional steps that need to be completed when someone has access&amp;nbsp;revoked.&lt;/li&gt;
&lt;li&gt;Any solution that we choose needs to be usable with self-service &lt;span class="caps"&gt;AWS&lt;/span&gt;; i.e. any user can spin up their own instances
  or stacks, provided that they use an &lt;span class="caps"&gt;AMI&lt;/span&gt; that is either built by our automation team, or follows guidelines on what
  must be included in all&amp;nbsp;AMIs.&lt;/li&gt;
&lt;li&gt;We have some administrative accounts (Jenkins, as well as some shared privileged accounts on select machines) that need
  unrestricted access to&amp;nbsp;everything.&lt;/li&gt;
&lt;li&gt;Local user accounts aren&amp;#8217;t an option. This would mean running Puppet constantly on every image and/or rebuilding
  every image each time we gain or lose an employee. That would be especially difficult when we occasionally have
  project-based&amp;nbsp;contractors.&lt;/li&gt;
&lt;li&gt;We&amp;#8217;re &lt;span class="caps"&gt;OK&lt;/span&gt; with having a bastion/keyhole server in &lt;span class="caps"&gt;AWS&lt;/span&gt;, we just don&amp;#8217;t want everyone to be able to access&amp;nbsp;everything.&lt;/li&gt;
&lt;li&gt;Our intended network security stance is to have bastion/keyhole servers in &lt;span class="caps"&gt;AWS&lt;/span&gt; (ideally one per &lt;span class="caps"&gt;AZ&lt;/span&gt;), which are only
  reachable via &lt;span class="caps"&gt;SSH&lt;/span&gt; from selected public addresses on our corporate network (which can only be reached by current
  employees with valid, working access). All other instances should only allow &lt;span class="caps"&gt;SSH&lt;/span&gt; from these selected&amp;nbsp;hosts.&lt;/li&gt;
&lt;li&gt;Despite the above, we don&amp;#8217;t want to rely on an instance being properly configured as our only security measure;
  if an instance is incorrectly configured to accept &lt;span class="caps"&gt;SSH&lt;/span&gt; from 0.0.0.0/0, we still want to prevent users whose
  access has been revoked from logging in to the&amp;nbsp;instance.&lt;/li&gt;
&lt;li&gt;We don&amp;#8217;t need access to be granted and revoked immediately. We&amp;#8217;ll assume that in normal operating conditions,
  thirty (30) minutes is a reasonable amount of time to either grant or revoke a user&amp;#8217;s&amp;nbsp;access.&lt;/li&gt;
&lt;li&gt;We want to minimize reliance on our existing corporate infrastructure, so that &lt;span class="caps"&gt;AWS&lt;/span&gt; can be used for business
  continuity&amp;nbsp;purposes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="main-goals"&gt;Main&amp;nbsp;Goals&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Provide users with &lt;span class="caps"&gt;SSH&lt;/span&gt; access to &lt;span class="caps"&gt;EC2&lt;/span&gt; servers; privilege should be able to be granted to a subset of users and/or groups
  for each &amp;#8220;application&amp;#8221;. Users should not be able to access other&amp;nbsp;instances.&lt;/li&gt;
&lt;li&gt;Allow a fixed list of users access to every&amp;nbsp;instance.&lt;/li&gt;
&lt;li&gt;Be able to revoke a user&amp;#8217;s access without rebuilding instances or ssh-in-a-loop&amp;#8217;ing to all of&amp;nbsp;them.&lt;/li&gt;
&lt;li&gt;Many instances are not going to be running Puppet after initial provisioning/&lt;span class="caps"&gt;AMI&lt;/span&gt; creation, so as much as we love Puppet,
  it&amp;#8217;s not an option to solve this&amp;nbsp;problem.&lt;/li&gt;
&lt;li&gt;This should involve a minimum of administrative overhead when a user leaves the&amp;nbsp;company.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="proposed-solution"&gt;Proposed&amp;nbsp;Solution&lt;/h1&gt;
&lt;p&gt;My solution relies on &lt;span class="caps"&gt;SSH&lt;/span&gt; agent forwarding and the &lt;code&gt;AuthorizedKeysCommand&lt;/code&gt; introduced in OpenSSH 6.2 (see &amp;#8220;Limitations&amp;#8221;, below, for more information),
most likely inspired by (or maybe literally the same code)
as the patch formerly used by GitHub. This allows sshd to execute an arbitrary command, passing it the login username, which returns output identical to what would
be in the &lt;code&gt;authorized_keys&lt;/code&gt; file. If none of the keys successfully authenticate the user, authentication continues using the usual &lt;code&gt;AuthorizedKeysFile&lt;/code&gt;. We take
advantage of this feature, in addition to &lt;span class="caps"&gt;SSH&lt;/span&gt; agent forwarding, to provide our granular access control. Public keys are pulled from a central location &lt;em&gt;at login time&lt;/em&gt;
(and cached for a set amount of time); each user has control over their own public keys, and a central process builds sets of public keys authorized to access a given
group of&amp;nbsp;instances.&lt;/p&gt;
&lt;h2 id="infrastructure"&gt;Infrastructure&lt;/h2&gt;
&lt;p&gt;Each &lt;span class="caps"&gt;EC2&lt;/span&gt; instance will be a member of an Access Group, which is a unique identifier for the set of users authorized to access instances
in the group. In implementation, Access Groups will likely just be a tag on &lt;span class="caps"&gt;EC2&lt;/span&gt; instances that maps to a set of predefined values
(see below for&amp;nbsp;more).&lt;/p&gt;
&lt;p&gt;We will have a number of &amp;#8220;bastion&amp;#8221; (keyhole/jump box/&lt;span class="caps"&gt;SSH&lt;/span&gt; gateway) hosts, ideally one in each Availability Zone where we have instances.
These bastion hosts will only be reachable from within our corporate network (or our &lt;span class="caps"&gt;VPN&lt;/span&gt;); therefore, users must have
current access to our corporate network (where we can rely on Active Directory and other systems to handle authorization) in order to
gain access to &lt;span class="caps"&gt;AWS&lt;/span&gt;. All other &lt;span class="caps"&gt;EC2&lt;/span&gt; instances will only be reachable over &lt;span class="caps"&gt;SSH&lt;/span&gt; from one of these bastion hosts. The bastion hosts themselves
will not have &lt;span class="caps"&gt;SSH&lt;/span&gt; keys to access other instances; they will, however, have &lt;span class="caps"&gt;SSH&lt;/span&gt; agent forwarding&amp;nbsp;enabled.&lt;/p&gt;
&lt;p&gt;Users reach &lt;span class="caps"&gt;AWS&lt;/span&gt; instances by SSHing from a host attached to our corporate network (including &lt;span class="caps"&gt;VPN&lt;/span&gt; hosts) to a bastion host in &lt;span class="caps"&gt;EC2&lt;/span&gt;. From there,
they &lt;span class="caps"&gt;SSH&lt;/span&gt; to the destination instance, making use of &lt;span class="caps"&gt;SSH&lt;/span&gt; agent forwarding to use their local key to authenticate to the instance. We get both
a restricted entry point to &lt;span class="caps"&gt;AWS&lt;/span&gt; (the bastion host, which can enforce further security and logging methods) and the ability to authenticate users
using their own personal public keys on the destination&amp;nbsp;instances.&lt;/p&gt;
&lt;p&gt;To make it easier for end-users, we could develop a wrapper script like &lt;a href="https://pypi.python.org/pypi/ec2-ssh"&gt;Instagram&amp;#8217;s ec2-ssh&lt;/a&gt; that
checks for a valid, running ssh agent with keys in it, and then crafts the correct &lt;span class="caps"&gt;SSH&lt;/span&gt; command to land the user on the desired end
host - i.e. something like &lt;code&gt;ec2ssh instance_id&lt;/code&gt; would generate and execute a command like &lt;code&gt;ssh -At bastion_hostname 'ssh instance_ip'&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="on-the-servers-instances"&gt;On the Servers&amp;nbsp;(Instances)&lt;/h2&gt;
&lt;p&gt;Each instance, when initially built/provisioned, is given a &lt;code&gt;get_authorized_keys&lt;/code&gt; script, which is configured to be run by sshd as the
&lt;code&gt;AuthorizedKeysCommand&lt;/code&gt;. This script uses one of the following three public key distribution services to retrieve the authorized public keys
for that instance, which are then echoed on &lt;span class="caps"&gt;STDOUT&lt;/span&gt; and used to authenticate the user. For the sake of simplicity, we&amp;#8217;ll assume (which is
currently the case in our infrastructure) that this script will only run for a single non-root user that is used for logins; it will exit
without returning any output for any other users on the system, effectively preventing logins to&amp;nbsp;them.&lt;/p&gt;
&lt;p&gt;The script will first check for authorized keys cached locally (either on disk or in memory, to be determined). If they&amp;#8217;re found and less
than some age threshold (we&amp;#8217;ll say five minutes), the cached version is returned. This is intended to both reduce latency when performing
multiple sequential logins, and to allow logins to continue functioning through short periods of degraded network connectivity. If no recent
keys are found cached on disk, the script will retrieve them from the configured public key distribution service. If the service does not
return an appropriate response within an acceptable time limit, or is unreachable, the script will exit with no output. This will prevent
logins from users authorized with this method, but will fall through to the standard &lt;code&gt;AuthorizedKeysFile&lt;/code&gt; method. A number of permanent
authorized public keys will be included in each instance, to allow emergency administrative access in the event that the key distribution
service&amp;nbsp;fails.&lt;/p&gt;
&lt;p&gt;If we&amp;#8217;re willing to assume that the instances themselves are trusted (which I think is a valid assumption), the key retrieval script on
each instance will determine the Access Group that the instance belongs to, and then request the authorized keys for that Access Group.
Determination of Access Group will likely be made via user data passed into the instance at provisioning time, or via retrieval of a
tag value for the&amp;nbsp;instance.&lt;/p&gt;
&lt;p&gt;If assuming trust locally on the instance is not sufficient, then the burden of identifying the instance&amp;#8217;s access group is shifted
to the key distribution service (likely by identifying the &lt;span class="caps"&gt;IP&lt;/span&gt; address of the requesting instance, and then using the &lt;span class="caps"&gt;EC2&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt; to
determine which group that instance belongs to). With this solution, only the second alternative key distribution service is&amp;nbsp;feasible.&lt;/p&gt;
&lt;p&gt;If a shorter delay to authorization changes is needed, it would be feasible for instances to also run a separate process
(cronjob, daemon, etc.) that polls the key distribution service at a regular interval to check for updates (i.e.
&lt;span class="caps"&gt;HTTP&lt;/span&gt; &lt;span class="caps"&gt;HEAD&lt;/span&gt;, something &lt;span class="caps"&gt;SQS&lt;/span&gt;-based, etc.) and updates the local cache when they&amp;nbsp;occur.&lt;/p&gt;
&lt;h1 id="public-key-distribution-service"&gt;Public Key Distribution&amp;nbsp;Service&lt;/h1&gt;
&lt;p&gt;Instances will retrieve their authorized public keys from a key distribution service. Three examples&amp;nbsp;follow:&lt;/p&gt;
&lt;h2 id="alternative-1-scalable-architecture-aws-and-local"&gt;Alternative 1 - Scalable Architecture - &lt;span class="caps"&gt;AWS&lt;/span&gt; and&amp;nbsp;Local&lt;/h2&gt;
&lt;p&gt;Keys will be managed by a web-based application (with a complete and documented &lt;span class="caps"&gt;API&lt;/span&gt;) living in the corporate data center.
The application will provide facilities for authorized users (managers, operations) to define new Access Groups and modify
the list of users allowed to access them. Individual end-users will be able to manage their public keys. At a set interval,
a standalone script will retrieve a list of all users defined in the application and check the status of their corporate Active
Directory accounts. Any users whose accounts have been deactivated or locked will be flagged as such in the application. Whenever
a change is made in the application (including a user being flagged as deactivated), all Access Groups that include that user
will have their authorized_keys file (composed of the authorized_keys files of all users with access) written to an S3 bucket
that&amp;#8217;s only writable by the privileged
user running the application. All instances will have &lt;span class="caps"&gt;IAM&lt;/span&gt; roles that allow them to read the&amp;nbsp;bucket.&lt;/p&gt;
&lt;p&gt;This method allows us to provide self-service to users and application administrators, and keeps all data about users within
the corporate network. It provides automatic revocation of access for disabled Active Directory accounts. It does introduce
a delay in revocation of access for disabled &lt;span class="caps"&gt;AD&lt;/span&gt; accounts, but a delay of ~10 minutes is certainly not a concern in our&amp;nbsp;environment.&lt;/p&gt;
&lt;h2 id="alternative-2-scalable-architecture-entirely-in-aws"&gt;Alternative 2 - Scalable Architecture Entirely in&amp;nbsp;&lt;span class="caps"&gt;AWS&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;A similar application exists, but lives entirely in &lt;span class="caps"&gt;AWS&lt;/span&gt;, utilizing its native high availability technologies (i.e. multi-&lt;span class="caps"&gt;AZ&lt;/span&gt;
&lt;span class="caps"&gt;RDS&lt;/span&gt; as a data store). A script still runs in the corporate data center, but all it does is query the &lt;span class="caps"&gt;API&lt;/span&gt; for a list of all
active users, check &lt;span class="caps"&gt;AD&lt;/span&gt; account status, and deactivate any users that no longer have a valid account. Instead of writing the
authorized key files to an S3 bucket, the application serves them directly in real-time. The application could
store keys and data in a &lt;span class="caps"&gt;RDBMS&lt;/span&gt;, or perhaps something like OpenLDAP, depending on which technologies are best known and
what the performance requirements&amp;nbsp;are.&lt;/p&gt;
&lt;p&gt;This is more of an infrastructure challenge and introduces additional points for failure; if the application above (1)
fails, it will only impact &lt;em&gt;changes&lt;/em&gt; to access, whereas if this application fails, all user access (aside from the static
emergency keys) will break. However, this method allows us to control access at a level finer than Access Groups; rules
could be developed based on any attributes of the requesting instance, including (if the latency was allowable) queries
to the &lt;span class="caps"&gt;EC2&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt; for instance-specific&amp;nbsp;data.&lt;/p&gt;
&lt;h2 id="alternative-3-simple-architecture"&gt;Alternative 3 - Simple&amp;nbsp;Architecture&lt;/h2&gt;
&lt;p&gt;A text file stores mappings of Access Groups to the Active Directory users and groups authorized for them. The text file
is manually maintained, stored in version control, and all changes must comply with an access policy and be peer-reviewed.
A script runs at a set interval (let&amp;#8217;s say cron every 5-10 minutes) that reads the user/group mapping, translates groups
to their membership list, and checks the &lt;span class="caps"&gt;AD&lt;/span&gt; account status of every listed user. Users without valid/current/enabled accounts
are removed from the lists in memory. For the remaining (active) users for each Access Group, their &lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt;
file is read. All user&amp;#8217;s authorized_keys files are concatenated together per Access Group, and the result is written to
an S3&amp;nbsp;bucket.&lt;/p&gt;
&lt;p&gt;This is by far the simplest method, and relies on our &lt;span class="caps"&gt;NFS&lt;/span&gt; shared home directories to allow users to manage their public
keys by simply using the standard file. This keeps all user-related data in our corporate data center, and means that we
have only one script and its&amp;#8217; cron job to maintain, rather than a whole application. The text-file-based method of access
control isn&amp;#8217;t terribly scalable, but it should work for the ~100 users that we have to deal with. Checking &lt;span class="caps"&gt;AD&lt;/span&gt; account status
when generating the file should provide a feasible safeguard for users whose corporate accounts are locked/revoked without
requiring someone to remember to also remove them from the &lt;span class="caps"&gt;AWS&lt;/span&gt; user&amp;nbsp;list.&lt;/p&gt;
&lt;h2 id="advantages-over-other-solutions"&gt;Advantages Over Other&amp;nbsp;Solutions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Self-service for users and for managers/administrators of&amp;nbsp;applications.&lt;/li&gt;
&lt;li&gt;No manual intervention when a user leaves the company; users automatically deactivated when their &lt;span class="caps"&gt;AD&lt;/span&gt; account&amp;nbsp;is.&lt;/li&gt;
&lt;li&gt;No cron job or daemon to run on instances, and no centralized process to break key distribution; each instance
  automatically pulls the current authorized keys when a login is&amp;nbsp;attempted.&lt;/li&gt;
&lt;li&gt;Doesn&amp;#8217;t depend on Puppet, so it allows individual applications to use Puppet as they desire, without complication
  or&amp;nbsp;confusion.&lt;/li&gt;
&lt;li&gt;Only depends on centralized (corporate data center) infrastructure for key updates (at most). Failure of connectivity
  between &lt;span class="caps"&gt;AWS&lt;/span&gt; and the corporate data center can be worked around assuming there is an alternate path of access (such as
  a bastion host that allows logins from engineers/managers from a trusted outside&amp;nbsp;host).&lt;/li&gt;
&lt;li&gt;Management of access can be delegated to application owners/managers, while still allowing engineers full&amp;nbsp;access.&lt;/li&gt;
&lt;li&gt;Uses the strength of public key authentication; no passwords to&amp;nbsp;change.&lt;/li&gt;
&lt;li&gt;Ensures that select static trusted keys always have access to instances, even during a failure of the key distribution&amp;nbsp;system.&lt;/li&gt;
&lt;li&gt;In emergencies, keys could be distributed directly to the authorized_keys file, bypassing the distribution system,
  or key file cache lifetime could be&amp;nbsp;increased.&lt;/li&gt;
&lt;li&gt;Can be easily audited by having a scheduled job add a key for all instances, wait ~15 minutes, and then attempt &lt;span class="caps"&gt;SSH&lt;/span&gt;
  connections to all&amp;nbsp;instances.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="trade-offs"&gt;Trade-Offs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Delay between user access addition/removal and updates (though this can be minimized by a shorter cache&amp;nbsp;time).&lt;/li&gt;
&lt;li&gt;Latency during initial login with a cold&amp;nbsp;cache.&lt;/li&gt;
&lt;li&gt;Addition of another system that could&amp;nbsp;break.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="limitations"&gt;Limitations&lt;/h2&gt;
&lt;p&gt;My company is a CentOS shop. The &lt;code&gt;AuthorizedKeysCommand&lt;/code&gt; feature of OpenSSH itself was only released in &lt;a href="http://www.openssh.com/txt/release-6.2"&gt;OpenSSH 6.2&lt;/a&gt;,
on March 22, 2013. A patch for it was backported to the 5.3p1 version of openssh-server in &lt;span class="caps"&gt;RHEL&lt;/span&gt; and CentOS 6. However,
this method will certainly not work on CentOS 5, which is still running OpenSSH 4.3. Be aware that when the new &lt;code&gt;AuthorizedKeysCommand&lt;/code&gt;
feature was backported, the man page was not updated; &lt;code&gt;man sshd_config&lt;/code&gt; is still conspicuously missing these options, and I couldn&amp;#8217;t
find anything in the &lt;span class="caps"&gt;RPM&lt;/span&gt; changelog about it, but the &lt;code&gt;openssh-5.3p1-authorized-keys-command.patch&lt;/code&gt; file is clearly there in the
5.3p1 &lt;span class="caps"&gt;SRPM&lt;/span&gt;, and the options are there but commented out in the &lt;code&gt;sshd_config&lt;/code&gt; it provides. I actually thought this would be near-impossible
to do on CentOS 6 until I found the &lt;code&gt;openssh-ldap&lt;/code&gt; package (in the default repos) and discovered that it uses this&amp;nbsp;feature.&lt;/p&gt;
&lt;p&gt;Also, this solution requires (depending on which alternative is chosen) working access to either S3 or instances serving an application.
Assuming proper configuration (and distribution across AZs) this should be a&amp;nbsp;non-issue.&lt;/p&gt;
&lt;h2 id="accountability"&gt;Accountability&lt;/h2&gt;
&lt;p&gt;If accountability is a concern, we will handle this through detailed logging in every step of the key creation, authorization, distribution
and retrieval process. In addition, all instances will run sshd with &lt;code&gt;LogLevel VERBOSE&lt;/code&gt;, which will log the fingerprint of all public keys
used to connect to the instance. Logs will be written to a secure, append-only&amp;nbsp;medium.&lt;/p&gt;
&lt;h1 id="references-and-further-details"&gt;References and Further&amp;nbsp;Details&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;There is an existing &lt;code&gt;openssh-ldap&lt;/code&gt; package in CentOS that provides instructions on setting up public key storage in an &lt;span class="caps"&gt;LDAP&lt;/span&gt; backend,
  using &lt;code&gt;AuthorizedKeysCommand&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://andriigrytsenko.net/2013/05/authorizedkeyscommand-support-and-centosrhel-5-x/"&gt;Someone said&lt;/a&gt; they successfully built the current
  6.2 OpenSSH for &lt;span class="caps"&gt;RHEL&lt;/span&gt;/Cent&amp;nbsp;5.&lt;/li&gt;
&lt;li&gt;An &lt;span class="caps"&gt;EC2&lt;/span&gt; instance can retrieve its own tags using tools such as &lt;code&gt;awscli&lt;/code&gt; or &lt;code&gt;ec2-api-tools&lt;/code&gt; and an appropriate &lt;span class="caps"&gt;IAM&lt;/span&gt; role set on the&amp;nbsp;instance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="rejected-ideas"&gt;Rejected&amp;nbsp;Ideas&lt;/h1&gt;
&lt;p&gt;While thinking through this I considered and rejected a number of alternate methods. Here are some of&amp;nbsp;them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;While &lt;span class="caps"&gt;SSH&lt;/span&gt;&amp;#8217;s relatively new Certificate support (&lt;span class="caps"&gt;CA&lt;/span&gt;-based) sounds nice, it doesn&amp;#8217;t solve the problem; according to
  &lt;a href="http://neocri.me/documentation/using-ssh-certificate-authentication/"&gt;this blog post&lt;/a&gt; it uses a &lt;span class="caps"&gt;CA&lt;/span&gt; to sign keys,
  but doesn&amp;#8217;t do a &lt;span class="caps"&gt;CRL&lt;/span&gt; lookup, it relies on a RevokedKeys file manually sync&amp;#8217;ed to all servers. So, this poses the
  same problem as managing authorized_keys as a file distributed to&amp;nbsp;instances.&lt;/li&gt;
&lt;li&gt;Managing per-application users or groups on the &lt;span class="caps"&gt;AWS&lt;/span&gt; bastion hosts requires a lot of administrative overhead, and isn&amp;#8217;t really an option for us.
  Though this would be a simple implementation using either groups for each application with private keys group-readable,
  or using per-application users and the proper sudo&amp;nbsp;configuration.&lt;/li&gt;
&lt;li&gt;Prior to finding out about &lt;code&gt;AuthorizedKeysCommand&lt;/code&gt;, my top idea was essentially this same implementation on the
  key distribution server side, but writing it to an S3 bucket, and running a cronjob on each &lt;span class="caps"&gt;EC2&lt;/span&gt; instance to pull
  down the authorized_keys&amp;nbsp;file.&lt;/li&gt;
&lt;li&gt;Just Don&amp;#8217;t - See &lt;a href="https://wblinks.com/notes/aws-tips-i-wish-id-known-before-i-started/"&gt;this blog post&lt;/a&gt;
  as a reference. But the gist is, &amp;#8220;If you have to &lt;span class="caps"&gt;SSH&lt;/span&gt; into your servers, then your automation has failed&amp;#8221;.
  Sure, development and test stacks will be spun up, probably with either a single user&amp;#8217;s key, or a shared
  key. But after that (i.e. in prod), instances are cattle. Logs should be shipped to a central store, CloudWatch
  and/or other monitoring technologies (i.e. NewRelic, Diamond to graphite) should get most of the data that&amp;#8217;s
  needed. I&amp;#8217;m not seriously agreeing to &lt;strong&gt;disable&lt;/strong&gt; &lt;span class="caps"&gt;SSH&lt;/span&gt; access, but to put in place the tools that it&amp;#8217;s needed
  so rarely (on non-dev instances) that it&amp;#8217;s feasible to ask one of a small group of privileged people to
  perform the&amp;nbsp;task.&lt;/li&gt;
&lt;li&gt;Trust our users - If someone can push to master, full control of our systems is just a backtick (or popen) away.
  Recognize that if someone wasn&amp;#8217;t trustworthy, we wouldn&amp;#8217;t hire them. Let everyone access a single bastion host.
  Discourage unauthorized use via strong password policies and other standard security measures
  (perhaps &lt;span class="caps"&gt;OTP&lt;/span&gt;-based two-factor authentication). Discourage malicious use via detailed audit logging, with logs
  shipped to an append-only secure storage&amp;nbsp;location.&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;SUID&lt;/span&gt; wrapper script - All users have &lt;span class="caps"&gt;SSH&lt;/span&gt; access to a bastion host as their normal
  active directory user. They run a &lt;span class="caps"&gt;SUID&lt;/span&gt; wrapper script that has a list of which users are allowed to access
  which &lt;span class="caps"&gt;EC2&lt;/span&gt; instances (or security groups, subnets, etc). When the user calls this script, it checks if the
  specified host is in a group they&amp;#8217;re allowed to access, and if so, SSHes to that host using a key only readable
  by the owner of the script. This is somewhat complex; there&amp;#8217;s a good possibility of security issues with the
  script itself, and it means that we&amp;#8217;re probably only allowing interactive logins - we&amp;#8217;re limited by the
  capabilities of the wrapper script, it&amp;#8217;s not just a normal &lt;span class="caps"&gt;SSH&lt;/span&gt;&amp;nbsp;client.&lt;/li&gt;
&lt;li&gt;Key Pushing- A script runs in one central location. It has a mapping of which users/groups are allowed
  to access which &lt;span class="caps"&gt;EC2&lt;/span&gt; instances. Every X minutes the script runs. It grabs &lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt; for all
  users that are allowed &lt;span class="caps"&gt;EC2&lt;/span&gt; access, and then generates an authorized_keys file for each group of instances.
  The script checks a cache, and if the file has changed for a group of instances since the last run, it queries
  the &lt;span class="caps"&gt;AWS&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt; to determine which instances are in that group, and distributes the authorized_keys file to them.
  The &amp;#8220;distributes&amp;#8221; part would, unfortunately, probably have to be&amp;nbsp;scp.&lt;/li&gt;
&lt;li&gt;Bastion host per application. Users are allowed access to this host either via authorized_keys managed by Puppet,
  or via sudoers rules on a bastion host in the corporate network. But yeah, we&amp;#8217;d end up with a &lt;strong&gt;lot&lt;/strong&gt; of&amp;nbsp;these.&lt;/li&gt;
&lt;li&gt;Various thoughts around &lt;span class="caps"&gt;AD&lt;/span&gt; in the cloud, replicated &lt;span class="caps"&gt;AD&lt;/span&gt; in the cloud, OpenLDAP in the cloud pulling from &lt;span class="caps"&gt;AD&lt;/span&gt;, or
  &lt;span class="caps"&gt;AD&lt;/span&gt; over &lt;span class="caps"&gt;VPN&lt;/span&gt;. These were all rejected either because of corporate security policies, or because relying on internal
  &lt;span class="caps"&gt;AD&lt;/span&gt; for authentication would mean that a data center or connectivity failure also affects&amp;nbsp;&lt;span class="caps"&gt;AWS&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Puppet - We actually &lt;em&gt;run&lt;/em&gt; puppet on every instance. Maybe against our master, maybe masterless with a script
  to deploy some modules before every run. At a minimum, it manages ssh authorized keys for ec2_user. We implement
  some method where each user has a manifest with their own public keys, that they can maintain. Managers can add users
  to the group(s) for their applications, and that users&amp;#8217; keys are automatically deployed. Revoking keys, on the other
  hand, is a bigger problem. This requires some sort of &amp;#8220;this person is going away&amp;#8221; procedure, which currently doesn&amp;#8217;t
  exist (or involve the groups who maintain &lt;span class="caps"&gt;AWS&lt;/span&gt; infrastructure), and would be one more thing for a human to forget.
  There are also instances that have &amp;#8220;special stuff&amp;#8221; going on with Puppet that would complicate&amp;nbsp;this.&lt;/li&gt;
&lt;li&gt;Generate a list of authorized keys, turn it into a manifest, and run puppet masterless on it via a cronjob (pulling
  the manifest from S3). This involves most of the same problems as above, plus means that we have Puppet running
  in two different ways on some instances (triggered via mco against a master, and cron&amp;#8217;ed in apply&amp;nbsp;mode).&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Sat, 04 Oct 2014 11:59:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2014-10-04:2014/10/managing-ec2-ssh-keys-an-idea/</guid><category>ssh</category><category>ec2</category><category>aws</category><category>keys</category><category>public key</category><category>pubkey</category></item></channel></rss>