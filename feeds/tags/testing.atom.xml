<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jason Antman's Blog</title><link href="http://blog.jasonantman.com/" rel="alternate"></link><link href="http://blog.jasonantman.com/feeds/tags/testing.atom.xml" rel="self"></link><id>http://blog.jasonantman.com/</id><updated>2015-05-05T06:45:00-04:00</updated><entry><title>Local S3 Server to Acceptance Test Netflix Ice Installation In Isolation</title><link href="http://blog.jasonantman.com/2015/05/local-s3-server-to-acceptance-test-netflix-ice-installation-in-isolation/" rel="alternate"></link><updated>2015-05-05T06:45:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2015-05-05:2015/05/local-s3-server-to-acceptance-test-netflix-ice-installation-in-isolation/</id><summary type="html">&lt;p&gt;At work, we recently started using &lt;a href="http://netflix.github.io/"&gt;Netflix &lt;span class="caps"&gt;OSS&lt;/span&gt;&lt;/a&gt;&amp;#8216;s &lt;a href="https://github.com/Netflix/ice"&gt;Ice&lt;/a&gt; &lt;span class="caps"&gt;AWS&lt;/span&gt; cost analysis tool.
It provides a Java daemon to read and parse &lt;span class="caps"&gt;AWS&lt;/span&gt;&amp;#8217; &lt;a href="http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/detailed-billing-reports.html"&gt;detailed billing reports&lt;/a&gt;
and a web interface to the data (&lt;a href="https://github.com/Netflix/ice/blob/master/README.md#screenshots"&gt;screenshots&lt;/a&gt;). The single biggest feature for us
is the ability to do cost breakdowns (by hour/day/week/month) based on Cost Allocation tags in the detailed billing reports. We tag every billable &lt;span class="caps"&gt;AWS&lt;/span&gt;
resource with the Application Name, Service Class (environment; dev/test/prod) and Responsible Party. Ice lets us configure &amp;#8220;Application Groups&amp;#8221;
based on applications as seen from a business/budgetary standpoint and allow up-to-the-hour data on that available to anyone who needs&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;We spun up the development install of Ice for a few weeks to give it a spin, but once people started complaining that my screen session died and took
Ice with it, it was clear we needed a real, permanent installation. While there is &lt;a href="https://github.com/mdsol/ice_cookbook"&gt;chef&lt;/a&gt; and &lt;a href="https://github.com/Answers4AWS/netflixoss-ansible"&gt;ansible&lt;/a&gt;
code to install and configure Ice, we&amp;#8217;re a Puppet shop, and there wasn&amp;#8217;t anything available that I could find for Puppet. So, I set about writing a
module to install and configure Ice, running in Tomcat behind an Nginx proxy. Like any good modern module, I wanted not only &lt;a href="http://rspec-puppet.com/"&gt;rspec-puppet&lt;/a&gt;
unit tests but also &lt;a href="https://github.com/puppetlabs/beaker"&gt;beaker&lt;/a&gt; acceptance tests. For those unfamiliar, Beaker is an acceptance testing framework for Puppet
that&amp;#8217;s similar to Test Kitchen; it spins up Vagrant machines, runs some code in them, and then uses &lt;a href="http://serverspec.org/"&gt;serverspec&lt;/a&gt; to make assertions about
the state of the system (file contents, running processes, command output, etc.) (side note: if you used Beaker prior to the
&lt;a href="https://github.com/puppetlabs/beaker/blob/master/HISTORY.md#beaker2.0.0"&gt;2.0 release&lt;/a&gt; in December 2014, you should really try it again; they&amp;#8217;ve made some great&amp;nbsp;improvements).&lt;/p&gt;
&lt;h2 id="the-problem"&gt;The&amp;nbsp;Problem&lt;/h2&gt;
&lt;p&gt;This posed a bit of a challenge, as Ice (in addition to being pretty poorly documented) is really designed to run in &lt;span class="caps"&gt;AWS&lt;/span&gt;. Firstly, the very reason we started running Ice was
to get a handle on our fast-growing &lt;span class="caps"&gt;AWS&lt;/span&gt; spend; as a result, we&amp;#8217;re trying hard not to use &lt;span class="caps"&gt;AWS&lt;/span&gt; for small-scale projects that could use existing resources. Second, while our
company very unfortunately doesn&amp;#8217;t have an open source policy and isn&amp;#8217;t releasing anything (hopefully this may be changing soon), we try hard to write generic, forge-quality&amp;nbsp;modules.&lt;/p&gt;
&lt;p&gt;As a result, I wanted to use the default Vagrant/VirtualBox provider for Beaker. To make matters worse, in keeping with the spirit of a community module, I didn&amp;#8217;t
want the acceptance tests to require anything specific to my company, such as an S3 bucket preseeded with our billing data. Ice both reads the detailed billing reports
(one of its three inputs; &lt;span class="caps"&gt;EC2&lt;/span&gt; pricing data and your accounts&amp;#8217; reservation pricing/capacity being the others) and writes state from and to S3. So, this was a bit difficult.
As we don&amp;#8217;t plan on upgrading Ice terribly often, and we wanted to install from the &lt;a href="https://netflixoss.ci.cloudbees.com/job/ice-master/"&gt;cloudbees master builds&lt;/a&gt;, we wanted
acceptance testing of not just the provisioning tooling, but also some basic smoke tests for the application&amp;nbsp;itself.&lt;/p&gt;
&lt;h2 id="the-solution"&gt;The&amp;nbsp;Solution&lt;/h2&gt;
&lt;p&gt;I managed to come up with a working, albeit somewhat Rube Goldberg, method of getting isolated acceptance tests to work. What follows is the gist of how I got Ice
working in complete isolation. The majority of this happens in &lt;code&gt;spec/acceptance/0prerequisite_spec.rb&lt;/code&gt; which runs first and both does the prerequisite setup
and validates that everything is setup right and working for the tests. The following solution is based on the amazingly helpful &lt;a href="https://github.com/jubos/fake-s3"&gt;fakes3&lt;/a&gt;
Ruby gem, the &lt;a href="http://www.apsis.ch/pound/"&gt;Pound&lt;/a&gt; reverse proxy, and some &lt;span class="caps"&gt;SSL&lt;/span&gt; certificate trickery. While my code was specific to Beaker, this should be generic
enough to use with any system acceptance testing&amp;nbsp;tool.&lt;/p&gt;
&lt;h2 id="prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;First, we obtain or create some files that we&amp;#8217;ll need on the test&amp;nbsp;instance:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Grab a relatively recent Detailed Billing With Resources and Tags zipped &lt;span class="caps"&gt;CSV&lt;/span&gt; report from an &lt;span class="caps"&gt;AWS&lt;/span&gt; account of yours (the filename is in the format
    &lt;code&gt;&amp;lt;ACCOUNT NUMBER&amp;gt;-aws-billing-detailed-line-items-with-resources-and-tags-&amp;lt;YYYY&amp;gt;-&amp;lt;MM&amp;gt;.csv&lt;/code&gt;). Manually trim it down to a sufficient sample of data;
    I took a few hours&amp;#8217; worth of data from one day and trimmed it down to just that referencing a few randomly chosen &lt;span class="caps"&gt;RDS&lt;/span&gt; instances, ELBs, on-demand &lt;span class="caps"&gt;EC2&lt;/span&gt;
    instances and reserved &lt;span class="caps"&gt;EC2&lt;/span&gt; instances. I then anonymized the account number, resource IDs, tag values, and anything else identifying. Ice needs billing
    data in order to do anything, so this will serve as our test&amp;nbsp;data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When Ice runs, it attempts to retrieve reserved instance pricing. It appears (I&amp;#8217;ve lost the mailing list or GitHub issue reference) that it&amp;#8217;s typical for
    the first Ice run on an empty S3 work directory to die because these files are missing. As a result, grab the &lt;code&gt;reservation_prices.oneyear.*&lt;/code&gt; files from
    the S3 work bucket of a running/working Ice installation. This will prevent a time-consuming shutdown of Ice on the first&amp;nbsp;run.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generate a self-signed &lt;span class="caps"&gt;SSL&lt;/span&gt; key and certificate for &lt;code&gt;fakebucket.s3.amazonaws.com&lt;/code&gt;. Package them together in a &lt;span class="caps"&gt;PEM&lt;/span&gt; file suitable for use in web servers.
    (Note that most modern S3 &lt;span class="caps"&gt;API&lt;/span&gt; clients accept a full &lt;span class="caps"&gt;URL&lt;/span&gt; to a bucket, as there are now third parties that implement the S3 &lt;span class="caps"&gt;API&lt;/span&gt;. Ice does not; it connects
    to https://&lt;span class="caps"&gt;BUCKETNAME&lt;/span&gt;.s3amazonaws.com. As a result, this &lt;span class="caps"&gt;SSL&lt;/span&gt; foolery is&amp;nbsp;required.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="setup"&gt;Setup&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Install the &lt;a href="https://rubygems.org/gems/fakes3"&gt;fakes3&lt;/a&gt; rubygem; this provides an s3-compliant &lt;span class="caps"&gt;API&lt;/span&gt; backed by local filesystem storage.
    Configure it to run during your tests (I set it up as a systemd service, but there are certainly other ways to do this). Note that
    while fakes3 stores the uploaded data on the local filesystem, it maintains a mapping of known objects in memory; as such, the process
    always starts completely empty, regardless of what&amp;#8217;s in the backing directory on the filesystem. fakes3 allows all &lt;span class="caps"&gt;IAM&lt;/span&gt; credentials,
    so fake ones are fine. It also automatically creates buckets the first time they&amp;#8217;re&amp;nbsp;accessed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install the &lt;a href="http://www.apsis.ch/pound/"&gt;pound&lt;/a&gt; reverse proxy and configure it to listen on port 443 with the &lt;span class="caps"&gt;PEM&lt;/span&gt; file you generated
    earlier, and proxy to fakes3 (which listens by default on port 10000). The &lt;code&gt;ListenHTTPS&lt;/code&gt;section of &lt;code&gt;pound.cfg&lt;/code&gt; will need the
    &lt;code&gt;xHTTP 1&lt;/code&gt; directive in order to enable &lt;span class="caps"&gt;HTTP&lt;/span&gt; verbs other than&amp;nbsp;&lt;span class="caps"&gt;GET&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Setup a local hosts file entry pointing &lt;code&gt;fakebucket.s3.amazonaws.com&lt;/code&gt; at &lt;code&gt;127.0.0.1&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After fakes3 starts, upload your sample billing data file and your reserved instance pricing files to the appropriate paths under a
    bucket called &amp;#8220;fakebucket&amp;#8221;. You can use a tool such as &lt;a href="http://s3tools.org/s3cmd"&gt;s3cmd&lt;/a&gt; to manipulate its contents, and other
    supported tools are listed in &lt;a href="https://github.com/jubos/fake-s3/wiki/Supported-Clients"&gt;the documentation&lt;/a&gt;. This step also serves
    to validate your Pound configuration, which should pass &lt;span class="caps"&gt;HTTPS&lt;/span&gt; port 443 traffic through to fakes3 and allow you to store and
    retrieve&amp;nbsp;objects.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Figure out the path to the trusted keystore for the version of Java that you&amp;#8217;re running Ice under. On CentOS 7 with OpenJDK 1.7.0,
    this was (after a lot of symlinks) &lt;code&gt;/usr/lib/jvm/jre/lib/security/cacerts&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Import your self-signed certificate into the Java keystore as a trusted certificate. This will allow &lt;span class="caps"&gt;SSL&lt;/span&gt; verification to succeed even
    with a self-signed&amp;nbsp;certificate:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;/bin/keytool -importcert -alias fakebucket -file fakebucket.s3.amazonaws.com.crt -keystore /usr/lib/jvm/jre/lib/security/cacerts -storepass changeit -trustcacerts -noprompt
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure &lt;code&gt;ice.properties&lt;/code&gt; for the above. The important and unintuitive parts that I found&amp;nbsp;are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Going by the above examples, your billing and work S3 bucket names should both be&amp;nbsp;&amp;#8220;fakebucket&amp;#8221;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Unless you want to mock out bigger parts of the &lt;span class="caps"&gt;AWS&lt;/span&gt; metadata service, run Ice with
   &lt;code&gt;-Dice.s3AccessKeyId=NotAValidAccessKeyId -Dice.s3SecretKey=NotAValidAwsSecretKeyXxxxxxxxxxxxxxxxxxx&lt;/code&gt;
   in the &lt;code&gt;JAVA_OPTS&lt;/code&gt;. If Ice can&amp;#8217;t retrieve an instance&amp;#8217;s &lt;span class="caps"&gt;IAM&lt;/span&gt; role from the metadata service
   (http://169.254.169.254/latest/meta-data/iam/security-credentials/) and doesn&amp;#8217;t have the
   access and secret keys defined, it won&amp;#8217;t run. Also note that while the documentation is &lt;strong&gt;very&lt;/strong&gt;
   unclear on this, a number of &lt;a href="https://github.com/Netflix/ice/issues/49#issuecomment-23497701"&gt;github issues&lt;/a&gt;
   clarify that these need to be passed in as Java runtime options; they can&amp;#8217;t be put in the properties&amp;nbsp;file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Disable the Reservation Capacity Poller (&lt;code&gt;ice.reservationCapacityPoller=false&lt;/code&gt;). This service
   needs to connect to the &lt;span class="caps"&gt;EC2&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt;, and will cause Ice to die if it&amp;nbsp;can&amp;#8217;t.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For testing purposes, it&amp;#8217;s a lot simpler and less error-prone (as well as being a lot faster) to
   test the processor and reader separately - at least in serial instead of simultaneously in the same&amp;nbsp;instance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once all this is done, running the Ice Processor should retrieve the billing file, process it, and write the processed data to the
fakes3 bucket. Running the Reader should display the data properly. So far I&amp;#8217;ve been unable to find any features (other than the
Reservation Capacity Poller, noted above) that don&amp;#8217;t work with this&amp;nbsp;setup.&lt;/p&gt;
&lt;p&gt;Whether it&amp;#8217;s related to Ice itself or ideas for acceptance testing isolated applications, I hope this can be of use to&amp;nbsp;someone&amp;#8230;&lt;/p&gt;</summary><category term="netflix"></category><category term="ice"></category><category term="puppet"></category><category term="beaker"></category><category term="acceptance testing"></category><category term="aws"></category><category term="s3"></category><category term="fakes3"></category><category term="testing"></category></entry><entry><title>Some Additional Serverspec Types</title><link href="http://blog.jasonantman.com/2015/03/some-additional-serverspec-types/" rel="alternate"></link><updated>2015-03-14T11:58:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2015-03-14:2015/03/some-additional-serverspec-types/</id><summary type="html">&lt;p&gt;&lt;a href="http://serverspec.org/"&gt;Serverspec&lt;/a&gt; is an rspec-based framework for testing live machines,
and making assertions about things like the output of commands, installed packages, running
services, file content, etc. However, it has a relatively limited and basic set of
&lt;a href="http://serverspec.org/resource_types.html"&gt;Resource Types&lt;/a&gt; that it can test&amp;nbsp;for.&lt;/p&gt;
&lt;p&gt;Before Serverspec completely disabled their GitHub issue tracker (they now seem to have no
issue tracker at all), I&amp;#8217;d suggested some improvements for more advanced resource types,
such as one that can perform an &lt;span class="caps"&gt;HTTP&lt;/span&gt; &lt;span class="caps"&gt;GET&lt;/span&gt; against an application and check the status code
and/or output. I was told in no uncertain terms that this is a task for application integration
testing, and that it&amp;#8217;s &amp;#8220;not what Serverspec is&amp;nbsp;for.&amp;#8221;&lt;/p&gt;
&lt;p&gt;I humbly disagree. I&amp;#8217;ve begun migrating my &lt;a href="https://www.linode.com/"&gt;Linode&lt;/a&gt; to an &lt;span class="caps"&gt;EC2&lt;/span&gt; machine,
using some technology that I&amp;#8217;ve been using at my day job; specifically, Puppet to configure the
machine and &lt;a href="https://packer.io/"&gt;Packer&lt;/a&gt; to build an &lt;span class="caps"&gt;AMI&lt;/span&gt;. Instead of using &lt;a href="http://aws.amazon.com/cloudformation/"&gt;Cloudformation&lt;/a&gt;
to spin up an entire stack, I just use a Rakefile to spin up a new &lt;span class="caps"&gt;EC2&lt;/span&gt; instance, test it, and
swap an &lt;a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html"&gt;Elastic &lt;span class="caps"&gt;IP&lt;/span&gt;&lt;/a&gt;
if all the tests pass. Of course, this requires that I have relatively complete automated testing
of the &lt;span class="caps"&gt;EC2&lt;/span&gt; instance. Stock Serverspec can handle 95% of what I want to test, but there are a few
other, more complex, things that it can&amp;#8217;t. So, I wrote some code to fix&amp;nbsp;that.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ll admit right off the bat that this code doesn&amp;#8217;t really work the way Serverspec is intended to,
but it works and it&amp;#8217;s relatively simple. This largely breaks the abstraction of serverspec using
&lt;a href="https://github.com/serverspec/specinfra"&gt;specinfra&lt;/a&gt; under the hood, but I&amp;#8217;m not sure if that&amp;#8217;s even
a concern (since specinfra seems to be all about testing a running machine via some local command
execution mechanism, and two of the types that I wrote use network &lt;span class="caps"&gt;IO&lt;/span&gt;&amp;nbsp;instead).&lt;/p&gt;
&lt;p&gt;For the time being, I&amp;#8217;ve written three additional &lt;a href="http://www.rubydoc.info/gems/serverspec-extended-types/#Types"&gt;types&lt;/a&gt;
that solve some specific use cases for&amp;nbsp;me:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;a href="http://www.rubydoc.info/gems/serverspec-extended-types/#bitlbee"&gt;bitlbee&lt;/a&gt;
type that connects to a &lt;a href="http://www.bitlbee.org/"&gt;Bitlbee&lt;/a&gt; &lt;span class="caps"&gt;IRC&lt;/span&gt; gateway, authenticates,
and checks the running bitlbee version. It has matchers to check whether or not the connection and
authentication was successful, whether or not it timed out, and the bitlbee version. Parameters for
the type include login nick and password, bitlbee port, and whether or not to connect with&amp;nbsp;&lt;span class="caps"&gt;SSL&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;A &lt;a href="http://www.rubydoc.info/gems/serverspec-extended-types/#http_get"&gt;http_get&lt;/a&gt;
type which connects to the system under test (with a specified port) and issues a
&lt;span class="caps"&gt;HTTP&lt;/span&gt; &lt;span class="caps"&gt;GET&lt;/span&gt; request for a specified path, with a specified &lt;code&gt;Host&lt;/code&gt; header and a timeout (default
10 seconds). Matchers are provided for the response content body (string), response headers
(hash), &lt;span class="caps"&gt;HTTP&lt;/span&gt; status code, and whether or not the request timed out (which also sets a status of&amp;nbsp;0).&lt;/li&gt;
&lt;li&gt;A &lt;a href="http://www.rubydoc.info/gems/serverspec-extended-types/#virtualenv"&gt;virtualenv&lt;/a&gt; type for testing
python &lt;a href="https://virtualenv.pypa.io/en/latest/"&gt;virtualenv&lt;/a&gt;s. It takes the absolute path to the venv
on the filesystem, and uses serverspec&amp;#8217;s built-in file and command execution features to ensure that
the path &amp;#8220;looks like&amp;#8221; a virtualenv, and has matchers for the pip and python versions used in the venv
as well as the &lt;code&gt;pip freeze&lt;/code&gt; output as a hash of requirements and their&amp;nbsp;versions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hopefully this will be of use to someone else as well. As I continue using serverspec, I plan on
adding to the&amp;nbsp;types.&lt;/p&gt;
&lt;p&gt;The code for serverspec-extended-types is on &lt;a href="https://github.com/jantman/serverspec-extended-types/tree/master"&gt;GitHub&lt;/a&gt;
(pull requests and issues welcome) and it&amp;#8217;s packaged and hosted as a &lt;a href="https://rubygems.org/gems/serverspec-extended-types"&gt;ruby gem&lt;/a&gt;.
&lt;a href="http://www.rubydoc.info/gems/serverspec-extended-types/0.0.2#Installation"&gt;Installation&lt;/a&gt; and usage is as simple
as adding it to your Gemfile and &lt;a href="http://www.rubydoc.info/gems/serverspec-extended-types/0.0.2#Usage"&gt;spec_helper&lt;/a&gt;
and then using the types and matchers in your&amp;nbsp;specs.&lt;/p&gt;</summary><category term="serverspec"></category><category term="specinfra"></category><category term="testing"></category><category term="beaker"></category><category term="ruby"></category><category term="rspec"></category><category term="gem"></category></entry><entry><title>RSpec Matcher For Hash Item Value</title><link href="http://blog.jasonantman.com/2015/02/rspec-matcher-for-hash-item-value/" rel="alternate"></link><updated>2015-02-21T10:33:00-05:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2015-02-21:2015/02/rspec-matcher-for-hash-item-value/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Well, this is embarassing. &lt;em&gt;After&lt;/em&gt; I posted this, I received a
&lt;a href="http://blog.jasonantman.com/2015/02/rspec-matcher-for-hash-item-value/#comment-1868422853"&gt;comment&lt;/a&gt;
within a few hours from &lt;a href="https://twitter.com/myronmarston"&gt;@myronmarston&lt;/a&gt;. I&amp;#8217;d originally
written this matcher for RSpec2, and then had to convert my project to use
RSpec3. I just blindly converted this matcher over. Myron pointed out that with
RSpec3&amp;#8217;s &lt;a href="http://rspec.info/blog/2014/01/new-in-rspec-3-composable-matchers/"&gt;composable matchers&lt;/a&gt;,
the functionality of this gem is built-in. It can be done as simply&amp;nbsp;as:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="n"&gt;its&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;:headers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="n"&gt;should&lt;/span&gt; &lt;span class="kp"&gt;include&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;server&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="sr"&gt;/nginx\/1\./&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;As such, I&amp;#8217;ve yanked them gem and am leaving the code and blog post here just for posterity.&lt;/strong&gt;
This should probably not be&amp;nbsp;used.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve been working on a project to move my &lt;a href="http://linode.com"&gt;Linode&lt;/a&gt; &lt;span class="caps"&gt;VM&lt;/span&gt; to an
Amazon &lt;span class="caps"&gt;EC2&lt;/span&gt; instance; the entire instance is a &amp;#8220;baked&amp;#8221; &lt;span class="caps"&gt;AMI&lt;/span&gt; built by Puppet. Since
I&amp;#8217;d like to be able to rebuild this quickly, I&amp;#8217;m using &lt;a href="http://serverspec.org/"&gt;ServerSpec&lt;/a&gt;
(which I have some non-technical issues with, but that&amp;#8217;s a long story) to run full
integration tests of the whole system - check that packages are installed, services
are running, and even make live &lt;span class="caps"&gt;HTTP&lt;/span&gt; requests agsinst&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;One part of this was making live &lt;span class="caps"&gt;HTTP&lt;/span&gt; requests (from inside ServerSpec / &lt;a href="http://rspec.info/"&gt;rspec&lt;/a&gt;)
and checking &lt;span class="caps"&gt;HTTP&lt;/span&gt; response headers. Unfortunately, RSpec doesn&amp;#8217;t have a nice, clean way to make
assertions about a hash&amp;nbsp;item.&lt;/p&gt;
&lt;p&gt;So, I wrote a little Ruby Gem to do this, &lt;a href="https://github.com/jantman/rspec-matcher-hash-item"&gt;rspec-matcher-hash-item&lt;/a&gt;. At the moment it just
has one matcher, &lt;code&gt;have_hash_item_matching&lt;/code&gt;. This operates on a hash, and takes two arguments,
a key and a regex for the value. It allows me to do simple but useful things&amp;nbsp;like:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;  &lt;span class="n"&gt;describe&lt;/span&gt; &lt;span class="n"&gt;http_get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;testapp1.jasonantman.com&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;/testapp1234&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
    &lt;span class="n"&gt;its&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;:headers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="n"&gt;should&lt;/span&gt; &lt;span class="n"&gt;have_hash_item_matching&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;server&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="sr"&gt;/nginx\/1\./&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(The &lt;code&gt;http_get&lt;/code&gt; serverspec matcher is coming in a future gem and blog&amp;nbsp;post)&lt;/p&gt;
&lt;p&gt;Among other things, it prints diffs on&amp;nbsp;failure:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;  2) privatepuppet::ec2::vhosts::testapp1 Http_get &amp;quot;&amp;quot; headers should include key &amp;#39;server&amp;#39; matching /badvalue/
     On host `54.149.198.147&amp;#39;
     Failure/Error: its(:headers) { should have_hash_item_matching(&amp;#39;server&amp;#39;, /badvalue/) }
       expected that hash[server] would match /badvalue/
       Diff:
       @@ -1,2 +1,6 @@
       -[&amp;quot;server&amp;quot;, /badvalue/]
       +&amp;quot;connection&amp;quot; =&amp;gt; &amp;quot;close&amp;quot;,
       +&amp;quot;content-type&amp;quot; =&amp;gt; &amp;quot;text/plain&amp;quot;,
       +&amp;quot;date&amp;quot; =&amp;gt; &amp;quot;Sat, 21 Feb 2015 16:07:42 GMT&amp;quot;,
       +&amp;quot;server&amp;quot; =&amp;gt; &amp;quot;nginx/1.6.2&amp;quot;,
       +&amp;quot;transfer-encoding&amp;quot; =&amp;gt; &amp;quot;chunked&amp;quot;,
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Using the gem is as simple as including it in your &lt;code&gt;Gemfile&lt;/code&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;gem &amp;quot;rspec-matcher-hash-item&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And adding a line to your &lt;code&gt;spec_helper.rb&lt;/code&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;require &amp;#39;rspec_matcher_hash_item&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that the gem is written for&amp;nbsp;RSpec3.&lt;/p&gt;
&lt;p&gt;This is available at &lt;a href="https://rubygems.org/gems/rspec-matcher-hash-item"&gt;rubygems.org&lt;/a&gt; or from
&lt;a href="https://github.com/jantman/rspec-matcher-hash-item"&gt;GitHub&lt;/a&gt;. See GitHub for the&amp;nbsp;documentation.&lt;/p&gt;</summary><category term="ruby"></category><category term="rspec"></category><category term="spec"></category><category term="testing"></category></entry><entry><title>Leap Year Windows Azure Cloud Outage</title><link href="http://blog.jasonantman.com/2012/03/leap-year-windows-azure-cloud-outage/" rel="alternate"></link><updated>2012-03-20T18:12:00-04:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2012-03-20:2012/03/leap-year-windows-azure-cloud-outage/</id><summary type="html">&lt;p&gt;I haven&amp;#8217;t talked about Microsoft in quite a while (mainly because I
don&amp;#8217;t follow mainstream tech news as much anymore), but I happened by a
very interesting &lt;a href="http://blogs.msdn.com/b/windowsazure/archive/2012/03/09/summary-of-windows-azure-service-disruption-on-feb-29th-2012.aspx"&gt;post on the Windows Azure
blog&lt;/a&gt;
the other day. It&amp;#8217;s a very detailed postmortem of the major outage of
the Windows Azure cloud service which occurred from 4:00 &lt;span class="caps"&gt;PM&lt;/span&gt; &lt;span class="caps"&gt;PST&lt;/span&gt; on
February 28&lt;sup&gt;th&lt;/sup&gt; through 2:15 &lt;span class="caps"&gt;AM&lt;/span&gt; on March 1&lt;sup&gt;st&lt;/sup&gt;. Before I get into any of
the details, I should say that it really is a nice, well-done post. And
the fact that they&amp;#8217;re willing to do such a detailed, public postmortem -
and admit the failures that they did - is a step in the right direction
for Microsoft (a company that I don&amp;#8217;t particularly care for, to put it&amp;nbsp;lightly).&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m going to glance over the majority of the post, though I highly
recommend that anyone interested in running web-scale services,
specifically highly available ones, read it. The general overview
(really just the points that are germane to my discussion) is as
follows: An agent running inside the guest &lt;span class="caps"&gt;VM&lt;/span&gt; instances (i.e. domU)
communicates with a counterpart on the host &lt;span class="caps"&gt;OS&lt;/span&gt; (i.e. dom0) over an
encrypted channel, authenticated by certificate. The certs are generated
and passed from the guest to the host when the guest instance is first
initialized, which means when an app is first deployed, scaled out, &lt;span class="caps"&gt;OS&lt;/span&gt;
updated, or when an app is reinitialized on a new host. This cert was
generated for a 1-year validity period, by adding 1 to the integer year
- hence, the generation process failed on February 29th of a leap year,
as the cert end date wasn&amp;#8217;t valid. When the cert generation failed, the
guest agent essentially stopped cold. The host agent waited for a 25
minute timeout, then re-initialized the guest and started over. After
three of these failures, the host assumes there&amp;#8217;s a hardware error
(since the guest would have reported a more specific error otherwise),
declares itself in an error state, and tries to move its current
workload over to another host. Which re-initializes the guests on that
host, thereby causing a chain-reaction of failures in this case. Skip
forward the 2-1/2 hours it took them to identify the problem, and
further 2-1/2 hours to get a fix ready. They fast-tracked their fix to 7
clusters that had already been in the process of a software update, but
ended up with those clusters in an inconsistent state with
incompatibilities between the guest and host networking subsystems,
bringing down previously-unaffected instances on these&amp;nbsp;clusters.&lt;/p&gt;
&lt;p&gt;This whole scenario offers a few important points on both the
development and operations&amp;nbsp;sides:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inputs need error checking, and errors need to be raised.&lt;/strong&gt; So the
first problem here was the failed cert generation. I&amp;#8217;ll leave alone the
fact that, in my opinion, doing math on a the integer year of a date is
a high school or college programming mistake, and never should have been
made by someone doing platform coding for a major company (believe it or
not, 25% of years are leap years &amp;lt;/sarcasm&gt;). If whatever code was
generating the cert was smart enough to check the cert end date validity
and error out, that error should have been pushed up the stack to
somewhere where it could be handled - or, at least, sent to a central
log server that does error&amp;nbsp;trending.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Secure communications when provisioning need an insecure error path.&lt;/strong&gt;
This is somewhat connected to the previous point. If the normal process
of creating a new instance and communicating errors up the stack relies
on certs and authentication or encryption, there should be some method
of communicating errors with &lt;em&gt;that&lt;/em&gt; process either up the stack, or to a
separate event correlation/trending system. Errors with a
certificate-based system are not unusual, and even something as simple
as a vastly incorrect time set on the guests could have caused this same
problem. In environments where management/control communication between
levels of a system are encrypted or authenticated, there should be some
way for lower levels of the system to deliver a meaningful error message
&amp;#8220;somewhere&amp;#8221;. Even if this is just a syslog server or web service that
listens for errors and can escalate a warning when the numbers spike,
it&amp;#8217;s a useful alarm and debugging&amp;nbsp;tool.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Autonomous systems shouldn&amp;#8217;t lightly assume hardware failures.&lt;/strong&gt; It&amp;#8217;s
arrogance for a host system to assume that just because it can&amp;#8217;t
instantiate new guests, a hardware failure exists. This entire incident
is a perfect example that, at least if hardware error indicators are
properly monitored, it&amp;#8217;s more likely for a software problem to be
falsely identified as a hardware problem than the other way around. All
of my points are somewhat related, but I can think of many more reasons
why a new guest can&amp;#8217;t be instantiated that are software-related rather
than&amp;nbsp;hardware-related.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Autonomous control mechanisms need historical trending, and need to
call for help if this looks wrong.&lt;/strong&gt; These host systems tried to
instantiate new guests three times, waiting 25 minutes in between, and
then declared themselves bad and tried to migrate guests to other hosts.
From what I understand, Microsoft got it right in having a &amp;#8220;kill switch&amp;#8221;
that prevented further migration of guests. What they didn&amp;#8217;t have right
was reporting of autonomous actions (guest migration) to a central
location that performs trending. The 25 minute timeout with three
attempts is a great safety feature, but if the status of guest creation
actions was reported to a central server, it would have been much more
quickly apparent that 100% of guest creations in the past, say, 10
minutes, had failed - across all clusters. I know plenty of shops that
do little, if any, real-time analysis and historical comparisons of
their log data. But when systems are designed to perform self-healing
and autonomous actions, it&amp;#8217;s imperative that these actions are tracked
in near-real-time, compared to historical averages, and that deviation
from a baseline is identified and escalated to&amp;nbsp;humans.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Release procedures are more, not less, important when the sky is
falling.&lt;/strong&gt; The extended downtime of the last seven clusters was because
of an improperly &lt;span class="caps"&gt;QA&lt;/span&gt;&amp;#8217;ed update that was pushed out bypassing the normal
release and testing procedures. As a matter of fact, it was so poorly
&lt;span class="caps"&gt;QA&lt;/span&gt;&amp;#8217;ed that the update totally broke networking for the guest VMs, and
was still pushed out. I&amp;#8217;m sure this was more of a management/executive
decision than one made by the actual engineers, but organizations (even
management) need to understand that when the sky is falling, services
are down, and everybody is stressed, it&amp;#8217;s &lt;em&gt;more&lt;/em&gt; likely for mistakes and
oversights to happen, and this is when a proper, well-documented &lt;span class="caps"&gt;QA&lt;/span&gt; and
release procedure (including phased rollout) is &lt;em&gt;most&lt;/em&gt; important.
Failure to follow these procedures results in exactly what happened in
this case - making an already bad problem much&amp;nbsp;worse.&lt;/p&gt;
&lt;p&gt;Even &lt;em&gt;I&lt;/em&gt; can&amp;#8217;t blame Microsoft specifically for all this (though the
whole thing would have been avoided if they just represented timestamps
as integers like the rest of us&amp;#8230;), but it is a good opportunity for us
all to learn from a major incident at a &amp;#8220;pretty well known&amp;#8221;&amp;nbsp;company.&lt;/p&gt;
&lt;p&gt;release procedures are most important when things are already going&amp;nbsp;wrong&lt;/p&gt;</summary><category term="azure"></category><category term="microsoft"></category><category term="outage"></category><category term="release"></category><category term="testing"></category><category term="windows"></category></entry></feed>