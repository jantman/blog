<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jason Antman's Blog - build</title><link href="https://blog.jasonantman.com/" rel="alternate"></link><link href="https://blog.jasonantman.com/feeds/tags/build.atom.xml" rel="self"></link><id>https://blog.jasonantman.com/</id><updated>2020-11-10T15:11:00-05:00</updated><entry><title>On The Creation, Use, and Management of Docker Images</title><link href="https://blog.jasonantman.com/2020/11/on-the-creation-use-and-management-of-docker-images/" rel="alternate"></link><published>2020-11-10T15:11:00-05:00</published><updated>2020-11-10T15:11:00-05:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2020-11-10:/2020/11/on-the-creation-use-and-management-of-docker-images/</id><summary type="html">&lt;p&gt;Some hard-earned thoughts on how to build, manage, and use Docker&amp;nbsp;images.&lt;/p&gt;</summary><content type="html">&lt;div class="toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#on-to-the-topic-docker"&gt;On to the topic: Docker&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#aside-nomenclature"&gt;Aside -&amp;nbsp;Nomenclature&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#what-docker-images-are-and-arent"&gt;What Docker Images Are and&amp;nbsp;Aren&amp;#8217;t&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#what-to-put-in-an-image"&gt;What to put in an&amp;nbsp;image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#configuration"&gt;Configuration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#logging"&gt;Logging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tagging-and-versioning"&gt;Tagging and&amp;nbsp;Versioning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#repeatable-builds"&gt;Repeatable&amp;nbsp;Builds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#no-runtime-downloads"&gt;No Runtime&amp;nbsp;Downloads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#everything-in-source-control-git"&gt;Everything in Source Control&amp;nbsp;(Git)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#labels"&gt;Labels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#add-a-healthcheck"&gt;Add a&amp;nbsp;&lt;span class="caps"&gt;HEALTHCHECK&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#testing-built-images"&gt;Testing Built&amp;nbsp;Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#updates-rollbacks-issue-reproduction-and-disaster-recovery"&gt;Updates, Rollbacks, Issue Reproduction, and Disaster&amp;nbsp;Recovery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#automated-builds"&gt;Automated&amp;nbsp;Builds&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#docker-image-checklist"&gt;Docker Image Checklist&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#footnotes"&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h1 id="introduction"&gt;&lt;a class="toclink" href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;I know that it&amp;#8217;s been ages since I&amp;#8217;ve posted anything here, but frankly, I haven&amp;#8217;t had much interest to. I&amp;#8217;ve been in a strange place personally for the past few years, and especially for much of 2020. I&amp;#8217;ve let much of my public/professional profile languish over the past few years, and I also haven&amp;#8217;t given my open source projects the attention they deserve. I&amp;#8217;m hoping to fix that soon, and hopefully this post is the first step. I&amp;#8217;m also hoping to add a few posts on the non-computer-related &lt;span class="caps"&gt;DIY&lt;/span&gt; carpentry and electronics projects that I&amp;#8217;ve worked on over the past year, as well as my first steps into 3D printing. Hopefully my interest in writing will&amp;nbsp;hold.&lt;/p&gt;
&lt;p&gt;For the past five years I&amp;#8217;ve been working on a team that&amp;#8217;s called Release Engineering, but is best described as a tooling &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; automation development and consulting team (we&amp;#8217;d likely be Developer Enablement anywhere else). Our goal is to provide tooling, consulting services, processes, documentation, and timely advice to a bunch (i.e. over 100) of software development teams. While my team is heavily involved in many aspects of software and infrastructure lifecycle, most of our work is with &lt;span class="caps"&gt;AWS&lt;/span&gt; infrastructure automation and with build/test/deploy pipelines. One common thread that connects the two is the use of Docker images, both as the environment where we run much of our tooling, build, and test processes, as well as the final artifact from our build processes - the blob of ones and zeroes that actually gets deployed and&amp;nbsp;run.&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s safe to say that I don&amp;#8217;t make it through a normal work day without running a bunch of Docker containers and likely building (via automated pipelines, of course) a few. It&amp;#8217;s also safe to say that, after spending five-ish years working on Docker-heavy processes at a large Enterprise, including being intimately involved with developing many of our tools, processes, and standards around Docker, and helping in the management of multiple private Docker Registries, I have some pretty strong opinions and some advice that I find myself passing on time after time. The extreme popularity and accessibility of Docker is wonderful, and has certainly been wonderful for everyone involved in the software and operations lifecycles. However, along with this has also come a large amount of misinformation and poor examples on how to use Docker, and a striking difficulty in finding good information on the hard-earned lessons from using Docker at&amp;nbsp;scale.&lt;/p&gt;
&lt;p&gt;There are some wonderful resources, including the official Docker documentation, for how to run Docker. This post is going to focus on Docker Images and their&amp;nbsp;lifecycle.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;IMPORTANT&lt;/span&gt;:&lt;/strong&gt; Please note that (1) while my language may be rather declarative, &lt;em&gt;this is just my opinion&lt;/em&gt;. It&amp;#8217;s shared by many others in the industry, and it&amp;#8217;s based on hard-learned lessons, but it&amp;#8217;s still an opinion. Also, (2), if you&amp;#8217;re not doing what I describe here, &lt;em&gt;I&amp;#8217;m not by any means saying that you&amp;#8217;re &amp;#8220;doing Docker wrong&amp;#8221;&lt;/em&gt;. These are lessons learned from a company that builds hundreds of Docker images every day, and has thousands of them running at any given time. &lt;strong&gt;In short, this is what I wish someone told us many years&amp;nbsp;ago.&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id="on-to-the-topic-docker"&gt;&lt;a class="toclink" href="#on-to-the-topic-docker"&gt;On to the topic:&amp;nbsp;Docker&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id="aside-nomenclature"&gt;&lt;a class="toclink" href="#aside-nomenclature"&gt;Aside -&amp;nbsp;Nomenclature&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;For those who may not be familiar with the difference, the following are taken from the &lt;a href="https://docs.docker.com/glossary/"&gt;Docker Glossary&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.docker.com/glossary/#image"&gt;Docker image&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Docker images are the basis of containers. An Image is an ordered collection of root filesystem changes and the corresponding execution parameters for use within a container runtime. An image typically contains a union of layered filesystems stacked on top of each other. An image does not have state and it never&amp;nbsp;changes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://docs.docker.com/glossary/#container"&gt;Docker container&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A container is a runtime instance of a docker&amp;nbsp;image.&lt;/p&gt;
&lt;p&gt;A Docker container consists&amp;nbsp;of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Docker&amp;nbsp;image&lt;/li&gt;
&lt;li&gt;An execution&amp;nbsp;environment&lt;/li&gt;
&lt;li&gt;A standard set of&amp;nbsp;instructions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The concept is borrowed from Shipping Containers, which define a standard to ship goods globally. Docker defines a standard to ship&amp;nbsp;software.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="what-docker-images-are-and-arent"&gt;&lt;a class="toclink" href="#what-docker-images-are-and-arent"&gt;What Docker Images Are and&amp;nbsp;Aren&amp;#8217;t&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;To begin with, I&amp;#8217;m going to make some blanket statements about what Docker (mainly in the context of images, and containers) is and&amp;nbsp;isn&amp;#8217;t:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Docker images are &lt;strong&gt;not Virtual Machines (VMs)&lt;/strong&gt;. While they do provide a means to isolate some data and process(es) and a way to start and stop them, they still share a kernel with the underlying operating system and are visible to it, and do not &lt;em&gt;virtualize&lt;/em&gt; anything. They&amp;#8217;re really just a way to group and (somewhat, and only if done very carefully) isolate things from the underlying Linux kernel (or the various compatibility layers for Mac, Windows, and other&amp;nbsp;OSes).&lt;/li&gt;
&lt;li&gt;Docker images are much more analogous to &lt;strong&gt;software packages&lt;/strong&gt;, albeit ones that also know about the environment and some networking, and can have their own storage (volumes). In so far as building and distributing software is concerned, Docker images should mostly be regarded like any other package or&amp;nbsp;artifact.&lt;/li&gt;
&lt;li&gt;Docker containers (and images) should ideally only &lt;a href="https://docs.docker.com/config/containers/multi-service_container/"&gt;run on service per image/container&lt;/a&gt;. Most of the docker ecosystem is built around this concept. While there are many images that don&amp;#8217;t follow this pattern (especially earlier images and proprietary software), you usually wouldn&amp;#8217;t put your application, web server, and database in the same package, and they shouldn&amp;#8217;t be in the same image either. &lt;a href="https://docs.docker.com/compose/"&gt;docker-compose&lt;/a&gt; was specifically designed to aid in this&amp;nbsp;pattern.&lt;/li&gt;
&lt;li&gt;Docker image &lt;strong&gt;tags are package versions.&lt;/strong&gt; No packaging system that I&amp;#8217;m aware of doesn&amp;#8217;t have a concept of a version. With Docker images, that versioning is entirely up to you - by tagging your images. You can tag a single image multiple times, and probably should. Every docker image that&amp;#8217;s built should have at least one completely unique tag, so that same exact image can be used where needed. For versioning, tags that get updated can and should be used (i.e. if you release version &lt;span class="caps"&gt;X.Y.&lt;/span&gt;Z of your image, you can have X and X.Y tags that point to the most recent relevant&amp;nbsp;image).&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;latest&lt;/code&gt; tag is horribly misleading. There is nothing magic or special about &lt;code&gt;latest&lt;/code&gt;; it is simply a convention. If you build and push a newer Docker image and don&amp;#8217;t tag it &lt;code&gt;latest&lt;/code&gt; (and push that tag), your &lt;code&gt;latest&lt;/code&gt; will still point to an older image. Using the &lt;code&gt;latest&lt;/code&gt; tag also removes repeatability when running&amp;nbsp;containers.&lt;/li&gt;
&lt;li&gt;If at all possible, Docker images should not write log files to disk. Docker has pluggable &lt;a href="https://docs.docker.com/config/containers/logging/configure/"&gt;logging drivers&lt;/a&gt;, the simplest being the default which is what&amp;#8217;s displayed by &lt;code&gt;docker logs&lt;/code&gt;. Ideally, all logs should go to &lt;span class="caps"&gt;STDOUT&lt;/span&gt; or &lt;span class="caps"&gt;STDERR&lt;/span&gt; of the container, and the Docker daemon should be configured to handle them&amp;nbsp;appropriately.&lt;/li&gt;
&lt;li&gt;Many of the best practices for working with Dockerized services match up well with the &lt;a href="https://12factor.net/"&gt;12 factor app&lt;/a&gt;&amp;nbsp;guidelines.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you&amp;#8217;re unsure about any of the prescriptive statements I&amp;#8217;ve made, I&amp;#8217;d encourage you to look at the &lt;a href="https://github.com/docker-library/official-images"&gt;docker-library Official images&lt;/a&gt;. These are the official Docker images for many popular programming languages, runtimes, and applications. Most, if not all, of them follow these guidelines. The &lt;a href="https://github.com/docker-library/official-images/blob/master/README.md"&gt;docker-library &lt;span class="caps"&gt;README&lt;/span&gt;&lt;/a&gt; provides some very helpful&amp;nbsp;information.&lt;/p&gt;
&lt;h2 id="what-to-put-in-an-image"&gt;&lt;a class="toclink" href="#what-to-put-in-an-image"&gt;What to put in an&amp;nbsp;image&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A Docker image should only run one service. That may mean more than one &lt;em&gt;process&lt;/em&gt; (in the case of a forking or threaded model), but there should only be one service, and ideally no real init subsystem; just a daemon, perhaps run via a wrapper script. Not only is this in line with the Docker model (see &lt;a href="https://docs.docker.com/config/containers/multi-service_container/"&gt;here&lt;/a&gt; as an official reference), but it also provides many benefits in terms of isolation (especially if using resource limits), monitoring, modularity and management. Even in trivial cases such as a desktop or home computer, it may be desirable to upgrade or restart services separately, move them to different machines on the same network, or swap out one service for another. When multiple services are needed, they should be run as separate containers and connected via &lt;a href="https://docs.docker.com/network/"&gt;Docker networking&lt;/a&gt;. This can be made easy for inexperienced users via &lt;a href="https://docs.docker.com/compose/"&gt;docker-compose&lt;/a&gt;, but retains the flexibility desired by more experienced users with more advanced&amp;nbsp;configurations.&lt;/p&gt;
&lt;h2 id="configuration"&gt;&lt;a class="toclink" href="#configuration"&gt;Configuration&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Configuration should never be included in a Docker image. One of the main advantages of Docker is &amp;#8220;build once, run anywhere&amp;#8221;, where a single image can be used anywhere it&amp;#8217;s needed (i.e. in the test environment, on a developer&amp;#8217;s laptop, and anywhere through production). I won&amp;#8217;t go into the many possibilities in configuration management, but for a general-purpose image, it&amp;#8217;s most desirable to take all configuration via environment variables with sane defaults provided as needed. For more complex scenarios (such as a web server needing many configuration files), it&amp;#8217;s preferable to provide sane defaults built-in to the container and allow overriding them by mounting a directory of configuration files to a known path in the&amp;nbsp;container.&lt;/p&gt;
&lt;p&gt;Under no circumstances should a Docker image be built multiple times for running on different&amp;nbsp;systems/environments/locations.&lt;/p&gt;
&lt;h2 id="logging"&gt;&lt;a class="toclink" href="#logging"&gt;Logging&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Logging should not be written directly to files. This is a bit more difficult if you deviate from the one-service-per-container model, but ideally all logging should be sent to the container&amp;#8217;s &lt;span class="caps"&gt;STDOUT&lt;/span&gt; and &lt;span class="caps"&gt;STDERR&lt;/span&gt; streams. This will be captured by the Docker daemon and available via the &lt;code&gt;docker logs&lt;/code&gt; command if using the default &lt;a href="https://docs.docker.com/config/containers/logging/configure/"&gt;logging driver&lt;/a&gt;, or sent wherever the daemon is configured otherwise. Handling logging this way has a number of benefits including a unified way to view logs (&lt;code&gt;docker logs&lt;/code&gt;), not bloating the container filesystem with log files, not needing to enter into the container to view logs, and compatibility with configurations that send logs to some variety of centralized aggregation, storage, or&amp;nbsp;analysis.&lt;/p&gt;
&lt;p&gt;Furthermore, the &lt;span class="caps"&gt;STDOUT&lt;/span&gt; and &lt;span class="caps"&gt;STDERR&lt;/span&gt; streams should be logically separated either by level (i.e. error messages to &lt;span class="caps"&gt;STDERR&lt;/span&gt;, normal output or info/debug to &lt;span class="caps"&gt;STDOUT&lt;/span&gt;) or by function (i.e. web server access logs to &lt;span class="caps"&gt;STDOUT&lt;/span&gt; and error logs to&amp;nbsp;&lt;span class="caps"&gt;STDERR&lt;/span&gt;).&lt;/p&gt;
&lt;h2 id="tagging-and-versioning"&gt;&lt;a class="toclink" href="#tagging-and-versioning"&gt;Tagging and&amp;nbsp;Versioning&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Docker image tags determine which one of an unlimited number of variations of a single image is used. On the official Docker Hub images, they&amp;#8217;re used both to specify a version (i.e. &lt;code&gt;python:2.7&lt;/code&gt; or &lt;code&gt;python:3.8.2&lt;/code&gt;) as well as to specify an optional variant with some sort of difference, often the base image use (i.e. &lt;code&gt;python:3.8.6-buster&lt;/code&gt; vs &lt;code&gt;python:3.8.6-alpine3.11&lt;/code&gt;). On most official images, a given image has multiple tags; for example, the current &lt;em&gt;newest&lt;/em&gt; stable Python image, &lt;code&gt;python:latest&lt;/code&gt; (what you get if you omit a tag, and just &lt;code&gt;docker pull python&lt;/code&gt;), is also tagged with &lt;code&gt;3.9.0-buster&lt;/code&gt;, &lt;code&gt;3.9-buster&lt;/code&gt;, &lt;code&gt;3-buster&lt;/code&gt;, and &lt;code&gt;buster&lt;/code&gt;. Similarly, the newest official Alpine Linux-based Python image is tagged with eight (8) tags: &lt;code&gt;3.9.0-alpine3.12&lt;/code&gt;, &lt;code&gt;3.9-alpine3.12&lt;/code&gt;, &lt;code&gt;3-alpine3.12&lt;/code&gt;, &lt;code&gt;alpine3.12&lt;/code&gt;, &lt;code&gt;3.9.0-alpine&lt;/code&gt;, &lt;code&gt;3.9-alpine&lt;/code&gt;, &lt;code&gt;3-alpine&lt;/code&gt;, &lt;code&gt;alpine&lt;/code&gt;. The first, and most specific, of these tags (&lt;code&gt;3.9.0-alpine3.12&lt;/code&gt;) is generally unchanging; there will (usually) only be one &lt;code&gt;python:3.9.0-alpine3.12&lt;/code&gt; image published ever. Running this image should always get you an identical container, without any changes from the last time you pulled and ran it, forever. The less-specific tags, however, change over time to point to the newest relevant image. In this way, image tags can be used like version specifiers in many packaging systems; you can choose to install a very specific, unchanging version of some dependency, or you can choose to install the newest version within some&amp;nbsp;range.&lt;/p&gt;
&lt;p&gt;One possible caveat in this is that I&amp;#8217;m not sure if Docker Hub (for official images) enforces that the most specific tag will &lt;em&gt;never&lt;/em&gt; change. In general, I strongly recommend that every image built have at least one completely unique tag that will never be used on another build of that image. This makes it much easier to refer to one specific, unique image, than having to deal with the image digest hash. Many examples that I&amp;#8217;ve seen build this unique tag based on some combination of source control information and timestamp; at my company, our usual practice is to build images with a tag based on the git branch or &lt;span class="caps"&gt;PR&lt;/span&gt; number, short commit &lt;span class="caps"&gt;SHA&lt;/span&gt; that&amp;#8217;s being built, and the current integer timestamp. If a build succeeds and gets released, we&amp;#8217;ll then re-tag the image with the &lt;a href="https://semver.org/"&gt;semver&lt;/a&gt; version&amp;nbsp;number.&lt;/p&gt;
&lt;p&gt;The key point here is that (in most cases) any image that makes it past the initial image build and testing stage should be tagged multiple or many times, to suit the two different purposes of&amp;nbsp;tags:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One completely unique tag, to identify that exact image for all&amp;nbsp;eternity.&lt;/li&gt;
&lt;li&gt;One or more (usually three or more) version tags, to allow specifying a major, major.minor, or major.minor.patch version of the&amp;nbsp;image.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For images that are used solely within an automated build and deploy process, you may choose to completely ignore and never use the &lt;code&gt;latest&lt;/code&gt; tag. For images that at any point will or may be manually pulled by humans, or any public images, the &lt;code&gt;latest&lt;/code&gt; tag should be used and point to the most recent &lt;em&gt;stable&lt;/em&gt;&amp;nbsp;version.&lt;/p&gt;
&lt;h2 id="repeatable-builds"&gt;&lt;a class="toclink" href="#repeatable-builds"&gt;Repeatable&amp;nbsp;Builds&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Especially since the 2020 &lt;a href="https://www.docker.com/blog/scaling-dockers-business-to-serve-millions-more-developers-storage/"&gt;Docker Hub announcement&lt;/a&gt; that images without any activity for six months will be deleted, it is vitally important that Docker image builds be &lt;a href="https://martinfowler.com/bliki/ReproducibleBuild.html"&gt;reproducible&lt;/a&gt;. Even for personal projects or companies with private Docker registries, it is always possible that you&amp;#8217;ll need to revisit an old version, test for a regression, or simply rebuild a system that was happily running an uncommon image for a long time. &lt;strong&gt;Running &lt;code&gt;docker build&lt;/code&gt; on a given Dockerfile, with the same arguments, should produce a functionally identical image on any machine at any point in time.&lt;/strong&gt; As such, all version information for sources (including dependencies) outside of your repository should be either hard-coded explicitly or passed via build-time &lt;a href="https://docs.docker.com/engine/reference/builder/#arg"&gt;&lt;span class="caps"&gt;ARG&lt;/span&gt;&lt;/a&gt; arguments in the Dockerfile. Further, nothing during the build process should ever download un-versioned URLs (i.e. clone from git master, or download the &amp;#8220;latest&amp;#8221; of&amp;nbsp;something).&lt;/p&gt;
&lt;p&gt;Two possible exceptions to this are the base / &lt;span class="caps"&gt;FROM&lt;/span&gt; image, and operating system packages. Ideally the base/&lt;span class="caps"&gt;FROM&lt;/span&gt; image should be defined in the Dockerfile with an immutable tag, but in some cases it&amp;#8217;s desirable to always use the latest image, or to use a less-constrained version tag. In these cases, your build tooling should resolve and record the image used in the &lt;span class="caps"&gt;FROM&lt;/span&gt; tag, and also ideally add this as a label on the final image. Similarly, when dealing with &lt;span class="caps"&gt;OS&lt;/span&gt; packages which may be updated within a given release, it&amp;#8217;s desirable to generate a listing of all installed packages before the build finishes and store this somewhere if needed at a later&amp;nbsp;date.&lt;/p&gt;
&lt;h2 id="no-runtime-downloads"&gt;&lt;a class="toclink" href="#no-runtime-downloads"&gt;No Runtime&amp;nbsp;Downloads&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Dependencies should never be downloaded by a container when it starts up. Doing so breaks repeatability of the image, introduces significant latency to the startup process, and makes possibly-invalid assumptions about network connectivity and available bandwidth. Dependencies that need to be downloaded from the Internet should either be packaged inside the image itself, or downloaded by the user (or some system/automation) and mounted into the&amp;nbsp;container.&lt;/p&gt;
&lt;h2 id="everything-in-source-control-git"&gt;&lt;a class="toclink" href="#everything-in-source-control-git"&gt;Everything in Source Control&amp;nbsp;(Git)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Your Dockerfile, as well as any dependencies for building the image that are not part of another project/artifact/package, should be stored in source control. More often than not these days, that means git. This repository should include the Dockerfile, instructions for building and developing the image, and anything that needs to be &lt;span class="caps"&gt;COPY&lt;/span&gt;&amp;#8217;ed or &lt;span class="caps"&gt;ADD&lt;/span&gt;&amp;#8217;ed into the image. If at all possible, your images should be tagged or labeled with the git commit hash that was used to build them. The repository should have tags (ideally full Releases, if hosting on GitHub or a similar system) at least corresponding to every released image (i.e. &lt;span class="caps"&gt;X.Y.&lt;/span&gt;Z for projects using&amp;nbsp;semver).&lt;/p&gt;
&lt;p&gt;This process has a number of benefits for every image, but especially for public images of open-source&amp;nbsp;projects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It&amp;#8217;s clear how to find the exact source code that was used to build a specific image, so that you or contributors can troubleshoot or modify&amp;nbsp;it.&lt;/li&gt;
&lt;li&gt;It allows easy reproduction and regression of bugs, by running specific versions of the&amp;nbsp;image.&lt;/li&gt;
&lt;li&gt;It enables using automated systems to build the image, such as Docker Hub automated&amp;nbsp;builds.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="labels"&gt;&lt;a class="toclink" href="#labels"&gt;Labels&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Docker images should make use of &lt;a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#label"&gt;LABELs&lt;/a&gt; for storing metadata, passed in as build arguments (&lt;span class="caps"&gt;ARG&lt;/span&gt;). There is a label schema that&amp;#8217;s gaining acceptance at &lt;a href="http://label-schema.org/"&gt;http://label-schema.org/&lt;/a&gt; which provides some very useful suggestions and guidelines. I recommend implementing as many of these as practical. In addition, I often find it useful to include a label with the &lt;span class="caps"&gt;URL&lt;/span&gt; to the automated build that generated the image if possible, as well as to any applicable test results. This can be quite useful when&amp;nbsp;troubleshooting.&lt;/p&gt;
&lt;h2 id="add-a-healthcheck"&gt;&lt;a class="toclink" href="#add-a-healthcheck"&gt;Add a&amp;nbsp;&lt;span class="caps"&gt;HEALTHCHECK&lt;/span&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The Dockerfile &lt;a href="https://docs.docker.com/engine/reference/builder/#healthcheck"&gt;&lt;span class="caps"&gt;HEALTHCHECK&lt;/span&gt;&lt;/a&gt; allows specifying a command to be executed inside running containers at a configurable interval, to check the health of the container. Unless you know for certain that any critical failure in the container will cause it to exit, you should add a health check. This is especially important in any container that uses an init system or runs multiple services. It is generally assumed that, when running Docker containers, they will exit on failure and leave it up to some external system - your service manager, the docker Daemon, etc. - to restart them and track these&amp;nbsp;events.&lt;/p&gt;
&lt;h2 id="testing-built-images"&gt;&lt;a class="toclink" href="#testing-built-images"&gt;Testing Built&amp;nbsp;Images&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;It is generally unwise to assume that a &lt;code&gt;docker build&lt;/code&gt; is correct just because all commands during the build succeeded. Many times I&amp;#8217;ve seen otherwise-good Dockerfiles result in broken images because a library version changed, an executable was moved to a different package, some dependency problem exists, or an exit code went unchecked somewhere deep in a script. The Dockerfile &lt;a href="https://docs.docker.com/engine/reference/builder/#healthcheck"&gt;&lt;span class="caps"&gt;HEALTHCHECK&lt;/span&gt;&lt;/a&gt; is very important, but it only applies to running&amp;nbsp;containers.&lt;/p&gt;
&lt;p&gt;At a minimum, a script should be included in the Dockerfile and executed via &lt;code&gt;RUN&lt;/code&gt; that performs a basic sanity/smoke test of the image before the build is complete. This can be as simple as running noop versions of important commands (such as a &lt;code&gt;--version&lt;/code&gt; flag) to ensure that they execute without error, or adding a sanity check command to your&amp;nbsp;service.&lt;/p&gt;
&lt;p&gt;Taken a step further, if at all possible, you should actually run containers from newly-built images before pushing them to a registry. This can be as simple as ensuring that the container starts up correctly, or running some basic network/functional tests against the service running in it. As a next step, you can run something like &lt;a href="https://serverspec.org/"&gt;serverspec&lt;/a&gt; / &lt;a href="https://testinfra.readthedocs.io/en/latest/"&gt;testinfra&lt;/a&gt; / &lt;a href="https://github.com/aelsabbahy/goss"&gt;goss&lt;/a&gt; against the container to verify the state of files, services, processes, listening ports, etc. Ideally, you should also run your application&amp;#8217;s test suite (what I&amp;#8217;d usually call &amp;#8220;acceptance tests&amp;#8221;), or a representative subset of it, against the&amp;nbsp;container.&lt;/p&gt;
&lt;h2 id="updates-rollbacks-issue-reproduction-and-disaster-recovery"&gt;&lt;a class="toclink" href="#updates-rollbacks-issue-reproduction-and-disaster-recovery"&gt;Updates, Rollbacks, Issue Reproduction, and Disaster&amp;nbsp;Recovery&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In general, assuming the service inside an image is designed correctly, deploying an update should be as simple as pulling and running a newer tag of the same image. Ideally, the service inside the container is written to gracefully handle both upgrades and downgrades (if applicable). This allows our deployment/update and rollback plan to be the same: just stop the container that&amp;#8217;s currently running, and start one of the unique tag that we want&lt;sup&gt;&lt;a href="#foot1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;a name="foot1source"&gt;&lt;/a&gt;. Some orchestration is required when running multiple instances of a service, but the overall concept remains the same: aside from the data we store or pass in (i.e. environment variables, volume mounts, and any external stores such as databases), we should be able to completely and identically recreate a previous state by running the previous tag of the&amp;nbsp;image.&lt;/p&gt;
&lt;p&gt;Since a Docker image is an immutable artifact with a unique identifier (tag), we can run a given image on any other system at any time in the future. This has very significant benefits for troubleshooting (issue reproduction) as well as disaster recovery. So long as we capture the state of all external data before changing the running image (i.e. dump databases, back up any filesystems mounted into the container), it should be possible to recreate a functionally identical system and state at any point in the future. Deploy an upgrade to production and find some really hard-to-troubleshoot bug? Just restore your backups (sanitized of any sensitive data, of course) to a test environment, run the same tag of your image with adjusted configuration, and reproduce the bug safely&lt;sup&gt;&lt;a href="#foot2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;a name="foot2source"&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Similarly, in a disaster recovery context, all we need to do is have a record of how our container was started/run (you&amp;#8217;re using some sort of configuration management for this, right?) and a backup of any volumes that it uses. If the machine it&amp;#8217;s running on catches fire, or gets deleted, two years from now&amp;#8230; just restore the backed-up volumes, and pull and run the container the same way you did before. You should end up with an identical&amp;nbsp;system.&lt;/p&gt;
&lt;h2 id="automated-builds"&gt;&lt;a class="toclink" href="#automated-builds"&gt;Automated&amp;nbsp;Builds&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Finally, while some may disagree, I&amp;#8217;m a staunch advocate that &lt;code&gt;docker build&lt;/code&gt; should &lt;em&gt;never&lt;/em&gt; be directly executed by a human. It is virtually impossible to follow the other guidance here reliably - especially when it comes to tags and labels - by building a &lt;code&gt;docker build&lt;/code&gt; command by hand. Ideally, all builds will be handled by an automated system, which could be anything from Docker Hub automated builds to Jenkins or another &lt;span class="caps"&gt;CI&lt;/span&gt; system, to a shell script. At times, I&amp;#8217;ve gone so far as to add a required &lt;code&gt;ARG never_build_manually&lt;/code&gt; to the Dockerfile to make this clear. For local development a &lt;code&gt;local_build.sh&lt;/code&gt; script can be added to the repository, which sets tags and labels appropriately to ensure that if the image is pushed to a registry it&amp;#8217;s clearly identified as a local development&amp;nbsp;build.&lt;/p&gt;
&lt;p&gt;Enforcing that only automated builds are considered &amp;#8220;real&amp;#8221; builds ensures that the above points - especially repeatability, proper tagging and labeling, and full testing - are always in place for each&amp;nbsp;image.&lt;/p&gt;
&lt;h1 id="docker-image-checklist"&gt;&lt;a class="toclink" href="#docker-image-checklist"&gt;Docker Image&amp;nbsp;Checklist&lt;/a&gt;&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Your Dockerfile follows the &lt;a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/"&gt;Best practices for writing Dockerfiles&lt;/a&gt;, and your service is as close to a &lt;a href="https://12factor.net/"&gt;12 factor app&lt;/a&gt; as&amp;nbsp;possible.&lt;/li&gt;
&lt;li&gt;Your image/container &lt;a href="#what-to-put-in-an-image"&gt;only runs one service&lt;/a&gt;, ideally without any sort of init&amp;nbsp;subsystem.&lt;/li&gt;
&lt;li&gt;Your image takes its &lt;a href="#configuration"&gt;configuration&lt;/a&gt; via environment variables, or if need be, via config files mounted into the running container (with sane defaults&amp;nbsp;provided).&lt;/li&gt;
&lt;li&gt;In no circumstances do you build different images for different environments or deployment&amp;nbsp;scenarios.&lt;/li&gt;
&lt;li&gt;The service running in your images &lt;a href="#logging"&gt;logs&lt;/a&gt; to &lt;span class="caps"&gt;STDOUT&lt;/span&gt;/&lt;span class="caps"&gt;STDERR&lt;/span&gt;, to be handled by the Docker daemon, and not to files on disk. Ideally, out and err have some logical&amp;nbsp;separation.&lt;/li&gt;
&lt;li&gt;Your image is &lt;a href="#tagging-and-versioning"&gt;tagged&lt;/a&gt; with both a unique/immutable tag per image as well as relevant version tags (ideally following semver, and allowing use of major or major.minor images). All images should be able to be referenced by a unique tag, for all time to&amp;nbsp;come.&lt;/li&gt;
&lt;li&gt;For released software or open-source projects, the &lt;code&gt;latest&lt;/code&gt; tag points to the most recent stable&amp;nbsp;release.&lt;/li&gt;
&lt;li&gt;Within the constraints of base images, &lt;span class="caps"&gt;OS&lt;/span&gt; packages, etc. any given image is &lt;a href="#repeatable-builds"&gt;repeatable&lt;/a&gt; and can be rebuilt from source control at any point in the&amp;nbsp;future.&lt;/li&gt;
&lt;li&gt;When run as a container, your image does not &lt;a href="#no-runtime-downloads"&gt;download dependencies at runtime&lt;/a&gt;. The image should include everything (except data) required to&amp;nbsp;work.&lt;/li&gt;
&lt;li&gt;Everything needed to build the image (aside from external artifacts) is &lt;a href="#everything-in-source-control-git"&gt;included in source control&lt;/a&gt;, and versioned along with the Dockerfile. It is possible to tie an image to the commit / source state that it was generated from, and to tie a tag/release in source control to the corresponding&amp;nbsp;image.&lt;/li&gt;
&lt;li&gt;Your image makes use of &lt;a href="#labels"&gt;labels&lt;/a&gt; on the image to store metadata about it, its contents, and the build&amp;nbsp;process.&lt;/li&gt;
&lt;li&gt;Your image includes a &lt;a href="#add-a-healthcheck"&gt;healthcheck&lt;/a&gt; so that the Docker daemon can tell if containers are in a healthy, functional&amp;nbsp;state.&lt;/li&gt;
&lt;li&gt;The process for building your image includes running &lt;a href="#testing-built-images"&gt;tests&lt;/a&gt; against it, and ideally also against a running&amp;nbsp;container.&lt;/li&gt;
&lt;li&gt;Data used by your image is isolated in volumes, so that users can &lt;a href="#updates-rollbacks-issue-reproduction-and-disaster-recovery"&gt;roll back and forward, reproduce issues, and perform disaster recovery&lt;/a&gt; via&amp;nbsp;tags.&lt;/li&gt;
&lt;li&gt;The process for building your image is &lt;a href="#automated-builds"&gt;automated&lt;/a&gt;, and manually/locally built images are easily identified as development / non-release&amp;nbsp;artifacts.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="footnotes"&gt;&lt;a class="toclink" href="#footnotes"&gt;Footnotes&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a name="foot1"&gt;1&lt;/a&gt;: This is a gross simplification, describing a lab or desktop environment or the most trivial and unimportant service. For anything else, even in the lowest environments, you&amp;#8217;d most likely have multiple containers running of the same service, and would use a zero-downtime deployment method such as blue/green or progressive traffic shifting. But at an extremely high level, the idea is the same: that you can roll backwards and forwards through container versions. &lt;a href="#foot1source"&gt;back to&amp;nbsp;source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name="foot2"&gt;2&lt;/a&gt;: I&amp;#8217;ll admit that this is rather optimistic, and makes a lot of assumptions. This may end up being &lt;em&gt;much&lt;/em&gt; more complicated than &amp;#8220;just restore your backups and run it in test&amp;#8221;, but it&amp;#8217;s still much simpler than what this process looked like a decade ago. &lt;a href="#foot2source"&gt;back to&amp;nbsp;source&lt;/a&gt;&lt;/p&gt;</content><category term="docker"></category><category term="build"></category><category term="deploy"></category><category term="image"></category><category term="container"></category></entry><entry><title>Modern (0.10.x+) NodeJS RPMs on CentOS/REHL 5 and 6</title><link href="https://blog.jasonantman.com/2013/06/modern-0-10-x-nodejs-rpms-on-centosrehl-5-and-6/" rel="alternate"></link><published>2013-06-06T20:47:00-04:00</published><updated>2013-06-06T20:47:00-04:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2013-06-06:/2013/06/modern-0-10-x-nodejs-rpms-on-centosrehl-5-and-6/</id><summary type="html">&lt;p&gt;I posted back in January about &lt;a href="/2013/01/rpm-spec-files-for-nodejs-0-9-5-and-v8-on-centos-5/"&gt;&lt;span class="caps"&gt;RPM&lt;/span&gt; Spec Files for nodejs 0.9.5 and v8
on CentOS
6&lt;/a&gt;. In
that post I also said that I was unable to get recent NodeJS to build on
CentOS 5 because of a long chain of dependencies including node-gyp, v8,
http-parser, glibc …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I posted back in January about &lt;a href="/2013/01/rpm-spec-files-for-nodejs-0-9-5-and-v8-on-centos-5/"&gt;&lt;span class="caps"&gt;RPM&lt;/span&gt; Spec Files for nodejs 0.9.5 and v8
on CentOS
6&lt;/a&gt;. In
that post I also said that I was unable to get recent NodeJS to build on
CentOS 5 because of a long chain of dependencies including node-gyp, v8,
http-parser, glibc, etc. I said I couldn&amp;#8217;t get it to build. Well, I have
good news for both distro&amp;nbsp;versions.&lt;/p&gt;
&lt;p&gt;On the CentOS/&lt;span class="caps"&gt;RHEL&lt;/span&gt; 6 side, thanks to a lot of work by &lt;span class="caps"&gt;T. C.
&lt;/span&gt;Hollingsworth and others, NodeJS 0.10.5 is currently in the official
&lt;a href="http://fedoraproject.org/wiki/EPEL"&gt;&lt;span class="caps"&gt;EPEL&lt;/span&gt;&lt;/a&gt; repositories. They seem to be
keeping the packages pretty current, but if you need newer, you can
always grab the SRPMs from &lt;span class="caps"&gt;EPEL&lt;/span&gt; and build the newer versions. This is
great, because it means I no longer need to maintain the spec files and
do my own builds. I don&amp;#8217;t think I really did anything to help get this
package in &lt;span class="caps"&gt;EPEL&lt;/span&gt;, other than ping a few people and comment on a few&amp;nbsp;tickets.&lt;/p&gt;
&lt;p&gt;For CentOS/&lt;span class="caps"&gt;RHEL&lt;/span&gt; 5, I finally have packages, but they&amp;#8217;re not exactly
pretty. The dependency solving issues still stand; they&amp;#8217;re rooted at the
dependency of node-gyp which requires the v8 C++ JavaScript library, and
is required to compile shared object addons. The best solution that I
(and a few others) could find is simply not to build node-gyp, and not
to have support for addons or package any addons; we just have the
binaries that NodeJS&amp;#8217;s Makefile creates, and everything else is
interpreted. A &lt;a href="https://twitter.com/toxigenicpoem"&gt;coworker&lt;/a&gt; found
&lt;a href="https://github.com/kazuhisya/nodejs-rpm"&gt;https://github.com/kazuhisya/nodejs-rpm&lt;/a&gt;
which contains a configure patch and specfile for a dead-simple CentOS
5/6 &lt;span class="caps"&gt;RPM&lt;/span&gt; of NodeJS 0.10.9, which essentially just uses &lt;span class="caps"&gt;EPEL&lt;/span&gt;&amp;#8217;s python26
packages to power the NodeJS build process, configures and uses the
Makefile&amp;#8217;s &lt;code&gt;make binary&lt;/code&gt; command to spit out a NodeJS binary tarball,
and then packages that. That whole process way out of line from the
&lt;a href="http://fedoraproject.org/wiki/Packaging:Guidelines"&gt;Fedora Packaging
Guidelines&lt;/a&gt;, and
also only dumps out nodejs, nodejs-binary and nodejs-debuginfo packages,
so I also can&amp;#8217;t just substitute in a different package name in my puppet
manifests (which install nodejs, nodejs-devel and npm packages). So I
&lt;a href="https://github.com/jantman/nodejs-rpm-centos5"&gt;forked that repository&lt;/a&gt;
and made some changes to the specfile: I gave the package name a prefix
(&amp;#8220;cmgd_&amp;#8221;, since that&amp;#8217;s where I work these days) and some warnings in
the description, to make it abundantly clear that these packages are
very far from what you find in &lt;span class="caps"&gt;EPEL&lt;/span&gt; and other repositories, and broke
npm and the devel files out into their own subpackages. Hopefully this
spec file will be of use to someone else who also has the unfortunate
need of supporting recent NodeJS on CentOS 5. If there&amp;#8217;s enough
interest, I&amp;#8217;ll consider building the packages and putting them in a
repository&amp;nbsp;somewhere.&lt;/p&gt;
&lt;p&gt;You can see the NodeJS 0.10.9 on CentOS 5 spec file, a patch, and the
READMEs at
&lt;a href="https://github.com/jantman/nodejs-rpm-centos5"&gt;https://github.com/jantman/nodejs-rpm-centos5&lt;/a&gt;.
Patches and/or pull requests are greatly appreciated, especially from
anyone who wants to make the spec file more Fedora guidelines&amp;nbsp;compliant.&lt;/p&gt;</content><category term="build"></category><category term="centos"></category><category term="EPEL"></category><category term="node"></category><category term="nodejs"></category><category term="package"></category><category term="packaging"></category><category term="redhat"></category><category term="RHEL"></category><category term="rpm"></category><category term="specfile"></category></entry><entry><title>Search for a small-scale but automated RPM build system</title><link href="https://blog.jasonantman.com/2013/05/search-for-a-small-scale-but-automated-rpm-build-system/" rel="alternate"></link><published>2013-05-13T05:00:00-04:00</published><updated>2013-05-13T05:00:00-04:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2013-05-13:/2013/05/search-for-a-small-scale-but-automated-rpm-build-system/</id><summary type="html">&lt;p&gt;&lt;strong&gt;This post is part of a series of older draft posts from a few months
ago that I&amp;#8217;m just getting around to publishing. Unfortunately, I have
yet to find a build system that meets my requirements (see the last&amp;nbsp;paragraph).&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At work, we have a handful - currently a really …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;This post is part of a series of older draft posts from a few months
ago that I&amp;#8217;m just getting around to publishing. Unfortunately, I have
yet to find a build system that meets my requirements (see the last&amp;nbsp;paragraph).&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At work, we have a handful - currently a really small number - of &lt;span class="caps"&gt;RPM&lt;/span&gt;
packages that we need to build and deploy internally for our CentOS
server infrastructure. A number of them are just pulled down from
specific third-party repositories and rebuilt to have the vendor set as
us, and some are internally patched or developed software. We run
websites, and on the product side, we&amp;#8217;re a
Python/&lt;a href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt; shop (in fact, probably
one of the largest Django apps out there). We don&amp;#8217;t deploy our Django
apps via &lt;span class="caps"&gt;RPM&lt;/span&gt;, so building and distributing RPMs is definitely not one of
our core competencies. In fact, we really only want to do it when we&amp;#8217;re
testing/deploying a new distro, or when an upstream package is&amp;nbsp;updated.&lt;/p&gt;
&lt;p&gt;Last week I pulled a ticket to deploy &lt;a href="http://nodejs.org/"&gt;node.js&lt;/a&gt; to
one of our build hosts, and we&amp;#8217;ve got a few things in the pipeline that
also rely on it. I found the
&lt;a href="https://github.com/puppetlabs/puppetlabs-nodejs"&gt;puppetlabs-nodejs&lt;/a&gt;
module on Github that&amp;#8217;s supposed to install it on &lt;span class="caps"&gt;RHEL&lt;/span&gt;/CentOS, but it
pulls packages from
&lt;a href="http://patches.fedorapeople.org/oldnode/stable/"&gt;http://patches.fedorapeople.org/oldnode/stable/&lt;/a&gt;,
and the newest version of nodejs there is 0.6.18, which is quite old. I
can&amp;#8217;t find any actively maintained sources of newer nodejs packages for
&lt;span class="caps"&gt;RHEL&lt;/span&gt;/CentOS (yeah, I know, that&amp;#8217;s one down side to the
distributions&amp;#8230;). However, I did find that nodejs 0.9.5 is being &lt;a href="http://koji.fedoraproject.org/koji/packageinfo?packageID=15154"&gt;built
for Fedora 18/19 in the Fedora build
system&lt;/a&gt;,
is already in the Fedora 18 Testing and Fedora Rawhide repos, but is
failing its &lt;span class="caps"&gt;EL6&lt;/span&gt; builds in their system. The decision I&amp;#8217;ve come to is to
use the puppetlabs-nodejs module to install it, but try and rebuild the
Fedora 18 RPMs under CentOS 5 and&amp;nbsp;6.&lt;/p&gt;
&lt;p&gt;So that&amp;#8217;s the background. Now, my current task: to search for an &lt;span class="caps"&gt;RPM&lt;/span&gt;
build system for my current job. My core requirements, in no specific
order,&amp;nbsp;are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Be relatively easy and quick to use for people who have a specfile
    or &lt;span class="caps"&gt;SRPM&lt;/span&gt; and want to be able to &amp;#8220;ensure =&gt; present&amp;#8221; the finished &lt;span class="caps"&gt;RPM&lt;/span&gt;
    on a system. i.e., require as little per-package configuration as&amp;nbsp;possible.&lt;/li&gt;
&lt;li&gt;Be able to handle rebuilding &amp;#8220;all&amp;#8221; of our RPMs when we roll out a
    new distro version. Doesn&amp;#8217;t necessarily need to be automatic, but
    should be relatively&amp;nbsp;simple.&lt;/li&gt;
&lt;li&gt;Ideally, not need to be running constantly - i.e. something that
    will cope well with build hosts being VMs that are shut down when
    they&amp;#8217;re not&amp;nbsp;needed.&lt;/li&gt;
&lt;li&gt;Handle automatically putting successfully built packages into a
    repository, ideally with some sort of (manual) promotion process
    from staging to&amp;nbsp;stable.&lt;/li&gt;
&lt;li&gt;Have minimal external (infrastructure) dependencies that we can&amp;#8217;t
    satisfy with existing&amp;nbsp;systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, the first step was to research existing &lt;span class="caps"&gt;RPM&lt;/span&gt; build systems and how
others do this. Here&amp;#8217;s a list of what I could find online, though most
of these are from distributions and software vendors/projects, not
end-user companies that are only building for internal&amp;nbsp;use.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://fedorahosted.org/koji/wiki"&gt;Koji&lt;/a&gt; is the build system used
    by &lt;a href="http://fedoraproject.org/wiki/Koji"&gt;Fedora&lt;/a&gt; and RedHat. It&amp;#8217;s
    about as full-featured as any can be, and I&amp;#8217;m familiar with it from
    my time at &lt;a href="http://koji.rutgers.edu/koji/"&gt;Rutgers University&lt;/a&gt;, as
    it&amp;#8217;s used to maintain their CentOS/&lt;span class="caps"&gt;RHEL&lt;/span&gt; packages. It&amp;#8217;s based largely
    on Mock. However, &lt;a href="http://fedoraproject.org/wiki/Koji/ServerHowTo"&gt;setting up the build
    server&lt;/a&gt; is no
    trivial task; there are few installations outside of Fedora/RedHat,
    and it relies on either Kerberos or an &lt;span class="caps"&gt;SSL&lt;/span&gt; &lt;span class="caps"&gt;CA&lt;/span&gt; infrastructure to
    authenticate machines and clients. So, it&amp;#8217;s designed for too large a
    scale and too much infrastructure for&amp;nbsp;me.&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;PLD&lt;/span&gt; Linux has a &lt;a href="https://www.pld-linux.org/developingpld/builderscript"&gt;builder
    script&lt;/a&gt; that
    seems to automate &lt;code&gt;rpmbuild&lt;/code&gt; as well as fetching sources and
    resolving/building dependencies. I haven&amp;#8217;t looked at the script yet,
    but apparently it&amp;#8217;s in &lt;span class="caps"&gt;PLD&lt;/span&gt;&amp;#8217;s &amp;#8220;rpm-build-tools&amp;#8221;&amp;nbsp;package.&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;PLD&lt;/span&gt; Linux also has a &lt;span class="caps"&gt;CVS&lt;/span&gt; repository for something called
    &lt;a href="http://cvs.pld-linux.org/cgi-bin/cvsweb/pld-builder.new"&gt;pld-builder.new&lt;/a&gt;.
    The
    &lt;a href="http://cvs.pld-linux.org/cgi-bin/cvsweb/pld-builder.new/doc/README?rev=1.5"&gt;&lt;span class="caps"&gt;README&lt;/span&gt;&lt;/a&gt;
    and
    &lt;a href="http://cvs.pld-linux.org/cgi-bin/cvsweb/pld-builder.new/doc/ARCHITECTURE?rev=1.6"&gt;&lt;span class="caps"&gt;ARCHITECTURE&lt;/span&gt;&lt;/a&gt;
    files make it sound like a relatively simple mainly-Python system
    that builds &lt;span class="caps"&gt;SRPMS&lt;/span&gt; and binary packages when requested, and most
    importantly, seems like a simple system that uses little more than
    shared filesystem access for communication and&amp;nbsp;coordination.&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;ALT&lt;/span&gt; Linux has &lt;a href="http://en.altlinux.org/Sisyphus"&gt;Sisyphus&lt;/a&gt;, which
    combines repository management and web interface tools, package
    building and testing tools, and&amp;nbsp;more.&lt;/li&gt;
&lt;li&gt;The Dries &lt;span class="caps"&gt;RPM&lt;/span&gt; repository uses (or at least used&amp;#8230; my reference is
    quite old) &lt;a href="http://dries.ulyssis.org/rpm/pydar2/index.html"&gt;pydar2&lt;/a&gt;,
    &amp;#8220;a distributed client/server program which allows you to build
    multiple spec files on multiple distribution/architecture
    combinations automatically.&amp;#8221; That sounds like it could be what I
    need, but the last update says that it isn&amp;#8217;t finished yet, and that
    was in &lt;strong&gt;2005&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Mandriva Linux has pretty extensive information on their build
    system &lt;a href="http://wiki.mandriva.com/en/Category:Build_System"&gt;on their
    wiki&lt;/a&gt; and a
    &lt;a href="http://wiki.mandriva.com/en/Development/Packaging/BuildSystem/Theory"&gt;build system theory
    page&lt;/a&gt;,
    but it seems to be largely a hodgepodge of shell scripts and
    cronjobs, and is likely not a candidate for use by anyone other than
    its&amp;nbsp;designers.&lt;/li&gt;
&lt;li&gt;Argeo provides the &lt;a href="https://www.argeo.org/wiki/SLC"&gt;&lt;span class="caps"&gt;SLC&lt;/span&gt; framework&lt;/a&gt;
    which has a &amp;#8220;&lt;span class="caps"&gt;RPM&lt;/span&gt; Factory&amp;#8221; component, but I can&amp;#8217;t seem to find much
    more than a wiki page, and can&amp;#8217;t tell if it&amp;#8217;s a build automation
    system or just handles mocking packages and putting them in a repo
    on a single&amp;nbsp;host.&lt;/li&gt;
&lt;li&gt;Dag Wieers&amp;#8217; repositories use (or used) a set of python scripts
    called &lt;a href="http://dag.wieers.com/home-made/dar/"&gt;&lt;span class="caps"&gt;DAR&lt;/span&gt;, &amp;#8220;Dynamic Apt Repository
    builder&amp;#8221;&lt;/a&gt;. They&amp;#8217;re on
    &lt;a href="https://github.com/dagwieers/dar"&gt;github&lt;/a&gt; but are listed as &amp;#8220;old&amp;#8221;
    and haven&amp;#8217;t been updated in at least 2 years. The features sound
    quite interesting, and though it&amp;#8217;s based on the Apt repo format, it
    might provide some good ideas for implementing a similar&amp;nbsp;system.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Update four months later:&lt;/strong&gt; I&amp;#8217;ve yet to find a build system that meets
my requirements above. For the moment I&amp;#8217;m only managing \~20 packages,
so my &amp;#8220;build system&amp;#8221; is a single shell script that reads in some
environment variables and runs through using
&lt;a href="http://fedoraproject.org/wiki/Projects/Mock"&gt;mock&lt;/a&gt; to build them in the
correct order (including pushing the finished RPMs back into the local
repository that mock reads from) and then pushing the finished packages
to our internal repository. Maybe when I have some spare time, I&amp;#8217;ll
consider a project to either make a slightly better (but simple) &lt;span class="caps"&gt;RPM&lt;/span&gt;
build system based on Python, or get our
&lt;a href="http://jenkins-ci.org/"&gt;Jenkins&lt;/a&gt; install to handle this for&amp;nbsp;me.&lt;/p&gt;</content><category term="build"></category><category term="linux"></category><category term="nodejs"></category><category term="package"></category><category term="packaging"></category><category term="repository"></category><category term="rpm"></category><category term="rpmbuild"></category><category term="software"></category><category term="sysadmin"></category><category term="yum"></category></entry><entry><title>RPM Spec Files for nodejs 0.9.5 and v8 on CentOS 6</title><link href="https://blog.jasonantman.com/2013/01/rpm-spec-files-for-nodejs-0-9-5-and-v8-on-centos-5/" rel="alternate"></link><published>2013-01-31T14:13:00-05:00</published><updated>2013-01-31T14:13:00-05:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2013-01-31:/2013/01/rpm-spec-files-for-nodejs-0-9-5-and-v8-on-centos-5/</id><summary type="html">&lt;p&gt;The latest version of nodejs that I could find as an &lt;span class="caps"&gt;RPM&lt;/span&gt; for CentOS was
0.6.16, from
&lt;a href="http://patches.fedorapeople.org/oldnode/stable/"&gt;http://patches.fedorapeople.org/oldnode/stable/&lt;/a&gt;.
That&amp;#8217;s the one that puppetlabs currently uses in their
&lt;a href="https://github.com/puppetlabs/puppetlabs-nodejs"&gt;puppetlabs-nodejs&lt;/a&gt;
module. There is, however, a nodejs 0.9.5 &lt;span class="caps"&gt;RPM&lt;/span&gt; in the Fedora Rawhide …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The latest version of nodejs that I could find as an &lt;span class="caps"&gt;RPM&lt;/span&gt; for CentOS was
0.6.16, from
&lt;a href="http://patches.fedorapeople.org/oldnode/stable/"&gt;http://patches.fedorapeople.org/oldnode/stable/&lt;/a&gt;.
That&amp;#8217;s the one that puppetlabs currently uses in their
&lt;a href="https://github.com/puppetlabs/puppetlabs-nodejs"&gt;puppetlabs-nodejs&lt;/a&gt;
module. There is, however, a nodejs 0.9.5 &lt;span class="caps"&gt;RPM&lt;/span&gt; in the Fedora Rawhide (19)
repository. Below are some patches to that specfile, and the specfile
for its v8 dependency, to get them to build on CentOS 6. You can also
find the full specfiles on my &lt;a href="https://github.com/jantman/specfiles"&gt;github specfile
repository&lt;/a&gt;. I had originally
wanted to get them built on CentOS 5 as well, but after following the
dependency tree from nodejs to http-parser to gyp, and then finding
issues in the gyp source that are incompatible with CentOS 5&amp;#8217;s python
2.4, I gave up on that&amp;nbsp;target.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;nodejs.spec&lt;/strong&gt;, diff from Fedora Rawhide nodejs-0.9.5-9.fc18.src.rpm,
buildID=377755 (&lt;a href="https://raw.github.com/jantman/specfiles/master/nodejs.spec"&gt;full
specfile&lt;/a&gt;)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gh"&gt;diff --git a/nodejs.spec b/nodejs.spec&lt;/span&gt;
&lt;span class="gh"&gt;index 050ed86..86c0f4b 100644&lt;/span&gt;
&lt;span class="gd"&gt;--- a/nodejs.spec&lt;/span&gt;
&lt;span class="gi"&gt;+++ b/nodejs.spec&lt;/span&gt;
&lt;span class="gu"&gt;@@ -1,6 +1,6 @@&lt;/span&gt;
 Name: nodejs
 Version: 0.9.5
&lt;span class="gd"&gt;-Release: 9%{?dist}&lt;/span&gt;
&lt;span class="gi"&gt;+Release: 10%{?dist}&lt;/span&gt;
 Summary: JavaScript runtime
 License: &lt;span class="caps"&gt;MIT&lt;/span&gt; and &lt;span class="caps"&gt;ASL&lt;/span&gt; 2.0 and &lt;span class="caps"&gt;ISC&lt;/span&gt; and &lt;span class="caps"&gt;BSD&lt;/span&gt;
 Group: Development/Languages
&lt;span class="gu"&gt;@@ -25,7 +25,7 @@ Source6: nodejs-fixdep&lt;/span&gt;
 BuildRequires: v8-devel &amp;gt;= %{v8_ge}
 BuildRequires: http-parser-devel &amp;gt;= 2.0
 BuildRequires: libuv-devel
&lt;span class="gd"&gt;-BuildRequires: c-ares-devel&lt;/span&gt;
&lt;span class="gi"&gt;+BuildRequires: c-ares-devel &amp;gt;= 1.9.0&lt;/span&gt;
 BuildRequires: zlib-devel
 # Node.js requires some features from openssl 1.0.1 for &lt;span class="caps"&gt;SPDY&lt;/span&gt; support
 BuildRequires: openssl-devel &amp;gt;= 1:1.0.1
&lt;span class="gu"&gt;@@ -165,9 +165,13 @@ cp -p common.gypi %{buildroot}%{_datadir}/node&lt;/span&gt;

 %files docs
 %{_defaultdocdir}/%{name}-docs-%{version}
&lt;span class="gd"&gt;-%doc &lt;span class="caps"&gt;LICENSE&lt;/span&gt;&lt;/span&gt;

 %changelog
&lt;span class="gi"&gt;+* Thu Jan 31 2013 Jason Antman  - 0.9.5-10&lt;/span&gt;
&lt;span class="gi"&gt;+- specify build requirement of c-ares-devel &amp;gt;= 1.9.0&lt;/span&gt;
&lt;span class="gi"&gt;+- specify build requirement of libuv-devel 0.9.4&lt;/span&gt;
&lt;span class="gi"&gt;+- remove duplicate %doc &lt;span class="caps"&gt;LICENSE&lt;/span&gt; that was causing cpio &amp;#39;Bad magic&amp;#39; error on CentOS6&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
 * Sat Jan 12 2013 &lt;span class="caps"&gt;T.C.&lt;/span&gt; Hollingsworth  - 0.9.5-9
 - fix brown paper bag bug in requires generation script
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;v8.spec&lt;/strong&gt;, diff from Fedora Rawhide 3.13.7.5-2 (&lt;a href="https://raw.github.com/jantman/specfiles/master/v8.spec"&gt;full
specfile&lt;/a&gt;)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gd"&gt;--- v8.spec.orig       2013-01-26 16:03:18.000000000 -0500&lt;/span&gt;
&lt;span class="gi"&gt;+++ v8.spec     2013-01-31 09:04:51.068029459 -0500&lt;/span&gt;
&lt;span class="gu"&gt;@@ -21,9 +21,11 @@&lt;/span&gt;

 # %%global svnver 20110721svn8716

&lt;span class="gi"&gt;+%{!?python_sitelib: %define python_sitelib %(%{__python} -c &amp;quot;import distutils.sysconfig as d; print d.get_python_lib()&amp;quot;)}&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
 Name:          v8
 Version:       %{somajor}.%{sominor}.%{sobuild}.%{sotiny}
&lt;span class="gd"&gt;-Release:       2%{?dist}&lt;/span&gt;
&lt;span class="gi"&gt;+Release:       5%{?dist}&lt;/span&gt;
 Epoch:         1
 Summary:       JavaScript Engine
 Group:         System Environment/Libraries
&lt;span class="gu"&gt;@@ -32,7 +34,7 @@&lt;/span&gt;
 Source0:       http://commondatastorage.googleapis.com/chromium-browser-official/v8-%{version}.tar.bz2
 BuildRoot:     %{_tmppath}/%{name}-%{version}-%{release}-root-%(%{__id_u} -n)
 ExclusiveArch: %{ix86} x86_64 %{arm}
&lt;span class="gd"&gt;-BuildRequires: scons, readline-devel, libicu-devel&lt;/span&gt;
&lt;span class="gi"&gt;+BuildRequires: scons, readline-devel, libicu-devel, ncurses-devel&lt;/span&gt;

 %description
 V8 is Google&amp;#39;s open source JavaScript engine. V8 is written in C++ and is used 
&lt;span class="gu"&gt;@@ -51,8 +53,13 @@&lt;/span&gt;
 %setup -q -n %{name}-%{version}

 # -fno-strict-aliasing is needed with gcc 4.4 to get past some ugly code
&lt;span class="gd"&gt;-PARSED_OPT_FLAGS=`echo \&amp;#39;$RPM_OPT_FLAGS -fPIC -fno-strict-aliasing -Wno-unused-parameter -Wno-error=strict-overflow -Wno-error=unused-local-typedefs -Wno-unused-but-set-variable\&amp;#39;| sed &amp;quot;s/ /&amp;#39;,/g&amp;quot; | sed &amp;quot;s/&amp;#39;,/&amp;#39;, &amp;#39;/g&amp;quot;`&lt;/span&gt;
&lt;span class="gi"&gt;+%if 0%{?el5}&lt;/span&gt;
&lt;span class="gi"&gt;+PARSED_OPT_FLAGS=`echo \&amp;#39;$RPM_OPT_FLAGS -fPIC -fno-strict-aliasing -Wno-unused-parameter -lncurses\&amp;#39;| sed &amp;quot;s/ /&amp;#39;,/g&amp;quot; | sed &amp;quot;s/&amp;#39;,/&amp;#39;, &amp;#39;/g&amp;quot;`&lt;/span&gt;
&lt;span class="gi"&gt;+sed -i &amp;quot;s|&amp;#39;-O3&amp;#39;,|$PARSED_OPT_FLAGS,|g&amp;quot; SConstruct&lt;/span&gt;
&lt;span class="gi"&gt;+%else&lt;/span&gt;
&lt;span class="gi"&gt;+PARSED_OPT_FLAGS=`echo \&amp;#39;$RPM_OPT_FLAGS -fPIC -fno-strict-aliasing -Wno-unused-parameter -Wno-error=strict-overflow -Wno-unused-but-set-variable\&amp;#39;| sed &amp;quot;s/ /&amp;#39;,/g&amp;quot; | sed &amp;quot;s/&amp;#39;,/&amp;#39;, &amp;#39;/g&amp;quot;`&lt;/span&gt;
 sed -i &amp;quot;s|&amp;#39;-O3&amp;#39;,|$PARSED_OPT_FLAGS,|g&amp;quot; SConstruct
&lt;span class="gi"&gt;+%endif&lt;/span&gt;

 # clear spurious executable bits
 find . \( -name \*.cc -o -name \*.h -o -name \*.py \) -a -executable   
&lt;span class="gu"&gt;@@ -198,6 +205,17 @@&lt;/span&gt;
 %{python_sitelib}/j*.py*

 %changelog
&lt;span class="gi"&gt;+* Thu Jan 31 2013 Jason Antman  - 1:3.13.7.5-5&lt;/span&gt;
&lt;span class="gi"&gt;+- remove -Werror=unused-local-typedefs on cent6&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
&lt;span class="gi"&gt;+* Wed Jan 30 2013 Jason Antman  - 1:3.13.7.5-4&lt;/span&gt;
&lt;span class="gi"&gt;+- define python_sitelib if it isn&amp;#39;t already (CentOS 5)&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
&lt;span class="gi"&gt;+* Wed Jan 30 2013 Jason Antman  - 1:3.13.7.5-3&lt;/span&gt;
&lt;span class="gi"&gt;+- pull 3.13.7.5-2 &lt;span class="caps"&gt;SRPM&lt;/span&gt; from Fedora 19 Koji most recent build&lt;/span&gt;
&lt;span class="gi"&gt;+- add ncurses-devel BuildRequires&lt;/span&gt;
&lt;span class="gi"&gt;+- modify PARSED_OPT_FLAGS to work with g++ 4.1.2 on CentOS 5&lt;/span&gt;
&lt;span class="gi"&gt;+ &lt;/span&gt;
 * Sat Jan 26 2013 &lt;span class="caps"&gt;T.C.&lt;/span&gt; Hollingsworth  - 1:3.13.7.5-2
 - rebuild for icu-50
 - ignore new &lt;span class="caps"&gt;GCC&lt;/span&gt; 4.8 warning
&lt;/pre&gt;&lt;/div&gt;</content><category term="build"></category><category term="centos"></category><category term="node"></category><category term="nodejs"></category><category term="package"></category><category term="packaging"></category><category term="redhat"></category><category term="RHEL"></category><category term="rpm"></category><category term="specfile"></category></entry></feed>