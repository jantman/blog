<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jason Antman's Blog - netflix</title><link href="http://blog.jasonantman.com/" rel="alternate"></link><link href="http://blog.jasonantman.com/feeds/tags/netflix.atom.xml" rel="self"></link><id>http://blog.jasonantman.com/</id><updated>2015-05-05T06:45:00-04:00</updated><entry><title>Local S3 Server to Acceptance Test Netflix Ice Installation In Isolation</title><link href="http://blog.jasonantman.com/2015/05/local-s3-server-to-acceptance-test-netflix-ice-installation-in-isolation/" rel="alternate"></link><published>2015-05-05T06:45:00-04:00</published><updated>2015-05-05T06:45:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2015-05-05:/2015/05/local-s3-server-to-acceptance-test-netflix-ice-installation-in-isolation/</id><summary type="html">&lt;p&gt;How I wrote isolated acceptance tests for Netflix Ice Puppet installation using a locally-backed S3 &lt;span class="caps"&gt;API&lt;/span&gt;&amp;nbsp;server.&lt;/p&gt;</summary><content type="html">&lt;p&gt;At work, we recently started using &lt;a href="http://netflix.github.io/"&gt;Netflix &lt;span class="caps"&gt;OSS&lt;/span&gt;&lt;/a&gt;&amp;#8216;s &lt;a href="https://github.com/Netflix/ice"&gt;Ice&lt;/a&gt; &lt;span class="caps"&gt;AWS&lt;/span&gt; cost analysis tool.
It provides a Java daemon to read and parse &lt;span class="caps"&gt;AWS&lt;/span&gt;&amp;#8217; &lt;a href="http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/detailed-billing-reports.html"&gt;detailed billing reports&lt;/a&gt;
and a web interface to the data (&lt;a href="https://github.com/Netflix/ice/blob/master/README.md#screenshots"&gt;screenshots&lt;/a&gt;). The single biggest feature for us
is the ability to do cost breakdowns (by hour/day/week/month) based on Cost Allocation tags in the detailed billing reports. We tag every billable &lt;span class="caps"&gt;AWS&lt;/span&gt;
resource with the Application Name, Service Class (environment; dev/test/prod) and Responsible Party. Ice lets us configure &amp;#8220;Application Groups&amp;#8221;
based on applications as seen from a business/budgetary standpoint and allow up-to-the-hour data on that available to anyone who needs&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;We spun up the development install of Ice for a few weeks to give it a spin, but once people started complaining that my screen session died and took
Ice with it, it was clear we needed a real, permanent installation. While there is &lt;a href="https://github.com/mdsol/ice_cookbook"&gt;chef&lt;/a&gt; and &lt;a href="https://github.com/Answers4AWS/netflixoss-ansible"&gt;ansible&lt;/a&gt;
code to install and configure Ice, we&amp;#8217;re a Puppet shop, and there wasn&amp;#8217;t anything available that I could find for Puppet. So, I set about writing a
module to install and configure Ice, running in Tomcat behind an Nginx proxy. Like any good modern module, I wanted not only &lt;a href="http://rspec-puppet.com/"&gt;rspec-puppet&lt;/a&gt;
unit tests but also &lt;a href="https://github.com/puppetlabs/beaker"&gt;beaker&lt;/a&gt; acceptance tests. For those unfamiliar, Beaker is an acceptance testing framework for Puppet
that&amp;#8217;s similar to Test Kitchen; it spins up Vagrant machines, runs some code in them, and then uses &lt;a href="http://serverspec.org/"&gt;serverspec&lt;/a&gt; to make assertions about
the state of the system (file contents, running processes, command output, etc.) (side note: if you used Beaker prior to the
&lt;a href="https://github.com/puppetlabs/beaker/blob/master/HISTORY.md#beaker2.0.0"&gt;2.0 release&lt;/a&gt; in December 2014, you should really try it again; they&amp;#8217;ve made some great&amp;nbsp;improvements).&lt;/p&gt;
&lt;h2 id="the-problem"&gt;The&amp;nbsp;Problem&lt;/h2&gt;
&lt;p&gt;This posed a bit of a challenge, as Ice (in addition to being pretty poorly documented) is really designed to run in &lt;span class="caps"&gt;AWS&lt;/span&gt;. Firstly, the very reason we started running Ice was
to get a handle on our fast-growing &lt;span class="caps"&gt;AWS&lt;/span&gt; spend; as a result, we&amp;#8217;re trying hard not to use &lt;span class="caps"&gt;AWS&lt;/span&gt; for small-scale projects that could use existing resources. Second, while our
company very unfortunately doesn&amp;#8217;t have an open source policy and isn&amp;#8217;t releasing anything (hopefully this may be changing soon), we try hard to write generic, forge-quality&amp;nbsp;modules.&lt;/p&gt;
&lt;p&gt;As a result, I wanted to use the default Vagrant/VirtualBox provider for Beaker. To make matters worse, in keeping with the spirit of a community module, I didn&amp;#8217;t
want the acceptance tests to require anything specific to my company, such as an S3 bucket preseeded with our billing data. Ice both reads the detailed billing reports
(one of its three inputs; &lt;span class="caps"&gt;EC2&lt;/span&gt; pricing data and your accounts&amp;#8217; reservation pricing/capacity being the others) and writes state from and to S3. So, this was a bit difficult.
As we don&amp;#8217;t plan on upgrading Ice terribly often, and we wanted to install from the &lt;a href="https://netflixoss.ci.cloudbees.com/job/ice-master/"&gt;cloudbees master builds&lt;/a&gt;, we wanted
acceptance testing of not just the provisioning tooling, but also some basic smoke tests for the application&amp;nbsp;itself.&lt;/p&gt;
&lt;h2 id="the-solution"&gt;The&amp;nbsp;Solution&lt;/h2&gt;
&lt;p&gt;I managed to come up with a working, albeit somewhat Rube Goldberg, method of getting isolated acceptance tests to work. What follows is the gist of how I got Ice
working in complete isolation. The majority of this happens in &lt;code&gt;spec/acceptance/0prerequisite_spec.rb&lt;/code&gt; which runs first and both does the prerequisite setup
and validates that everything is setup right and working for the tests. The following solution is based on the amazingly helpful &lt;a href="https://github.com/jubos/fake-s3"&gt;fakes3&lt;/a&gt;
Ruby gem, the &lt;a href="http://www.apsis.ch/pound/"&gt;Pound&lt;/a&gt; reverse proxy, and some &lt;span class="caps"&gt;SSL&lt;/span&gt; certificate trickery. While my code was specific to Beaker, this should be generic
enough to use with any system acceptance testing&amp;nbsp;tool.&lt;/p&gt;
&lt;h2 id="prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;First, we obtain or create some files that we&amp;#8217;ll need on the test&amp;nbsp;instance:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Grab a relatively recent Detailed Billing With Resources and Tags zipped &lt;span class="caps"&gt;CSV&lt;/span&gt; report from an &lt;span class="caps"&gt;AWS&lt;/span&gt; account of yours (the filename is in the format
    &lt;code&gt;&amp;lt;ACCOUNT NUMBER&amp;gt;-aws-billing-detailed-line-items-with-resources-and-tags-&amp;lt;YYYY&amp;gt;-&amp;lt;MM&amp;gt;.csv&lt;/code&gt;). Manually trim it down to a sufficient sample of data;
    I took a few hours&amp;#8217; worth of data from one day and trimmed it down to just that referencing a few randomly chosen &lt;span class="caps"&gt;RDS&lt;/span&gt; instances, ELBs, on-demand &lt;span class="caps"&gt;EC2&lt;/span&gt;
    instances and reserved &lt;span class="caps"&gt;EC2&lt;/span&gt; instances. I then anonymized the account number, resource IDs, tag values, and anything else identifying. Ice needs billing
    data in order to do anything, so this will serve as our test&amp;nbsp;data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When Ice runs, it attempts to retrieve reserved instance pricing. It appears (I&amp;#8217;ve lost the mailing list or GitHub issue reference) that it&amp;#8217;s typical for
    the first Ice run on an empty S3 work directory to die because these files are missing. As a result, grab the &lt;code&gt;reservation_prices.oneyear.*&lt;/code&gt; files from
    the S3 work bucket of a running/working Ice installation. This will prevent a time-consuming shutdown of Ice on the first&amp;nbsp;run.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generate a self-signed &lt;span class="caps"&gt;SSL&lt;/span&gt; key and certificate for &lt;code&gt;fakebucket.s3.amazonaws.com&lt;/code&gt;. Package them together in a &lt;span class="caps"&gt;PEM&lt;/span&gt; file suitable for use in web servers.
    (Note that most modern S3 &lt;span class="caps"&gt;API&lt;/span&gt; clients accept a full &lt;span class="caps"&gt;URL&lt;/span&gt; to a bucket, as there are now third parties that implement the S3 &lt;span class="caps"&gt;API&lt;/span&gt;. Ice does not; it connects
    to https://&lt;span class="caps"&gt;BUCKETNAME&lt;/span&gt;.s3amazonaws.com. As a result, this &lt;span class="caps"&gt;SSL&lt;/span&gt; foolery is&amp;nbsp;required.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="setup"&gt;Setup&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Install the &lt;a href="https://rubygems.org/gems/fakes3"&gt;fakes3&lt;/a&gt; rubygem; this provides an s3-compliant &lt;span class="caps"&gt;API&lt;/span&gt; backed by local filesystem storage.
    Configure it to run during your tests (I set it up as a systemd service, but there are certainly other ways to do this). Note that
    while fakes3 stores the uploaded data on the local filesystem, it maintains a mapping of known objects in memory; as such, the process
    always starts completely empty, regardless of what&amp;#8217;s in the backing directory on the filesystem. fakes3 allows all &lt;span class="caps"&gt;IAM&lt;/span&gt; credentials,
    so fake ones are fine. It also automatically creates buckets the first time they&amp;#8217;re&amp;nbsp;accessed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install the &lt;a href="http://www.apsis.ch/pound/"&gt;pound&lt;/a&gt; reverse proxy and configure it to listen on port 443 with the &lt;span class="caps"&gt;PEM&lt;/span&gt; file you generated
    earlier, and proxy to fakes3 (which listens by default on port 10000). The &lt;code&gt;ListenHTTPS&lt;/code&gt;section of &lt;code&gt;pound.cfg&lt;/code&gt; will need the
    &lt;code&gt;xHTTP 1&lt;/code&gt; directive in order to enable &lt;span class="caps"&gt;HTTP&lt;/span&gt; verbs other than&amp;nbsp;&lt;span class="caps"&gt;GET&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Setup a local hosts file entry pointing &lt;code&gt;fakebucket.s3.amazonaws.com&lt;/code&gt; at &lt;code&gt;127.0.0.1&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After fakes3 starts, upload your sample billing data file and your reserved instance pricing files to the appropriate paths under a
    bucket called &amp;#8220;fakebucket&amp;#8221;. You can use a tool such as &lt;a href="http://s3tools.org/s3cmd"&gt;s3cmd&lt;/a&gt; to manipulate its contents, and other
    supported tools are listed in &lt;a href="https://github.com/jubos/fake-s3/wiki/Supported-Clients"&gt;the documentation&lt;/a&gt;. This step also serves
    to validate your Pound configuration, which should pass &lt;span class="caps"&gt;HTTPS&lt;/span&gt; port 443 traffic through to fakes3 and allow you to store and
    retrieve&amp;nbsp;objects.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Figure out the path to the trusted keystore for the version of Java that you&amp;#8217;re running Ice under. On CentOS 7 with OpenJDK 1.7.0,
    this was (after a lot of symlinks) &lt;code&gt;/usr/lib/jvm/jre/lib/security/cacerts&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Import your self-signed certificate into the Java keystore as a trusted certificate. This will allow &lt;span class="caps"&gt;SSL&lt;/span&gt; verification to succeed even
    with a self-signed&amp;nbsp;certificate:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/bin/keytool -importcert -alias fakebucket -file fakebucket.s3.amazonaws.com.crt -keystore /usr/lib/jvm/jre/lib/security/cacerts -storepass changeit -trustcacerts -noprompt
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure &lt;code&gt;ice.properties&lt;/code&gt; for the above. The important and unintuitive parts that I found&amp;nbsp;are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Going by the above examples, your billing and work S3 bucket names should both be&amp;nbsp;&amp;#8220;fakebucket&amp;#8221;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Unless you want to mock out bigger parts of the &lt;span class="caps"&gt;AWS&lt;/span&gt; metadata service, run Ice with
   &lt;code&gt;-Dice.s3AccessKeyId=NotAValidAccessKeyId -Dice.s3SecretKey=NotAValidAwsSecretKeyXxxxxxxxxxxxxxxxxxx&lt;/code&gt;
   in the &lt;code&gt;JAVA_OPTS&lt;/code&gt;. If Ice can&amp;#8217;t retrieve an instance&amp;#8217;s &lt;span class="caps"&gt;IAM&lt;/span&gt; role from the metadata service
   (http://169.254.169.254/latest/meta-data/iam/security-credentials/) and doesn&amp;#8217;t have the
   access and secret keys defined, it won&amp;#8217;t run. Also note that while the documentation is &lt;strong&gt;very&lt;/strong&gt;
   unclear on this, a number of &lt;a href="https://github.com/Netflix/ice/issues/49#issuecomment-23497701"&gt;github issues&lt;/a&gt;
   clarify that these need to be passed in as Java runtime options; they can&amp;#8217;t be put in the properties&amp;nbsp;file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Disable the Reservation Capacity Poller (&lt;code&gt;ice.reservationCapacityPoller=false&lt;/code&gt;). This service
   needs to connect to the &lt;span class="caps"&gt;EC2&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt;, and will cause Ice to die if it&amp;nbsp;can&amp;#8217;t.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For testing purposes, it&amp;#8217;s a lot simpler and less error-prone (as well as being a lot faster) to
   test the processor and reader separately - at least in serial instead of simultaneously in the same&amp;nbsp;instance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once all this is done, running the Ice Processor should retrieve the billing file, process it, and write the processed data to the
fakes3 bucket. Running the Reader should display the data properly. So far I&amp;#8217;ve been unable to find any features (other than the
Reservation Capacity Poller, noted above) that don&amp;#8217;t work with this&amp;nbsp;setup.&lt;/p&gt;
&lt;p&gt;Whether it&amp;#8217;s related to Ice itself or ideas for acceptance testing isolated applications, I hope this can be of use to&amp;nbsp;someone&amp;#8230;&lt;/p&gt;</content><category term="netflix"></category><category term="ice"></category><category term="puppet"></category><category term="beaker"></category><category term="acceptance testing"></category><category term="aws"></category><category term="s3"></category><category term="fakes3"></category><category term="testing"></category></entry><entry><title>A Collection of Great Links on Monitoring, SysAdmin, Scaling, etc.</title><link href="http://blog.jasonantman.com/2012/04/a-collection-of-great-links-on-monitoring-sysadmin-scaling-etc/" rel="alternate"></link><published>2012-04-21T10:24:00-04:00</published><updated>2012-04-21T10:24:00-04:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2012-04-21:/2012/04/a-collection-of-great-links-on-monitoring-sysadmin-scaling-etc/</id><summary type="html">&lt;p&gt;I&amp;#8217;ve had a bunch of tabs open in my browser for a while - stuff that I
read, thought was wonderful, and wanted to comment on. At risk of
letting it pile up forever, here&amp;#8217;s a collection of links that I thought
were really interesting or&amp;nbsp;insightful&amp;#8230;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://blog.mongodb.org/post/172254834/mongodb-is-fantastic-for-logging"&gt;MongoDB is …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;I&amp;#8217;ve had a bunch of tabs open in my browser for a while - stuff that I
read, thought was wonderful, and wanted to comment on. At risk of
letting it pile up forever, here&amp;#8217;s a collection of links that I thought
were really interesting or&amp;nbsp;insightful&amp;#8230;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://blog.mongodb.org/post/172254834/mongodb-is-fantastic-for-logging"&gt;MongoDB is Fantastic for
    Logging&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;I was looking into some log storage ideas, and came by this post
(on the MongoDB blog) about why Mongo is well-suited to storing&amp;nbsp;logs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.sonian.com/cloud-monitoring-sensu/"&gt;Sensu&lt;/a&gt; - a
    Ruby-based cloud-oriented monitoring system. It uses &lt;span class="caps"&gt;AMQP&lt;/span&gt;/RabbitMQ
    to communicate between the clients and server, which is a really big
    part of what I think monitoring should&amp;nbsp;be.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://highscalability.com/"&gt;High Scalability&lt;/a&gt; - this is one of the
    few blogs I follow on a regular basis. Some really wonderful stuff,
    and great food for&amp;nbsp;thought.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://everythingsysadmin.com/2012/03/fear-of-rebooting.html"&gt;Everything Sysadmin: Fear of
    Rebooting&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;A great article on Tom Limoncelli&amp;#8217;s blog about why we fear
rebooting machines and why this is bad - moreover, why we should
reboot&amp;nbsp;often.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://techblog.netflix.com/2012/02/fault-tolerance-in-high-volume.html"&gt;The Netflix Tech Blog: Fault Tolerance in a High Volume,
    Distributed
    System&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;This is a &lt;em&gt;really, really&lt;/em&gt; cool post NetFlix about how latency
increases in a single subsystem can bring down their whole &lt;span class="caps"&gt;API&lt;/span&gt; in
seconds, and how they combat this. Really cool&amp;nbsp;stuff.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arstechnica.com/business/news/2012/04/exclusive-a-behind-the-scenes-look-at-facebook-release-engineering.ars/1"&gt;Ars Technica - Exclusive: a behind-the-scenes look at Facebook
    release
    engineering&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Ars Technical is more or less &amp;#8220;mainstream media&amp;#8221; to me, but this
is a really interesting writeup on Facebook&amp;#8217;s release engineering
process, albeit at a higher level. Specifically, it talks about
their automation, phased rollouts, rollbacks, and how they release
the Facebook codebase as a single giant binary, sent out via&amp;nbsp;BitTorrent.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/monitoringsucks/blog-posts"&gt;Monitoring Sucks blog posts
    (github)&lt;/a&gt; - The
    &amp;#8220;monitoing sucks&amp;#8221; movement really speaks to me, having worked
    extensively with Nagios, Cacti, and similar technologies.
    Specifically, having rolled out monitoring in a variety of &amp;#8220;weird&amp;#8221;
    scenarios (a lot of monitoring devices or whole networks behind &lt;span class="caps"&gt;NAT&lt;/span&gt;,
    on dynamic &lt;span class="caps"&gt;IP&lt;/span&gt; connections, or otherwise unreachable from a central
    server), I&amp;#8217;ve felt a lot of pain in the current want of doing
    things. There are a lot of &lt;strong&gt;really&lt;/strong&gt; good thoughts linked here,
    especially the &lt;a href="http://jedi.be/blog/2012/01/03/monitoring-wonderland-survey-introduction"&gt;&amp;#8220;wonderland&amp;#8221; series by Patrick
    Debois&lt;/a&gt;
    and the &lt;a href="http://holmwood.id.au/~lindsay/2012/01/09/monitoring-sucks-latency-sucks-more"&gt;&amp;#8220;Latency sucks&amp;#8221; series by Lindsay
    Holmwood&lt;/a&gt;.
    This really got me thinking about my ideal monitoring system, which
    among other things, would integrate the &amp;#8220;alerting&amp;#8221; functions of
    Nagios with graphing/trending and correlation, would be based on
    some sort of message queue architecture (that supports multiple
    levels of proxies that could gracefully support &lt;span class="caps"&gt;NAT&lt;/span&gt; and multiple
    hops), and would be configured almost totally on the originating
    &amp;#8220;client&amp;#8221; (unlike the pain of distributed&amp;nbsp;Nagios/Icinga).&lt;/li&gt;
&lt;li&gt;&lt;a href="http://assets.en.oreilly.com/1/event/65/Metrics-driven%20Engineering%20at%20Etsy%20Presentation.pdf"&gt;Mike Brittain - Metrics Driven Engineering at Etsy (3.&lt;span class="caps"&gt;2MB&lt;/span&gt;
    &lt;span class="caps"&gt;PDF&lt;/span&gt;)&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;presentation slides. I&amp;#8217;d &lt;em&gt;love&lt;/em&gt; to see the video. Some really good
ideas about putting the science back into being a SysAdmin. Also
mentions a few tools I really want to play around with (including
ganglia, graphite, logster and StatsD). Also mentions adding &lt;span class="caps"&gt;PHP&lt;/span&gt;
memory usage and time to Apache logs, which I don&amp;#8217;t believe I never
thought&amp;nbsp;of.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Some really thoughtful posts from &lt;span class="caps"&gt;R. I.&lt;/span&gt; Pienaar on &lt;a href="http://www.devco.net/archives/2011/03/19/thinking_about_monitoring_frameworks.php"&gt;Thinking about
    monitoring
    frameworks&lt;/a&gt;
    and &lt;a href="http://www.devco.net/archives/2011/04/04/monitoring_framework_composable_architectures.php"&gt;Composable
    Architectures&lt;/a&gt;.
    Some really good stuff, but what else would you expect from someone
    &lt;a href="https://github.com/ripienaar/"&gt;like this&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</content><category term="amqp"></category><category term="etsy"></category><category term="facebook"></category><category term="limoncelli"></category><category term="links"></category><category term="mongodb"></category><category term="monitoring"></category><category term="monitoringsucks"></category><category term="netflix"></category><category term="sensu"></category></entry></feed>